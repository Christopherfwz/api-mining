@relation hadoop

@attribute fqCaller string
@attribute fqCalls string

@data
'org.apache.mahout.math.hadoop.stochasticsvd.ABtDenseOutJob.ABtMapper.cleanup','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.ABtDenseOutJob.ABtMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.ABtDenseOutJob.QRReducer.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.ABtDenseOutJob.QRReducer.getSplitFilePath','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getUniqueFile org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getWorkOutputPath org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.ABtDenseOutJob.QRReducer.createOutputCollector','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.math.hadoop.stochasticsvd.K,org.apache.mahout.math.hadoop.stochasticsvd.V>.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.ABtDenseOutJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setMinInputSplitSize org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'org.commoncrawl.hadoop.io.ARCInputFormat.setARCSourceClass','org.apache.hadoop.mapred.JobConf.setClass'
'org.commoncrawl.hadoop.io.ARCInputFormat.setIOBlockSize','org.apache.hadoop.mapred.JobConf.setInt'
'org.commoncrawl.hadoop.io.ARCInputFormat.setIOBufferSize','org.apache.hadoop.mapred.JobConf.setInt'
'org.commoncrawl.hadoop.io.ARCInputFormat.setIOTimeout','org.apache.hadoop.mapred.JobConf.setLong'
'org.commoncrawl.hadoop.io.ARCInputFormat.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.hadoop.io.ARCResource.write','org.apache.hadoop.io.Text.writeString'
'org.commoncrawl.hadoop.io.ARCResource.readFields','org.apache.hadoop.io.Text.readString'
'org.commoncrawl.hadoop.io.ARCSplit.readFields','org.apache.hadoop.io.Text.readString'
'org.commoncrawl.hadoop.io.ARCSplitReader.ARCSplitReader','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.hadoop.io.ARCSplitReader.createKey','org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.hadoop.io.ARCSplitReader.next','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.io.Text.set'
'org.commoncrawl.hadoop.io.ARCSplitReader.ARCSplitReader','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.hadoop.io.ARCSplitReader.next','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.io.Text.set'
'org.commoncrawl.hadoop.io.ARCSplitReader.createKey','org.apache.hadoop.io.Text.<init>'
'eu.scape_project.tb.wc.archd.test.ARCTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init>'
'eu.scape_project.tb.wc.archd.test.ARCTest.testNextKeyValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.getCurrentValue org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.hama.graph.AbsDiffAggregator.aggregate','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get'
'org.apache.hama.graph.AbsDiffAggregator.getValue','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.hama.graph.AbstractAggregator.getTimesAggregated','org.apache.hadoop.io.IntWritable.<init>'
'com.datasalt.pangool.utils.test.AbstractBaseTest.createNewConfiguration','org.apache.hadoop.conf.Configuration.<init>'
'com.twitter.elephanttwin.indexing.AbstractBlockIndexingJob.createIndexDescriptors','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.getFileChecksum org.apache.hadoop.fs.FileChecksum.getAlgorithmName org.apache.hadoop.fs.FileChecksum.getBytes org.apache.hadoop.fs.FileChecksum.getLength org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'com.twitter.elephanttwin.indexing.AbstractBlockIndexingJob.IndexingWorker.run','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.fs.FileStatus.getPath'
'com.twitter.elephanttwin.indexing.AbstractBlockIndexingJob.work','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Job.setJobName'
'com.twitter.elephanttwin.indexing.AbstractBlockIndexingJob.setupJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'com.twitter.elephanttwin.indexing.AbstractBlockIndexingJob.hasPreviousIndex','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.asakusafw.runtime.stage.directio.AbstractDirectOutputMapper.AbstractDirectOutputMapper','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.nutch.crawl.AbstractFetchSchedule.setConf','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.crawl.AbstractFetchSchedule.setConf','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'extramuros.java.jobs.file.filter.AbstractFilterMapper.map','org.apache.hadoop.io.IntWritable.<init>'
'extramuros.java.jobs.file.filter.AbstractFilterMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get'
'com.rapleaf.hank.hadoop.AbstractHadoopDomainBuilder.buildHankDomain','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.runJob'
'com.rapleaf.hank.hadoop.AbstractHadoopDomainBuilder.configureJobCommon','org.apache.hadoop.mapred.JobConf.setOutputCommitter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setNumReduceTasks'
'voldemort.store.readonly.mr.azkaban.AbstractHadoopJob.run','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.RunningJob.getTrackingURL org.apache.hadoop.mapred.RunningJob.waitForCompletion org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroupNames org.apache.hadoop.mapred.Counters.getGroup org.apache.hadoop.mapred.Counters.Counter.getDisplayName org.apache.hadoop.mapred.Counters.Counter.getValue'
'voldemort.store.readonly.mr.azkaban.AbstractHadoopJob.createJobConf','org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.getJar org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheArchive org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.mapred.JobConf.set'
'voldemort.store.readonly.mr.azkaban.AbstractHadoopJob.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'voldemort.store.readonly.mr.azkaban.AbstractHadoopJob.cancel','org.apache.hadoop.mapred.RunningJob.killJob'
'voldemort.store.readonly.mr.azkaban.AbstractHadoopJob.getProgress','org.apache.hadoop.mapred.RunningJob.mapProgress org.apache.hadoop.mapred.RunningJob.reduceProgress'
'voldemort.store.readonly.mr.azkaban.AbstractHadoopJob.getCounters','org.apache.hadoop.mapred.RunningJob.getCounters'
'voldemort.store.readonly.mr.azkaban.AbstractHadoopJob.setClassLoaderAndJar','org.apache.hadoop.mapred.JobConf.setClassLoader org.apache.hadoop.mapred.JobConf.setJar'
'org.apache.whirr.service.yarn.integration.AbstractHadoopServiceTest.test','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.whirr.service.yarn.integration.AbstractHadoopServiceTest.testExistsTemporaryFolderAndHiveWarehouse','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.initHadoop','org.apache.hadoop.fs.FileSystem.get'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.openWriter','org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.writable','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.BooleanWritable.<init>'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.assertRun','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.cleanUp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.toString'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.trash','org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.withInput','org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.withOutput','org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.readTuples','org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.withTupleOutput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'com.datasalt.pangool.utils.test.AbstractHadoopTestLibrary.ensureOutput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.oreilly.springdata.batch.item.AbstractHdfsItemWriter.initializeCounterIfNecessary','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileStatus.getPath'
'com.oreilly.springdata.hadoop.streaming.AbstractHdfsWriter.initializeCounterIfNecessary','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileStatus.getPath'
'com.inadco.hbl.model.AbstractHierarchy.optimizeHierarchySliceScan','org.apache.hadoop.hbase.util.Bytes.compareTo'
'hipi.imagebundle.AbstractImageBundle.open','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'com.asakusafw.testtools.inspect.AbstractInspector.getValue','org.apache.hadoop.io.Writable.getClass'
'org.apache.mahout.common.AbstractJobTest.testFlag','org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.common.AbstractJobTest.testOptions','org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.common.AbstractJobTest.testInputOutputPaths','org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.common.AbstractJob.parseDirectories','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.common.AbstractJob.prepareJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.common.AbstractJob.getCustomJobName','org.apache.hadoop.mapreduce.JobContext.getJobName'
'com.twitter.elephanttwin.lucene.indexing.AbstractLuceneIndexingJob.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.mapreduce.Job.isSuccessful'
'com.twitter.elephanttwin.lucene.indexing.AbstractLuceneIndexingJob.addInputPathRecursively','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath'
'com.twitter.elephanttwin.lucene.indexing.AbstractLuceneIndexingJob.writeIndexDescriptors','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'com.twitter.elephanttwin.lucene.indexing.AbstractLuceneIndexingJob.buildFileIndexDescriptor','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileChecksum.getAlgorithmName org.apache.hadoop.fs.FileChecksum.getBytes org.apache.hadoop.fs.FileChecksum.getLength'
'com.twitter.elephanttwin.lucene.indexing.AbstractLuceneIndexingJob.getIndexDescriptor','org.apache.hadoop.fs.Path.<init>'
'com.twitter.elephanttwin.lucene.indexing.AbstractLuceneIndexingJob.eval','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.cloudata.util.matrix.AbstractMatrix.mutiply','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMaxReduceAttempts org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'pignlproc.storage.AbstractNTriplesLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'pignlproc.storage.AbstractNTriplesLoader.getInputFormat','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init>'
'pignlproc.storage.AbstractNTriplesStorer.getOutputFormat','org.apache.hadoop.mapreduce.lib.output.TextOutputFormat<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text>.<init>'
'pignlproc.storage.AbstractNTriplesStorer.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput'
'org.apache.ivory.converter.AbstractOozieEntityMapper.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.ivory.converter.AbstractOozieEntityMapper.getJarName','org.apache.hadoop.fs.Path.getName'
'org.apache.ivory.converter.AbstractOozieEntityMapper.getCoordPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.converter.AbstractOozieEntityMapper.copySharedLibs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.converter.AbstractOozieEntityMapper.createCoordDefaultConfiguration','org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.converter.AbstractOozieEntityMapper.marshal','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.converter.AbstractOozieEntityMapper.createTempDir','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission'
'org.apache.ivory.converter.AbstractOozieEntityMapper.getHDFSPath','org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.pail.AbstractPail.PailOutputStream.PailOutputStream','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.pail.AbstractPail.PailOutputStream.close','org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.pail.AbstractPail.mkAttr','org.apache.hadoop.fs.Path.<init>'
'backtype.hadoop.pail.AbstractPail.writeMetadata','org.apache.hadoop.fs.Path.getParent'
'backtype.hadoop.pail.AbstractPail.getUserFileNames','org.apache.hadoop.fs.Path.<init>'
'backtype.hadoop.pail.AbstractPail.toStoredPath','org.apache.hadoop.fs.Path.<init>'
'backtype.hadoop.pail.AbstractPail.toStoredMetadataPath','org.apache.hadoop.fs.Path.<init>'
'backtype.hadoop.pail.AbstractPail.toStoredMetadataTmpPath','org.apache.hadoop.fs.Path.<init>'
'backtype.hadoop.pail.AbstractPail.getMetadataFileNames','org.apache.hadoop.fs.Path.<init>'
'backtype.hadoop.pail.AbstractPail.getStoredFilesAndMetadata','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'backtype.hadoop.pail.AbstractPail.getStoredUnfinishedFiles','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'backtype.hadoop.pail.AbstractPail.readDir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.isDir'
'backtype.hadoop.pail.AbstractPail.relify','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.pail.AbstractPail.getFilesHelper','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen'
'org.apache.mahout.common.parameters.AbstractParameter.AbstractParameter','org.apache.hadoop.conf.Configuration.get'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.<init>'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.runTripleToResourceJob','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.Job.waitForCompletion'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.bulkLoadIdStringMappingTables','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.Job.waitForCompletion'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.createTripleToResourceJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.createString2IdJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.setInputPaths org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.setOutputPath org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.createId2StringJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.setInputPaths org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.setOutputPath org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.retrieveTripleToResourceCounters','org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counters.getGroup org.apache.hadoop.mapreduce.CounterGroup.findCounter org.apache.hadoop.mapreduce.CounterGroup.findCounter org.apache.hadoop.mapreduce.Counters.getGroup org.apache.hadoop.mapreduce.CounterGroup.findCounter org.apache.hadoop.mapreduce.CounterGroup.findCounter'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.moveIdStringAssocDirectory','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.doTableBulkLoad','org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.doBulkLoad org.apache.hadoop.hbase.client.HTableInterface.getTableDescriptor'
'nl.vu.datalayer.hbase.bulkload.AbstractPrefixMatchBulkLoad.configureShuffle','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setInt'
'com.sematext.hbase.wd.AbstractRowKeyDistributor.getDistributedIntervals','org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.<init>'
'com.sematext.hbase.wd.AbstractRowKeyDistributor.getDistributedScans','org.apache.hadoop.hbase.client.Scan.getStartRow org.apache.hadoop.hbase.client.Scan.getStopRow org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Pair<byte,byte>.getFirst org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.util.Pair<byte,byte>.getSecond org.apache.hadoop.hbase.client.Scan.setStopRow'
'com.sematext.hbase.wd.AbstractRowKeyDistributor.addInfo','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.inmobi.databus.AbstractService.getLatestDir','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.AbstractService.getPreviousRuntime','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'com.inmobi.databus.AbstractService.publishMissingPaths','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'voldemort.store.readonly.mr.AbstractStoreBuilderConfigurable.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.ivory.entity.AbstractTestBase.storeEntity','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'com.twitter.elephantbird.pig.util.AbstractTestWritableConverter.setup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IOUtils.closeStream'
'com.twitter.elephantbird.pig.util.AbstractTestWritableConverter.readOutsidePig','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer>.initialize org.apache.hadoop.mapreduce.Job.getConfiguration'
'net.joshdevins.talks.hadoopstart.mr.AccessLogThroughputParseGroupBySecondsMapperTest.before','org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init>'
'net.joshdevins.talks.hadoopstart.mr.AccessLogThroughputParseGroupBySecondsMapperTest.testInFilter','org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'net.joshdevins.talks.hadoopstart.mr.AccessLogThroughputParseGroupBySecondsMapperTest.testOutsideFilter','org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.withInput org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'net.joshdevins.talks.hadoopstart.mr.AccessLogThroughputParseGroupBySecondsMapperTest.createTestInput','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.types.Pair<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.<init>'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.AccessTimeHandler.get','org.apache.hadoop.fs.FileStatus.getAccessTime'
'org.apache.giraph.io.accumulo.edgemarker.AccumuloEdgeOutputFormat.createVertexWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.giraph.io.accumulo.edgemarker.AccumuloEdgeOutputFormat.AccumuloEdgeVertexWriter.AccumuloEdgeVertexWriter','org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.accumulo.edgemarker.AccumuloEdgeOutputFormat.AccumuloEdgeVertexWriter.writeVertex','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,org.apache.accumulo.core.data.Mutation>.write'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.RecordWriter<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>.<init>'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.write','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.setFileType','org.apache.hadoop.conf.Configuration.set'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.setZooKeeperInstance','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.getInstance','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.setReplication','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.setDFSBlockSize','org.apache.hadoop.conf.Configuration.setLong'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.setCompressedBlockSize','org.apache.hadoop.conf.Configuration.setLong'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.setCompressedBlockSizeIndex','org.apache.hadoop.conf.Configuration.setLong'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormat.setCompressionType','org.apache.hadoop.conf.Configuration.set'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormatTest.setup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormatTest.teardown','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormatTest.handleWriteTests','org.apache.hadoop.mapreduce.RecordWriter<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>.write org.apache.hadoop.mapreduce.RecordWriter<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>.close org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getParent'
'org.apache.accumulo.core.client.mapreduce.AccumuloFileOutputFormatTest.validateConfiguration','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.AccumuloInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setOutputInfo','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setZooKeeperInstance','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setMockInstance','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setMaxMutationBufferSize','org.apache.hadoop.conf.Configuration.setLong'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setMaxLatency','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setMaxWriteThreads','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setTimeout','org.apache.hadoop.conf.Configuration.setLong'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setLogLevel','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.setSimulationMode','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getUsername','org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getPassword','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.canCreateTables','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getDefaultTableName','org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getInstance','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getMaxMutationBufferSize','org.apache.hadoop.conf.Configuration.getLong'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getMaxLatency','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getMaxWriteThreads','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getTimeout','org.apache.hadoop.conf.Configuration.getLong'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getLogLevel','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getSimulationMode','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.AccumuloRecordWriter.AccumuloRecordWriter','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.AccumuloRecordWriter.write','org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.AccumuloRecordWriter.addTable','org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.lib.output.NullOutputFormat<org.apache.hadoop.io.Text,org.apache.accumulo.core.data.Mutation>.<init> org.apache.hadoop.mapreduce.lib.output.NullOutputFormat<org.apache.hadoop.io.Text,org.apache.accumulo.core.data.Mutation>.getOutputCommitter'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormatTest.TestMapper.map','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormatTest.TestMapper.cleanup','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormatTest.testMR','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getMapperClass org.apache.hadoop.mapreduce.RecordReader<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>.initialize org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,org.apache.accumulo.core.data.Mutation>.close'
'org.apache.accumulo.core.client.mapreduce.AccumuloRowInputFormat.initialize','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.AccumuloRowInputFormat.nextKeyValue','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.AccumuloRowInputFormatTest.test','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.nextKeyValue org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.nextKeyValue org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.nextKeyValue org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.accumulo.core.util.PeekingIterator<java.util.Map.Entry<org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value>>>.nextKeyValue'
'org.apache.accumulo.pig.AccumuloStorage.getMutations','org.apache.hadoop.io.Text.getLength'
'org.apache.gora.accumulo.store.AccumuloStore.readMapping','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.gora.accumulo.store.AccumuloStore.populate','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals'
'org.apache.gora.accumulo.store.AccumuloStore.get','org.apache.hadoop.io.Text.<init>'
'org.apache.gora.accumulo.store.AccumuloStore.put','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.gora.accumulo.store.AccumuloStore.createRange','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.gora.accumulo.store.AccumuloStore.pad','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append'
'org.apache.gora.accumulo.store.AccumuloStore.getPartitions','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.bah.culvert.accumulo.database.AccumuloTableAdapter.get','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.bah.culvert.accumulo.database.AccumuloTableAdapter.getStartKeys','org.apache.hadoop.io.Text.getBytes'
'com.bah.culvert.accumulo.database.AccumuloTableAdapter.getEndKeys','org.apache.hadoop.io.Text.getBytes'
'com.bah.culvert.accumulo.database.AccumuloTableAdapter.apply','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.accumulo.AccumuloVertexOutputFormat.AccumuloVertexWriter.close','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,org.apache.accumulo.core.data.Mutation>.close'
'org.apache.oozie.service.ActionCheckerService.init','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.ActionCheckerService.init','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.ActionCheckerService.init','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.command.wf.ActionEndCommand.call','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong'
'org.apache.oozie.command.wf.ActionEndXCommand.execute','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.Context.getActionDir','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.getAppPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.createBaseWorkflow','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.createBaseWorkflowWithCredentials','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.createWorkflow','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.writeToFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.Context.getActionDir','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.getAppPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.createBaseWorkflow','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.createBaseWorkflowWithCredentials','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.createWorkflow','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.ActionExecutorTestCase.writeToFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.service.ActionService.register','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.oozie.service.ActionService.getExecutor','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.oozie.command.wf.ActionStartXCommand.execute','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong'
'org.apache.oozie.command.wf.ActionXCommand.ActionExecutorContext.getActionDir','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.wf.ActionXCommand.ActionExecutorContext.getActionDir','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.wf.ActionXCommand.ActionExecutorContext.getAppFileSystem','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.nutch.crawl.AdaptiveFetchSchedule.setConf','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getFloat'
'org.apache.nutch.crawl.AdaptiveFetchSchedule.setFetchSchedule','org.apache.hadoop.io.FloatWritable.get'
'org.apache.nutch.crawl.AdaptiveFetchSchedule.main','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.AddSplitTest.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.AddSplitTest.verifyData','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.AddSplitTest.insertData','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.lilyproject.server.modules.repository.AddressResolver.AddressResolver','org.apache.hadoop.net.DNS.getDefaultHost'
'org.apache.accumulo.core.util.AddressUtil.parseAddress','org.apache.hadoop.io.Text.toString'
'org.apache.giraph.io.AdjacencyListTextVertexInputFormat.AdjacencyListTextVertexReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get'
'org.apache.giraph.io.AdjacencyListTextVertexInputFormat.AdjacencyListTextVertexReader.preprocessLine','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.giraph.io.AdjacencyListTextVertexOutputFormat.AdjacencyListTextVertexWriter.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.giraph.io.AdjacencyListTextVertexOutputFormat.AdjacencyListTextVertexWriter.convertVertexToLine','org.apache.hadoop.io.Text.<init>'
'com.hphoto.admin.AdminTable.createTable','org.apache.hadoop.hbase.HBaseAdmin.tableExists org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HBaseAdmin.createTable'
'com.hphoto.admin.AdminTable.deleteTable','org.apache.hadoop.hbase.HBaseAdmin.deleteTable'
'com.hphoto.admin.AdminTable.addColumn','org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HBaseAdmin.addColumn'
'com.hphoto.admin.AdminTable.deleteColumn','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HBaseAdmin.deleteColumn'
'com.hphoto.admin.AdminTable.enableTable','org.apache.hadoop.hbase.HBaseAdmin.enableTable'
'com.hphoto.admin.AdminTable.disableTable','org.apache.hadoop.hbase.HBaseAdmin.disableTable'
'com.nearinfinity.hbase.dsl.AdminTest.defineTable','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.getFamily org.apache.hadoop.hbase.HColumnDescriptor.getBloomFilterType org.apache.hadoop.hbase.HColumnDescriptor.isInMemory org.apache.hadoop.hbase.HColumnDescriptor.isBlockCacheEnabled'
'com.nearinfinity.hbase.dsl.AdminTest.reconfigureTable','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.getFamily org.apache.hadoop.hbase.HColumnDescriptor.getBloomFilterType org.apache.hadoop.hbase.HColumnDescriptor.isInMemory org.apache.hadoop.hbase.HColumnDescriptor.isBlockCacheEnabled org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.getFamily org.apache.hadoop.hbase.HColumnDescriptor.getBloomFilterType org.apache.hadoop.hbase.HColumnDescriptor.isInMemory org.apache.hadoop.hbase.HColumnDescriptor.isBlockCacheEnabled'
'upenn.junto.algorithm.parallel.AdsorptionHadoop.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'upenn.junto.algorithm.parallel.AdsorptionHadoop.Reduce.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'upenn.junto.algorithm.parallel.AdsorptionHadoop.Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'upenn.junto.algorithm.parallel.AdsorptionHadoop.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.mahout.clustering.spectral.common.AffinityMatrixInputJob.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.spectral.common.AffinityMatrixInputMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.item.AggregateAndRecommendReducer.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.apache.accumulo.core.iterators.AggregatingIteratorTest.nk','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.AggregatingIteratorTest.nr','org.apache.hadoop.io.Text.<init>'
'edu.jhu.thrax.hadoop.paraphrasing.AggregationReducer.setup','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.jhu.thrax.hadoop.paraphrasing.AggregationReducer.reduce','org.apache.hadoop.io.NullWritable.get'
'org.apache.accumulo.core.iterators.aggregation.conf.AggregatorConfigurationTest.testBinary','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.iterators.aggregation.conf.AggregatorConfigurationTest.testBasic','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.fpm.pfpgrowth.AggregatorMapper.map','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.fpm.pfpgrowth.AggregatorMapper.map','org.apache.hadoop.io.Text.<init>'
'.AlgorithmMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'.AlgorithmMapper.map','org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.OutputCollector<.FromNodeKey,.ToNodeValues>.collect org.apache.hadoop.mapred.OutputCollector<.FromNodeKey,.ToNodeValues>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<.FromNodeKey,.ToNodeValues>.collect'
'.AlgorithmMapper.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt'
'.AlgorithmOutputValueGroupingComparator.compare','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'.AlgorithmReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'.AlgorithmReducer.reduce','org.apache.hadoop.mapred.OutputCollector<.FromNodeKey,.ToNodeValues>.collect'
'org.archive.wayback.hadoop.AlphaPartitioner.getPartition','org.apache.hadoop.io.Text.toString'
'org.archive.wayback.hadoop.AlphaPartitioner.setConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.archive.wayback.hadoop.AlphaPartitioner.setPartitionPath','org.apache.hadoop.conf.Configuration.set'
'org.archive.wayback.hadoop.AlphaPartitioner.getPartitionPath','org.apache.hadoop.conf.Configuration.get'
'.AnagramReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'.AnagramSortedValuesDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputValueGroupingComparator org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'.AnagramSortedValuesDriver.main','org.apache.hadoop.util.ToolRunner.run'
'.AnagramSortedValuesOutputValueGroupingComparator.compare','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'.AnagramSortedValuesReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'hitune.analysis.mapreduce.Analysis.parseArgs','org.apache.hadoop.chukwa.conf.ChukwaConfiguration.<init> org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.chukwa.conf.ChukwaConfiguration.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.chukwa.conf.ChukwaConfiguration.addResource org.apache.hadoop.chukwa.conf.ChukwaConfiguration.get org.apache.hadoop.chukwa.conf.ChukwaConfiguration.get org.apache.hadoop.chukwa.conf.ChukwaConfiguration.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.chukwa.conf.ChukwaConfiguration.addResource org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set'
'hitune.analysis.mapreduce.Analysis.run','org.apache.hadoop.chukwa.conf.ChukwaConfiguration.get org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set org.apache.hadoop.chukwa.conf.ChukwaConfiguration.set'
'hitune.analysis.mapreduce.AnalysisConfiguration.LoadConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.iterator org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.umd.cloud9.example.bigram.AnalyzeBigramCount.main','org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.example.bigram.AnalyzeBigramRelativeFrequency.main','org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.example.bigram.AnalyzeBigramRelativeFrequencyTuple.main','org.apache.hadoop.fs.Path.<init>'
'ivory.core.util.AnnotateClueRunWithURLs.MyMapper.run','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.util.LineReader.close org.apache.hadoop.fs.FSDataOutputStream.close'
'ivory.core.util.AnnotateClueRunWithURLs.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.core.util.AnnotateClueRunWithURLs.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'ivory.core.util.AnnotateClueRunWithURLs.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'gr.ntua.cslab.distributed.job.AnonymizeJob.runAnonymization','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'gr.ntua.cslab.distributed.job.AnonymizeReducer.configure','org.apache.hadoop.mapred.JobConf.get'
'gr.ntua.cslab.distributed.job.AnonymizeReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'eu.scape_project.tb.wc.archd.tools.App.myRun','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.getCurrentValue'
'eu.scape_project.tb.wc.archd.tools.App.getTIKAtype','org.apache.hadoop.io.Text.toString'
'eu.scape_project.tb.wc.archd.tools.App.printAllInfo','org.apache.hadoop.io.Text.toString'
'eu.scape_project.tb.wc.archd.tools.App.printAllSkipNoneResponse','org.apache.hadoop.io.Text.toString'
'org.apache.hcatalog.templeton.AppConfig.AppConfig','org.apache.hadoop.util.VersionInfo.getVersion'
'org.apache.hcatalog.templeton.AppConfig.loadOneFileConfig','org.apache.hadoop.fs.Path.<init>'
'com.smartitengineering.cms.client.impl.AppTest.globalSetup','org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setClientPort org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.HBaseTestingUtility.setZkCluster org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration'
'com.smartitengineering.cms.client.impl.AppTest.globalTearDown','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown'
'com.smartitengineering.cms.client.impl.AppTest.ConfigurationModule.configure','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration'
'com.odiago.flumebase.util.AppUtils.initConfResources','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource'
'co.nubetech.hiho.mapreduce.lib.output.AppendSequenceFileOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.FileAlreadyExistsException.<init> org.apache.hadoop.mapred.InvalidJobConfException.<init>'
'co.nubetech.hiho.mapreduce.lib.output.AppendSequenceFileOutputFormat.getDefaultWorkFile','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getWorkPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getWorkPath org.apache.hadoop.fs.Path.<init>'
'co.nubetech.hiho.mapreduce.lib.output.AppendSequenceFileOutputFormat.getUniqueFile','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskID.getId'
'co.nubetech.hiho.mapreduce.lib.output.AppendTextOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.FileAlreadyExistsException.<init> org.apache.hadoop.mapred.InvalidJobConfException.<init>'
'co.nubetech.hiho.mapreduce.lib.output.AppendTextOutputFormat.getDefaultWorkFile','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getWorkPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getWorkPath org.apache.hadoop.fs.Path.<init>'
'co.nubetech.hiho.mapreduce.lib.output.AppendTextOutputFormat.getUniqueFile','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskID.getId'
'org.apache.sqoop.util.AppendUtils.append','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.delete'
'org.apache.sqoop.util.AppendUtils.getNextPartition','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.sqoop.util.AppendUtils.moveFiles','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.rename'
'org.apache.sqoop.util.AppendUtils.getTempAppendDir','org.apache.hadoop.fs.Path.<init>'
'mia.clustering.ch08.ApplesToVectors.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'com.cloudera.kitten.appmaster.ApplicationMaster.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.aquaint2.Aquaint2DocnoMappingBuilder.MyMapper.map','org.apache.hadoop.io.Text.set'
'edu.umd.cloud9.collection.aquaint2.Aquaint2DocnoMappingBuilder.MyReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.collection.aquaint2.Aquaint2DocnoMappingBuilder.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.collection.aquaint2.Aquaint2DocnoMappingBuilder.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.collection.aquaint2.Aquaint2DocnoMappingBuilder.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.aquaint2.Aquaint2DocumentInputFormat.Aquaint2DocumentRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'edu.umd.cloud9.collection.aquaint2.Aquaint2ForwardIndex.getDocument','org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.read'
'edu.umd.cloud9.collection.aquaint2.Aquaint2ForwardIndex.loadIndex','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'edu.umd.cloud9.collection.aquaint2.Aquaint2ForwardIndexBuilder.MyMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal'
'edu.umd.cloud9.collection.aquaint2.Aquaint2ForwardIndexBuilder.MyMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text>.collect'
'edu.umd.cloud9.collection.aquaint2.Aquaint2ForwardIndexBuilder.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.collection.aquaint2.Aquaint2ForwardIndexBuilder.runTool','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.findCounter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeUTF org.apache.hadoop.fs.FSDataOutputStream.writeUTF org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.util.LineReader.close org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.collection.aquaint2.Aquaint2ForwardIndexBuilder.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.protocol.shared.ArcFileItem.newInstance','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.protocol.shared.ArcFileItem.setContent','org.apache.hadoop.record.Buffer.get org.apache.hadoop.record.Buffer.getCount'
'org.commoncrawl.protocol.shared.ArcFileMetadata.newInstance','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.protocol.shared.ArcFileMetadata.setSignature','org.apache.hadoop.record.Buffer.get org.apache.hadoop.record.Buffer.getCount'
'org.commoncrawl.util.ArcFileReader.ArcFileBuilder.transitionState','org.apache.hadoop.record.Buffer.setCapacity'
'org.commoncrawl.util.ArcFileReader.ArcFileBuilder.finish','org.apache.hadoop.record.Buffer.getCount org.apache.hadoop.record.Buffer.<init>'
'org.commoncrawl.util.ArcFileReader.ArcFileBuilder.inputData','org.apache.hadoop.record.Buffer.getCapacity org.apache.hadoop.record.Buffer.getCount org.apache.hadoop.record.Buffer.getCapacity org.apache.hadoop.record.Buffer.setCapacity org.apache.hadoop.record.Buffer.append'
'org.commoncrawl.util.ArcFileReader.run','org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.util.ArcFileReader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.util.ArcFileWriter.ArcFile.restartWrite','org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create'
'org.commoncrawl.util.ArcFileWriter.ArcFile.ArcFile','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create'
'org.commoncrawl.util.ArcFileWriter.ArcFile.run','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.commoncrawl.util.ArcFileWriter.ArcFile.close','org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.util.ArcFileWriter.ArcFile.delete','org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.util.ArcFileWriter.ArcFileWriter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.util.ArcFileWriter.testArcFileWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.WritableName.setName org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.commoncrawl.util.ArcFileWriter.rotateFile','org.apache.hadoop.fs.Path.<init>'
'dk.statsbiblioteket.scape.arcunpacker.mapred.ArcInputFormatTest.testGetRecordReaderOnArc','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileSplit.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.createKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.createValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.next'
'dk.statsbiblioteket.scape.arcunpacker.mapred.ArcInputFormatTest.testGetRecordReaderOnWArc','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileSplit.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.createKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.createValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.next'
'org.apache.nutch.tools.arc.ArcInputFormat.getRecordReader','org.apache.hadoop.mapred.InputSplit.toString org.apache.hadoop.mapred.Reporter.setStatus'
'dk.statsbiblioteket.scape.arcunpacker.mapreduce.ArcRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.Text.<init>'
'dk.statsbiblioteket.scape.arcunpacker.mapreduce.ArcRecordReader.getCurrentKey','org.apache.hadoop.io.Text.set'
'dk.statsbiblioteket.scape.arcunpacker.mapreduce.ArcRecordReaderTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>'
'dk.statsbiblioteket.scape.arcunpacker.mapreduce.ArcRecordReaderTest.testNextKeyValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,dk.statsbiblioteket.scape.arcunpacker.HadoopArcRecord>.getCurrentValue org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.nutch.tools.arc.ArcRecordReader.ArcRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.fs.FSDataInputStream.seek'
'org.apache.nutch.tools.arc.ArcRecordReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.nutch.tools.arc.ArcRecordReader.createKey','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.nutch.tools.arc.ArcRecordReader.createValue','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.nutch.tools.arc.ArcRecordReader.getPos','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.nutch.tools.arc.ArcRecordReader.next','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.hadoop.mapred.ArcRecordReader.ArcRecordReader','org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.commoncrawl.hadoop.mapred.ArcRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.hadoop.mapred.ArcRecordReader.next','org.apache.hadoop.io.Text.set'
'org.commoncrawl.nutch.tools.arc.ArcRecordReader.ArcRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.fs.FSDataInputStream.seek'
'org.commoncrawl.nutch.tools.arc.ArcRecordReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.nutch.tools.arc.ArcRecordReader.createKey','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.nutch.tools.arc.ArcRecordReader.createValue','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.nutch.tools.arc.ArcRecordReader.getPos','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.commoncrawl.nutch.tools.arc.ArcRecordReader.next','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.arc.ArcRecordReader.ArcRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.fs.FSDataInputStream.seek'
'org.apache.nutch.tools.arc.ArcRecordReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.nutch.tools.arc.ArcRecordReader.createKey','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.nutch.tools.arc.ArcRecordReader.createValue','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.nutch.tools.arc.ArcRecordReader.getPos','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.nutch.tools.arc.ArcRecordReader.next','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.arc.ArcSegmentCreator.configure','org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.nutch.tools.arc.ArcSegmentCreator.output','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.io.Text.equals org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.arc.ArcSegmentCreator.logError','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.arc.ArcSegmentCreator.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.mapred.Reporter.progress'
'org.apache.nutch.tools.arc.ArcSegmentCreator.createSegments','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.tools.arc.ArcSegmentCreator.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.tools.arc.ArcSegmentCreator.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.arc.ArcSegmentCreator.configure','org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.nutch.tools.arc.ArcSegmentCreator.output','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.io.Text.equals org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.arc.ArcSegmentCreator.logError','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.arc.ArcSegmentCreator.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.mapred.Reporter.progress'
'org.apache.nutch.tools.arc.ArcSegmentCreator.createSegments','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.tools.arc.ArcSegmentCreator.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.tools.arc.ArcSegmentCreator.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'uk.bl.wap.hadoop.ArchiveFileRecordReader.ArchiveFileRecordReader','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.MultiFileSplit.getPaths'
'uk.bl.wap.hadoop.ArchiveFileRecordReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'uk.bl.wap.hadoop.ArchiveFileRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'uk.bl.wap.hadoop.ArchiveFileRecordReader.getPos','org.apache.hadoop.fs.FSDataInputStream.getPos'
'uk.bl.wap.hadoop.ArchiveFileRecordReader.getProgress','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FileStatus.getLen'
'uk.bl.wap.hadoop.ArchiveFileRecordReader.next','org.apache.hadoop.io.Text.set'
'uk.bl.wap.hadoop.ArchiveFileRecordReader.nextFile','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'com.splunk.shuttl.archiver.filesystem.ArchiveFileSystemFactory.createHadoopFileSystem','org.apache.hadoop.fs.Path.<init>'
'com.splunk.shuttl.archiver.filesystem.ArchiveFileSystemFactory.getHadoopFileSystemSafe','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'uk.bl.wap.hadoop.tika.ArchiveTikaMapper.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'uk.bl.wap.hadoop.tika.ArchiveTikaMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,uk.bl.wap.util.solr.WritableSolrRecord>.collect'
'uk.bl.wap.hadoop.tika.ArchiveTikaReducer.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'uk.bl.wap.hadoop.tika.ArchiveTikaReducer.reduce','org.apache.hadoop.io.Text.toString'
'uk.bl.wap.hadoop.mapreduce.lib.ArchiveToCDXFileInputFormat.getSplits','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.getSplits'
'edu.ucsc.srl.damasc.netcdf.io.input.ArrayBasedFileSplit.ArrayBasedFileSplit','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init>'
'edu.ucsc.srl.damasc.netcdf.io.input.ArrayBasedFileSplit.write','org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.writeString'
'edu.ucsc.srl.damasc.netcdf.io.input.ArrayBasedFileSplit.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.fs.Path.<init>'
'edu.ucsc.srl.damasc.netcdf.io.input.ArrayBasedFileSplit.toString','org.apache.hadoop.fs.Path.getName'
'edu.umd.cloud9.io.array.ArrayListOfFloatsWritableTest.testReadWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.io.array.ArrayListOfIntsWritableTest.testReadWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.io.array.ArrayListOfIntsWritableTest.testIO','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.io.array.ArrayListOfLongsWritableTest.testReadWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.io.array.ArrayListOfShortsWritableTest.testReadWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.giraph.utils.ArrayListWritable.readFields','org.apache.hadoop.util.ReflectionUtils.newInstance'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testBasic','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testSerialize1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testSerialize2','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testToString','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testClear','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testEmpty','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testListMethods','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testSorting1','org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testSorting2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableComparableTest.testSorting3','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableTest.testBasic','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableTest.testSerialize1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableTest.testSerialize2','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableTest.testSerialize3','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.io.array.ArrayListWritableTest.testToString','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableTest.testClear','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableTest.testEmpty','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.array.ArrayListWritableTest.testListMethods','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.manning.hip.ch13.ArrayOutOfBoundsImproved.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch13.ArrayOutOfBoundsImproved.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'edu.ucsc.srl.damasc.netcdf.io.ArraySpec.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'edu.ucsc.srl.damasc.netcdf.io.ArraySpec.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'.ArrayWritableTest.test','org.apache.hadoop.io.ArrayWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.ArrayWritable.set org.apache.hadoop.io.WritableUtils.cloneInto org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.rpc.base.internal.AsyncClientChannel.Excepted','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.rpc.base.internal.AsyncClientChannel.Readable','org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.rpc.base.internal.AsyncClientChannel.readResponseFrames','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.sqoop.mapreduce.AsyncSqlOutputFormat.AsyncSqlExecThread.run','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.sqoop.mapreduce.AsyncSqlOutputFormat.AsyncSqlExecThread.setLastError','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.server.AsyncWebServerRequest.timerFired','org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.science.matching.graph.AuctionMessage.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields'
'com.cloudera.science.matching.graph.AuctionMessage.write','org.apache.hadoop.io.Text.write'
'com.cloudera.science.matching.graph.AuctionMessage.compareTo','org.apache.hadoop.io.Text.hashCode'
'com.cloudera.science.matching.graph.AuctionMessage.toString','org.apache.hadoop.io.Text.toString'
'org.apache.oozie.action.hadoop.AuthHelper.get','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.oozie.client.AuthOozieClient.createConnection','org.apache.hadoop.security.authentication.client.AuthenticatedURL.injectToken org.apache.hadoop.security.authentication.client.AuthenticatedURL.<init> org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection org.apache.hadoop.security.authentication.client.AuthenticationException.getMessage org.apache.hadoop.security.authentication.client.AuthenticatedURL.injectToken'
'org.apache.oozie.client.AuthOozieClient.getAuthenticator','org.apache.hadoop.security.authentication.client.KerberosAuthenticator.getName'
'org.apache.oozie.service.AuthorizationService.authorizeForApp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.service.AuthorizationService.authorizeForApp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open'
'org.apache.nutch.urlfilter.automaton.AutomatonURLFilter.getRulesReader','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getConfResourceAsReader'
'org.apache.hama.graph.AverageAggregator.finalizeAggregation','org.apache.hadoop.io.DoubleWritable.<init>'
'edu.ucsc.srl.damasc.netcdf.map.AverageMapper.map','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.set'
'edu.ucsc.srl.damasc.netcdf.reduce.AverageReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.IntWritable.set'
'de.tuberlin.dima.aim3.assignment1.AverageTemperaturePerMonthTest.countWords','org.apache.hadoop.conf.Configuration.<init>'
'de.tuberlin.dima.aim.exercises.one.AverageTemperaturePerMonthTest.countWords','org.apache.hadoop.conf.Configuration.<init>'
'de.tuberlin.dima.aim.exercises.one.AverageTemperaturePerMonth.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.manning.hip.ch3.avro.AvroBytesRecord.toGenericRecord','org.apache.hadoop.io.Writable.write'
'com.manning.hip.ch3.avro.AvroBytesRecord.fromGenericRecord','org.apache.hadoop.io.Writable.readFields'
'com.linkedin.haivvreo.AvroContainerOutputFormat.getHiveRecordWriter','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.create','org.apache.hadoop.io.BooleanWritable.isAssignableFrom org.apache.hadoop.io.BytesWritable.isAssignableFrom org.apache.hadoop.io.ByteWritable.isAssignableFrom org.apache.hadoop.io.DoubleWritable.isAssignableFrom org.apache.hadoop.io.FloatWritable.isAssignableFrom org.apache.hadoop.io.IntWritable.isAssignableFrom org.apache.hadoop.io.LongWritable.isAssignableFrom org.apache.hadoop.io.NullWritable.isAssignableFrom org.apache.hadoop.io.Text.isAssignableFrom'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.BooleanWritableConverter.convert','org.apache.hadoop.io.BooleanWritable.get'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.BytesWritableConverter.convert','org.apache.hadoop.io.BytesWritable.getBytes'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.ByteWritableConverter.convert','org.apache.hadoop.io.ByteWritable.get'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.DoubleWritableConverter.convert','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.FloatWritableConverter.convert','org.apache.hadoop.io.FloatWritable.get'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.IntWritableConverter.convert','org.apache.hadoop.io.IntWritable.get'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.LongWritableConverter.convert','org.apache.hadoop.io.LongWritable.get'
'org.apache.avro.hadoop.io.AvroDatumConverterFactory.TextConverter.convert','org.apache.hadoop.io.Text.toString'
'com.linkedin.haivvreo.AvroDeserializer.worker','org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory'
'com.linkedin.haivvreo.AvroDeserializer.deserializeStruct','org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldTypeInfos org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldNames'
'com.linkedin.haivvreo.AvroDeserializer.deserializeList','org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo'
'com.linkedin.haivvreo.AvroDeserializer.deserializeMap','org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapValueTypeInfo'
'org.apache.sqoop.mapreduce.AvroExportMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.DefaultStringifier.load'
'org.apache.sqoop.mapreduce.AvroExportMapper.map','org.apache.hadoop.io.NullWritable.get'
'org.apache.sqoop.mapreduce.AvroExportMapper.toSqoopRecord','org.apache.hadoop.io.MapWritable.entrySet'
'org.apache.sqoop.mapreduce.AvroExportMapper.fromAvro','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.<init>'
'com.tomslabs.grid.avro.AvroFileOutputFormat.setDeflateLevel','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.tomslabs.grid.avro.AvroFileOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create'
'com.tomslabs.grid.avro.AvroFileOutputFormat.getWriteSchema','org.apache.hadoop.conf.Configuration.get'
'com.tomslabs.grid.avro.AvroFileOutputFormat.getDatumWriter','org.apache.hadoop.conf.Configuration.get'
'org.apache.crunch.io.avro.AvroFileReaderFactory.read','org.apache.hadoop.fs.FileSystem.getConf'
'org.apache.crunch.io.avro.AvroFileReaderFactoryTest.testRead_GenericReader','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.avro.AvroFileReaderFactoryTest.testRead_SpecificReader','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.avro.AvroFileReaderFactoryTest.testRead_ReflectReader','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.avro.AvroFileSourceTest.setUp','org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.crunch.io.avro.AvroFileSourceTest.testConfigureJob_SpecificData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.crunch.io.avro.AvroFileSourceTest.testConfigureJob_GenericData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration'
'tap.formats.avro.AvroFormat.setupOutput','org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass'
'tap.formats.avro.AvroFormat.setupInput','org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set'
'com.manning.hip.ch3.avro.AvroGenericFileDumper.readFromAvro','org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.avro.AvroGenericFileDumper.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'tap.formats.avro.AvroGroupPartitioner.configure','org.apache.hadoop.mapred.JobConf.get'
'org.apache.crunch.types.avro.AvroGroupedTableType.configureShuffle','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.mapreduce.AvroImportMapper.setup','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getWorkOutputPath'
'org.apache.sqoop.mapreduce.AvroImportMapper.map','org.apache.hadoop.io.NullWritable.get'
'org.apache.sqoop.mapreduce.AvroImportMapper.toAvro','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'org.apache.avro.mapred.AvroInputFormat.listStatus','org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.avro.mapred.AvroInputFormat.getRecordReader','org.apache.hadoop.mapred.InputSplit.toString org.apache.hadoop.mapred.Reporter.setStatus'
'org.apache.sqoop.mapreduce.AvroJob.setMapOutputSchema','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.AvroJob.getMapOutputSchema','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.AvroJob.getOutputSchema','org.apache.hadoop.conf.Configuration.get'
'com.wibidata.avro.mapreduce.AvroKeyInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.avro.mapreduce.AvroKeyValueOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getOutputKeyClass org.apache.hadoop.mapreduce.TaskAttemptContext.getOutputValueClass'
'org.apache.avro.mapreduce.AvroMapReduceTest.shouldRunMapReduceJobWithNonAvroInput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.avro.mapreduce.AvroMapReduceTest.shouldRunMapperOnlyJobWithNonAvroInput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.avro.mapreduce.AvroMapReduceTest.shouldRunMapperOnlyJobWithAvroInput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.avro.mapreduce.AvroMapReduceTest.shouldRunMapReduceJobWithAvroInput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.avro.mapreduce.AvroMapReduceTest.WordCountingTextToAvroMapper.map','org.apache.hadoop.io.Text.toString'
'org.apache.avro.mapreduce.AvroMapReduceTest.WordCountingTextToAvroMapperOnlyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'org.apache.avro.mapreduce.AvroMultipleOutputs.getNamedOutputsList','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.avro.mapreduce.AvroMultipleOutputs.getNamedOutputFormatClass','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.avro.mapreduce.AvroMultipleOutputs.addNamedOutput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setClass'
'org.apache.avro.mapreduce.AvroMultipleOutputs.setCountersEnabled','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.avro.mapreduce.AvroMultipleOutputs.getCountersEnabled','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.avro.mapreduce.AvroMultipleOutputs.RecordWriterWithCounter.write','org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter org.apache.hadoop.mapreduce.RecordWriter.write'
'org.apache.avro.mapreduce.AvroMultipleOutputs.RecordWriterWithCounter.close','org.apache.hadoop.mapreduce.RecordWriter.close'
'org.apache.avro.mapreduce.AvroMultipleOutputs.write','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.RecordWriter.write org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getConfiguration org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getTaskAttemptID org.apache.hadoop.mapreduce.RecordWriter.write'
'org.apache.avro.mapreduce.AvroMultipleOutputs.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getOutputFormatClass org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.OutputFormat.getRecordWriter'
'org.apache.avro.mapreduce.AvroMultipleOutputs.getContext','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getConfiguration org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getTaskAttemptID'
'org.apache.avro.mapreduce.AvroMultipleOutputs.close','org.apache.hadoop.mapreduce.RecordWriter.close'
'org.apache.sqoop.mapreduce.AvroOutputFormat.configureDataFileWriter','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getCompressOutput org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.sqoop.mapreduce.AvroOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getNumReduceTasks org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.RecordWriter<org.apache.avro.mapred.AvroWrapper<org.apache.sqoop.mapreduce.T>,org.apache.hadoop.io.NullWritable>.<init>'
'com.datasalt.pangool.tuplemr.avro.AvroOutputFormat.setDeflateLevel','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.datasalt.pangool.tuplemr.avro.AvroOutputFormat.setSyncInterval','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.datasalt.pangool.tuplemr.avro.AvroOutputFormat.configureDataFileWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getCompressOutput org.apache.hadoop.conf.Configuration.getInt'
'com.datasalt.pangool.tuplemr.avro.AvroOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.RecordWriter<org.apache.avro.mapred.AvroWrapper<com.datasalt.pangool.tuplemr.avro.T>,org.apache.hadoop.io.NullWritable>.<init>'
'org.apache.avro.mapreduce.lib.output.AvroOutputFormat.setDeflateLevel','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.avro.mapreduce.lib.output.AvroOutputFormat.setSyncInterval','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.avro.mapreduce.lib.output.AvroOutputFormat.configureDataFileWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getCompressOutput org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'org.apache.avro.mapreduce.lib.output.AvroOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getNumReduceTasks org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.RecordWriter<org.apache.avro.mapred.AvroWrapper<org.apache.avro.mapreduce.lib.output.T>,org.apache.hadoop.io.NullWritable>.<init>'
'.AvroProjection.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob'
'.AvroProjection.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.avro.mapreduce.AvroRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'org.apache.avro.mapreduce.AvroRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.avro.mapred.AvroRecordReader.AvroRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength'
'org.apache.avro.mapred.AvroRecordReader.createValue','org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.tuplemr.avro.AvroRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'com.datasalt.pangool.tuplemr.avro.AvroRecordReader.init','org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'com.datasalt.pangool.tuplemr.avro.AvroRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.avro.mapreduce.lib.input.AvroRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.avro.mapreduce.lib.input.AvroRecordReader.init','org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'org.apache.avro.mapreduce.lib.input.AvroRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'com.tomslabs.grid.avro.AvroRecordToJSONStringTest.test','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileSplit.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.datasalt.pangool.utils.AvroRecordToTupleConverter.toTuple','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.serializer.Deserializer.open org.apache.hadoop.io.serializer.Deserializer.deserialize org.apache.hadoop.io.serializer.Deserializer.close org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.avro.mapreduce.AvroReducer.write','org.apache.hadoop.io.NullWritable.get'
'org.lilyproject.repository.impl.test.AvroRepositoryTest.setUpBeforeClass','org.apache.hadoop.fs.Path.<init>'
'com.maxpoint.cascading.avro.AvroScheme.sourceInit','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat'
'com.maxpoint.cascading.avro.AvroScheme.fromAvro','org.apache.hadoop.io.BytesWritable.<init>'
'com.maxpoint.cascading.avro.AvroScheme.bytesWritable','org.apache.hadoop.io.BytesWritable.<init>'
'com.maxpoint.cascading.avro.AvroScheme.sinkInit','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass'
'com.maxpoint.cascading.avro.AvroScheme.addAvroSerialization','org.apache.hadoop.mapred.JobConf.getStringCollection org.apache.hadoop.mapred.JobConf.setStrings'
'com.maxpoint.cascading.avro.AvroScheme.sink','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector.collect'
'com.maxpoint.cascading.avro.AvroScheme.toAvro','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getBytes'
'org.apache.avro.hadoop.io.AvroSequenceFile.createWriter','org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.avro.hadoop.io.AvroSequenceFile.Writer.Options.Options','org.apache.hadoop.io.SequenceFile.Metadata.<init>'
'org.apache.avro.hadoop.io.AvroSequenceFile.Writer.Options.getConfigurationWithAvroSerialization','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.avro.hadoop.io.AvroSequenceFile.Writer.Options.getBufferSizeBytes','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.avro.hadoop.io.AvroSequenceFile.Writer.Options.getReplicationFactor','org.apache.hadoop.fs.FileSystem.getDefaultReplication'
'org.apache.avro.hadoop.io.AvroSequenceFile.Writer.Options.getBlockSizeBytes','org.apache.hadoop.fs.FileSystem.getDefaultBlockSize'
'org.apache.avro.hadoop.io.AvroSequenceFile.Writer.Options.getMetadataWithAvroSchemas','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Metadata.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Metadata.set'
'org.apache.avro.hadoop.io.AvroSequenceFile.Reader.Options.getConfigurationWithAvroSerialization','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.SequenceFile.Metadata.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.SequenceFile.Metadata.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.wibidata.avro.mapreduce.AvroSequenceFileInputFormat.AvroSequenceFileRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart'
'org.apache.avro.mapreduce.AvroSequenceFileInputFormat.AvroSequenceFileRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart'
'org.apache.avro.mapreduce.AvroSequenceFileOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.TaskAttemptContext.getOutputKeyClass org.apache.hadoop.mapreduce.TaskAttemptContext.getOutputValueClass org.apache.hadoop.mapreduce.RecordWriter<org.apache.avro.mapreduce.K,org.apache.avro.mapreduce.V>.<init>'
'org.apache.avro.mapreduce.AvroSequenceFileOutputFormat.setOutputCompressionType','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.SequenceFile.CompressionType.name'
'org.apache.avro.mapreduce.AvroSequenceFileOutputFormat.getOutputCompressionType','org.apache.hadoop.io.SequenceFile.CompressionType.name org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.SequenceFile.CompressionType.valueOf'
'org.apache.avro.hadoop.io.AvroSerialization.getDeserializer','org.apache.hadoop.conf.Configuration.getClassLoader org.apache.hadoop.conf.Configuration.getClassLoader'
'org.apache.avro.hadoop.io.AvroSerialization.addToConfiguration','org.apache.hadoop.conf.Configuration.getStringCollection org.apache.hadoop.conf.Configuration.setStrings'
'org.apache.avro.hadoop.io.AvroSerialization.setKeyWriterSchema','org.apache.hadoop.conf.Configuration.set'
'org.apache.avro.hadoop.io.AvroSerialization.setKeyReaderSchema','org.apache.hadoop.conf.Configuration.set'
'org.apache.avro.hadoop.io.AvroSerialization.setValueWriterSchema','org.apache.hadoop.conf.Configuration.set'
'org.apache.avro.hadoop.io.AvroSerialization.setValueReaderSchema','org.apache.hadoop.conf.Configuration.set'
'org.apache.avro.hadoop.io.AvroSerialization.getKeyWriterSchema','org.apache.hadoop.conf.Configuration.get'
'org.apache.avro.hadoop.io.AvroSerialization.getKeyReaderSchema','org.apache.hadoop.conf.Configuration.get'
'org.apache.avro.hadoop.io.AvroSerialization.getValueWriterSchema','org.apache.hadoop.conf.Configuration.get'
'org.apache.avro.hadoop.io.AvroSerialization.getValueReaderSchema','org.apache.hadoop.conf.Configuration.get'
'com.linkedin.haivvreo.AvroSerializer.serialize','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldsDataAsList org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory'
'com.linkedin.haivvreo.AvroSerializer.serializeStruct','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldsDataAsList org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldTypeInfos org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector'
'com.linkedin.haivvreo.AvroSerializer.serializePrimitive','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject'
'com.linkedin.haivvreo.AvroSerializer.serializeUnion','org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector.getTag org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector.getObjectInspectors org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector.getField'
'com.linkedin.haivvreo.AvroSerializer.extraByteArray','org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListLength org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElementObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElementObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getList org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject'
'com.linkedin.haivvreo.AvroSerializer.serializeList','org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getList org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElementObjectInspector'
'com.linkedin.haivvreo.AvroSerializer.serializeMap','org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapKeyObjectInspector org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.toString org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapKeyObjectInspector org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapValueObjectInspector org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapKeyTypeInfo org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapValueTypeInfo org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMap org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapSize'
'com.linkedin.haivvreo.AvroSerializer.mapHasStringKey','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory'
'.AvroSort.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'.AvroSort.main','org.apache.hadoop.util.ToolRunner.run'
'com.manning.hip.ch3.avro.AvroStockAvgFileRead.readFromAvro','org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.avro.AvroStockAvgFileRead.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch3.avro.AvroStockFileRead.readFromAvro','org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.avro.AvroStockFileRead.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch3.avro.AvroStockMapReduce.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.io.compress.SnappyCodec.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'com.manning.hip.ch3.avro.AvroStockMapReduce.Map.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.collect'
'com.manning.hip.ch3.avro.AvroStockMapReduce.Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.avro.mapred.AvroWrapper<com.manning.hip.ch3.avro.gen.StockAvg>,org.apache.hadoop.io.NullWritable>.collect'
'org.apache.pig.piggybank.storage.avro.AvroStorageInputStream.AvroStorageInputStream','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.pig.piggybank.storage.avro.AvroStorageInputStream.read','org.apache.hadoop.fs.FSDataInputStream.read'
'org.apache.pig.piggybank.storage.avro.AvroStorageInputStream.seek','org.apache.hadoop.fs.FSDataInputStream.seek'
'org.apache.pig.piggybank.storage.avro.AvroStorageInputStream.tell','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.pig.piggybank.storage.avro.AvroStorageInputStream.close','org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.pig.piggybank.storage.avro.AvroStorageUtils.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'org.apache.pig.piggybank.storage.avro.AvroStorageUtils.addInputPaths','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getInputPaths org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'org.apache.pig.piggybank.storage.avro.AvroStorageUtils.getAllSubDirs','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.pig.piggybank.storage.avro.AvroStorageUtils.noDir','org.apache.hadoop.fs.FileStatus.isDir'
'org.apache.pig.piggybank.storage.avro.AvroStorageUtils.getLast','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'voldemort.store.readonly.mr.AvroStoreBuilderPartitioner.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.avro.mapred.AvroTextOutputFormat.getRecordWriter','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.avro.mapred.AvroTextOutputFormat.AvroTextRecordWriter.toByteBuffer','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'com.tomslabs.grid.avro.AvroTextRecordReader.AvroTextRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength'
'com.tomslabs.grid.avro.AvroTextRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'com.tomslabs.grid.avro.AvroTextRecordReader.createValue','org.apache.hadoop.io.Text.<init>'
'com.tomslabs.grid.avro.AvroTextRecordReader.next','org.apache.hadoop.io.Text.set'
'org.lilyproject.repository.impl.test.AvroTypeManagerFieldTypeTest.setUpBeforeClass','org.apache.hadoop.fs.Path.<init>'
'org.apache.avro.mapred.AvroUtf8InputFormat.Utf8LineRecordReader.Utf8LineRecordReader','org.apache.hadoop.mapred.LineRecordReader.<init>'
'org.apache.avro.mapred.AvroUtf8InputFormat.Utf8LineRecordReader.close','org.apache.hadoop.mapred.LineRecordReader.close'
'org.apache.avro.mapred.AvroUtf8InputFormat.Utf8LineRecordReader.getPos','org.apache.hadoop.mapred.LineRecordReader.getPos'
'org.apache.avro.mapred.AvroUtf8InputFormat.Utf8LineRecordReader.getProgress','org.apache.hadoop.mapred.LineRecordReader.getProgress'
'org.apache.avro.mapred.AvroUtf8InputFormat.Utf8LineRecordReader.next','org.apache.hadoop.mapred.LineRecordReader.next org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.avro.mapred.AvroUtf8InputFormat.Utf8LineRecordReader.createValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.avro.mapred.AvroUtf8InputFormat.configure','org.apache.hadoop.io.compress.CompressionCodecFactory.<init>'
'org.apache.avro.mapred.AvroUtf8InputFormat.isSplitable','org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'org.apache.avro.mapred.AvroUtf8InputFormat.getRecordReader','org.apache.hadoop.mapred.InputSplit.toString org.apache.hadoop.mapred.Reporter.setStatus'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.Utf8LineRecordReader.Utf8LineRecordReader','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.<init>'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.Utf8LineRecordReader.close','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.Utf8LineRecordReader.getProgress','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.Utf8LineRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue org.apache.hadoop.io.Text.toString'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.Utf8LineRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.Utf8LineRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.Utf8LineRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.configure','org.apache.hadoop.io.compress.CompressionCodecFactory.<init>'
'org.apache.crunch.types.avro.AvroUtf8InputFormat.isSplitable','org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'voldemort.store.readonly.mr.utils.AvroUtils.getSchemaFromPath','org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.isDirectory org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'voldemort.store.readonly.mr.utils.AvroUtils.getAvroSchemaFromPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.crunch.types.avro.AvrosTest.testWritables','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.accumulo.core.file.rfile.bcfile.BCFile.Writer.WBlockState.WBlockState','org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.io.BytesWritable.setCapacity org.apache.hadoop.io.BytesWritable.getBytes'
'org.apache.accumulo.core.file.rfile.bcfile.BCFile.Writer.WBlockState.getCurrentPos','org.apache.hadoop.fs.FSDataOutputStream.getPos'
'org.apache.accumulo.core.file.rfile.bcfile.BCFile.Writer.Writer','org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.io.BytesWritable.<init>'
'org.apache.accumulo.core.file.rfile.bcfile.BCFile.Writer.close','org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.fs.FSDataOutputStream.flush'
'org.apache.accumulo.core.file.rfile.bcfile.BCFile.Reader.Reader','org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.seek'
'.BMTColumnLoader.Map.map','org.apache.hadoop.io.Text.toString'
'.BMTColumnLoader.Map.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'.BMTColumnLoader.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'.BMTColumnLoader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'.BMTKeyValueLoader.Map.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'.BMTKeyValueLoader.Reduce.reduce','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getBytes'
'.BMTKeyValueLoader.Reduce.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'.BMTKeyValueLoader.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'.BMTKeyValueLoader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'.BMTTableLoader.Map.map','org.apache.hadoop.io.Text.toString'
'.BMTTableLoader.Map.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'.BMTTableLoader.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'.BMTTableLoader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.mongodb.hadoop.io.BSONFile.Writer.Writer','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getDefaultReplication org.apache.hadoop.fs.FileSystem.getDefaultBlockSize org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getDefaultReplication org.apache.hadoop.fs.FileSystem.getDefaultBlockSize org.apache.hadoop.fs.FileSystem.create'
'com.mongodb.hadoop.io.BSONFile.Writer.close','org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FSDataOutputStream.flush'
'com.mongodb.hadoop.io.BSONFile.Writer.append','org.apache.hadoop.io.DataOutputBuffer.reset'
'com.mongodb.hadoop.io.BSONFile.Writer.getLength','org.apache.hadoop.fs.FSDataOutputStream.getPos'
'com.mongodb.hadoop.BSONFileOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.RecordWriter<com.mongodb.hadoop.K,com.mongodb.hadoop.V>.<init>'
'com.mongodb.hadoop.hive.BSONSerde.initialize','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector org.apache.hadoop.hive.serde2.SerDeStats.<init>'
'com.mongodb.hadoop.hive.BSONSerde.deserialize','org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.Writable.getClass org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName'
'com.mongodb.hadoop.hive.BSONSerde.getSerDeStats','org.apache.hadoop.hive.serde2.SerDeStats.setRawDataSize'
'com.mongodb.hadoop.io.BSONWritable.copy','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Writable.write org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'com.mongodb.hadoop.io.BSONWritable.toBSON','org.apache.hadoop.io.ArrayWritable.get org.apache.hadoop.io.BooleanWritable.get org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.ByteWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.IntWritable.get'
'org.apache.hama.bsp.BSPJobClient.init','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.net.NetUtils.getSocketFactory org.apache.hadoop.ipc.RPC.getProxy'
'org.apache.hama.bsp.BSPJobClient.close','org.apache.hadoop.ipc.RPC.stopProxy'
'org.apache.hama.bsp.BSPJobClient.getFs','org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.hama.bsp.BSPJobClient.submitJobInternal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FileSystem.setReplication org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.hama.bsp.BSPJobClient.launchJob','org.apache.hadoop.fs.Path.makeQualified'
'org.apache.hama.bsp.BSPJobClient.partition','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.hama.bsp.BSPJobClient.getOutputCompressionType','org.apache.hadoop.io.SequenceFile.CompressionType.valueOf'
'org.apache.hama.bsp.BSPJobClient.getOutputCompressorClass','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getClassByName'
'org.apache.hama.bsp.BSPJobClient.writeSplits','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.apache.hama.bsp.BSPJobClient.writeSplitsFileHeader','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt'
'org.apache.hama.bsp.BSPJobClient.readSplitFile','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.hama.bsp.BSPJobClient.getSystemDir','org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.BSPJobClient.run','org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.BSPJobClient.executeShellCommand','org.apache.hadoop.util.Shell.execCommand'
'org.apache.hama.bsp.BSPJobClient.RawSplit.setBytes','org.apache.hadoop.io.BytesWritable.set'
'org.apache.hama.bsp.BSPJobClient.RawSplit.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.BytesWritable.readFields org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Text.readString'
'org.apache.hama.bsp.BSPJobClient.RawSplit.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.BytesWritable.write org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.Text.writeString'
'org.apache.hama.bsp.BSPJobClient.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hama.bsp.BSPJobContext.BSPJobContext','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.addResource'
'org.apache.hama.bsp.BSPJobContext.getWorkingDirectory','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.hama.bsp.BSPJobContext.getJobName','org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.bsp.BSPJobContext.getJar','org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.bsp.BSPJobContext.getLocalPath','org.apache.hadoop.conf.Configuration.getLocalPath'
'org.apache.hama.bsp.BSPJobContext.getUser','org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.bsp.BSPJobContext.writeXml','org.apache.hadoop.conf.Configuration.writeXml'
'org.apache.hama.bsp.BSPJobContext.get','org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.bsp.BSPJobContext.getInt','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.hama.bsp.BSPMaster.BSPMaster','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.hama.bsp.BSPMaster.register','org.apache.hadoop.ipc.RPC.waitForProxy'
'org.apache.hama.bsp.BSPMaster.getSystemDirectoryForJob','org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.BSPMaster.deleteLocalFiles','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.BSPMaster.getAddress','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.net.NetUtils.createSocketAddr'
'org.apache.hama.bsp.BSPMaster.offerService','org.apache.hadoop.ipc.Server.start org.apache.hadoop.ipc.Server.join'
'org.apache.hama.bsp.BSPMaster.submitJob','org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.BSPMaster.getFilesystemName','org.apache.hadoop.fs.FileSystem.getUri'
'org.apache.hama.bsp.BSPMaster.getSystemDir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified'
'org.apache.hama.bsp.BSPMaster.shutdown','org.apache.hadoop.ipc.Server.stop'
'org.apache.hama.BSPMasterRunner.run','org.apache.hadoop.util.StringUtils.startupShutdownMessage org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.hama.BSPMasterRunner.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hama.bsp.BSPMessageBundle.readFields','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.hama.bsp.message.compress.BSPMessageCompressorFactory.getCompressor','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getClassByName org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.hama.bsp.BSPRunner.BSPRunner','org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.ipc.RPC.getProxy org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getClassByName'
'org.apache.hama.bsp.BSPRunner.startComputation','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.hama.bsp.BSPRunner.main','org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.BSPTask.runBSP','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.hama.bsp.BSPTask.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.BytesWritable.write'
'org.apache.hama.bsp.BSPTask.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.readFields'
'org.apache.hama.bsp.BSPTaskLauncher.BSPTaskLauncher','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.bsp.BSPTaskLauncher.stopAndCleanup','org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.Container.getId org.apache.hadoop.yarn.api.protocolrecords.StopContainerRequest.setContainerId org.apache.hadoop.yarn.api.ContainerManager.stopContainer'
'org.apache.hama.bsp.BSPTaskLauncher.start','org.apache.hadoop.yarn.api.records.Container.getId'
'org.apache.hama.bsp.BSPTaskLauncher.poll','org.apache.hadoop.yarn.api.ContainerManager.getContainerStatus org.apache.hadoop.yarn.api.records.ContainerStatus.getState org.apache.hadoop.yarn.api.records.ContainerStatus.getExitStatus org.apache.hadoop.yarn.api.records.ContainerStatus.getDiagnostics org.apache.hadoop.yarn.api.records.ContainerStatus.getExitStatus'
'org.apache.hama.bsp.BSPTaskLauncher.setupContainer','org.apache.hadoop.yarn.api.records.Container.getId org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.Container.getId org.apache.hadoop.yarn.api.records.ContainerLaunchContext.setContainerId org.apache.hadoop.yarn.api.records.Container.getResource org.apache.hadoop.yarn.api.records.ContainerLaunchContext.setResource org.apache.hadoop.yarn.api.records.ContainerLaunchContext.setUser org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.yarn.util.ConverterUtils.getYarnUrlFromPath org.apache.hadoop.yarn.api.records.URL.toString org.apache.hadoop.yarn.util.ConverterUtils.getPathFromYarnURL org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.yarn.api.records.LocalResource.setResource org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.yarn.api.records.LocalResource.setSize org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.yarn.api.records.LocalResource.setTimestamp org.apache.hadoop.yarn.api.records.LocalResource.setType org.apache.hadoop.yarn.api.records.LocalResource.setVisibility org.apache.hadoop.yarn.api.records.LocalResource.getResource org.apache.hadoop.yarn.api.records.ContainerLaunchContext.setLocalResources org.apache.hadoop.yarn.api.records.URL.getScheme org.apache.hadoop.yarn.api.records.URL.getScheme org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.yarn.api.records.ContainerLaunchContext.setCommands org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest.setContainerLaunchContext org.apache.hadoop.yarn.api.ContainerManager.startContainer org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.Container.getId org.apache.hadoop.yarn.api.protocolrecords.GetContainerStatusRequest.setContainerId'
'.BTraceJobProfiler.onJobClient_submitJobInternal_entry','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.setBoolean'
'.BTraceJobProfiler.onJobClient_writeNewSplits_getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get'
'.BTraceJobProfiler.onJob_waitForCompletion_return','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get'
'.BTraceJobProfiler.onJobClient_runJob_return','org.apache.hadoop.conf.Configuration.get'
'.BTraceJobProfiler.onJobControl_addToQueue_entry','org.apache.hadoop.conf.Configuration.get'
'.BTraceOldApiTaskAndMemProfile.onTaskReporter_setInputSplit_Entry','org.apache.hadoop.mapred.FileSplit.getPath'
'.BTraceOldApiTaskAndMemProfile.onReduceCopier_configureClasspath_return','org.apache.hadoop.conf.Configuration.getBoolean'
'.BTraceOldApiTaskProfile.onTaskReporter_setInputSplit_Entry','org.apache.hadoop.mapred.FileSplit.getPath'
'.BTraceOldApiTaskProfile.onReduceCopier_configureClasspath_return','org.apache.hadoop.conf.Configuration.getBoolean'
'pl.edu.icm.coansys.importers.admin.BWMetaCollectionSplitAlgorithm.split','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.urbanairship.datacube.backfill.BackfillUtil.getSplitKeys','org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst'
'com.mozilla.hadoop.Backup.BackupMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'com.mozilla.hadoop.Backup.BackupMapper.copyFile','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.StringUtils.humanReadableInt org.apache.hadoop.util.StringUtils.humanReadableInt org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.util.StringUtils.humanReadableInt org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.mozilla.hadoop.Backup.BackupMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get'
'com.mozilla.hadoop.Backup.loadPaths','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'com.mozilla.hadoop.Backup.getPaths','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.mozilla.hadoop.Backup.createInputSources','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'com.mozilla.hadoop.Backup.initJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'com.mozilla.hadoop.Backup.printUsage','org.apache.hadoop.util.GenericOptionsParser.printGenericCommandUsage'
'com.mozilla.hadoop.Backup.run','org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.mozilla.hadoop.Backup.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.cloudata.core.tablet.backup.BackupBinaryJob.runBackUp','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob'
'org.cloudata.core.tablet.backup.BackupBinaryMap.map','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect'
'com.tripadvisor.hadoop.BackupHdfs.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.toUri'
'com.tripadvisor.hadoop.BackupHdfs.backupFiles','org.apache.hadoop.fs.FileSystem.getContentSummary org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.FileStatus.getModificationTime'
'com.tripadvisor.hadoop.BackupHdfs.checkDir','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileSystem.getContentSummary'
'com.tripadvisor.hadoop.BackupHdfs.compareChecksums','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileChecksum org.apache.hadoop.fs.FileChecksum.toString org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileChecksum.toString'
'org.cloudata.core.tablet.backup.BackupJob.runBackUp','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob'
'org.cloudata.core.tablet.backup.BackupMap.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.oclc.firefly.hadoop.backup.BackupRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart'
'org.oclc.firefly.hadoop.backup.BackupRecordReader.nextKeyValue','org.apache.hadoop.hbase.HRegionInfo.<init>'
'com.datasalt.utils.commons.BaseConfigurationFactory.synchronize','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.utils.commons.BaseConfigurationFactory.populate','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath'
'com.datasalt.utils.commons.BaseConfigurationFactory.configureSerialization','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.datasalt.utils.commons.BaseConfigurationFactory.create','org.apache.hadoop.conf.Configuration.<init>'
'com.datasalt.utils.commons.BaseConfigurationFactory.configureSnappyCompression','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.datasalt.utils.commons.BaseHadoopJob.getProgress','org.apache.hadoop.mapreduce.Job.reduceProgress org.apache.hadoop.mapreduce.Job.mapProgress org.apache.hadoop.mapreduce.Job.reduceProgress'
'com.datasalt.utils.commons.BaseHadoopJob.cancel','org.apache.hadoop.mapreduce.Job.killJob'
'com.datasalt.utils.commons.BaseHadoopJob.execute','org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.datasalt.utils.commons.BaseJob.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'org.apache.oozie.servlet.BaseJobServlet.doPut','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.BaseJobServlet.checkAuthorizationForApp','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.BaseJobServlet.doPut','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.BaseJobServlet.checkAuthorizationForApp','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.BaseJobsServlet.validateJobConfiguration','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.BaseJobsServlet.validateJobConfiguration','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.BaseJobsServlet.validateJobConfiguration','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.nearinfinity.hbase.dsl.BaseTest.setUpBeforeClass','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster'
'com.nearinfinity.hbase.dsl.BaseTest.tearDownAfterClass','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'com.nearinfinity.hbase.dsl.BaseTest.setUp','org.apache.hadoop.hbase.client.HTable.<init>'
'com.datasalt.pangool.BaseTest.fillString','org.apache.hadoop.io.Text.<init>'
'com.datasalt.pangool.BaseTest.fillObject','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.datasalt.pangool.BaseTest.assertSerializable','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'edu.duke.starfish.whatif.scheduler.BasicFIFOSchedulerForOptimizer.scheduleJobGetTime','org.apache.hadoop.conf.Configuration.getFloat'
'org.apache.nutch.indexer.field.basic.BasicFieldFilter.setConf','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.indexer.field.BasicFields.runExtractor','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.indexer.field.BasicFields.runFlipper','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.indexer.field.BasicFields.runScorer','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.indexer.field.BasicFields.runMerger','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.indexer.field.BasicFields.Extractor.configure','org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.nutch.indexer.field.BasicFields.Extractor.map','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.ObjectWritable>.collect'
'org.apache.nutch.indexer.field.BasicFields.Extractor.reduce','org.apache.hadoop.io.ObjectWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.field.FieldsWritable>.collect'
'org.apache.nutch.indexer.field.BasicFields.Flipper.map','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.ObjectWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.ObjectWritable>.collect org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.ObjectWritable>.collect'
'org.apache.nutch.indexer.field.BasicFields.Flipper.reduce','org.apache.hadoop.io.ObjectWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum>.collect'
'org.apache.nutch.indexer.field.BasicFields.Scorer.map','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.ObjectWritable>.collect'
'org.apache.nutch.indexer.field.BasicFields.Scorer.reduce','org.apache.hadoop.io.ObjectWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.field.FieldsWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.field.FieldsWritable>.collect'
'org.apache.nutch.indexer.field.BasicFields.Merger.reduce','org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.field.FieldsWritable>.collect'
'org.apache.nutch.indexer.field.BasicFields.createFields','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.indexer.field.BasicFields.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.indexer.field.BasicFields.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.springframework.data.hadoop.hive.BasicHiveTest.doInHive','org.apache.hadoop.hive.service.HiveClient.get_all_databases'
'org.apache.nutch.indexer.basic.BasicIndexingFilter.filter','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.nutch.indexer.basic.BasicIndexingFilter.setConf','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.mahout.math.hadoop.stats.BasicStatsTest.setUp','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.stats.BasicStatsTest.produceTestData','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.math.hadoop.stats.BasicStatsTest.testStdDev2','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.nutch.summary.basic.BasicSummarizer.setConf','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.softlang.test.Basics.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.softlang.test.Basics.testTotal','org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.DoubleWritable.get'
'org.softlang.test.Basics.testCut','org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.DoubleWritable.get'
'org.softlang.test.Basics.fetchOutputFromDisk','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.softlang.test.Basics.deleteOutput','org.apache.hadoop.fs.FileSystem.delete'
'org.lilyproject.indexer.master.BatchIndexBuilder.startBatchBuildJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.submit'
'ivory.smrf.retrieval.BatchQueryRunner.parseParameters','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.accumulo.server.test.functional.BatchScanSplitTest.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.randomwalk.sequential.BatchVerify.visit','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.datasalt.pangool.solr.BatchWriter.close','org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus'
'org.apache.solr.hadoop.BatchWriter.close','org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus'
'org.apache.mahout.classifier.bayes.mapreduce.bayes.BayesClassifierDriver.runJob','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configurable.setConf org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.bayes.mapreduce.bayes.BayesClassifierDriver.readResult','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.mapreduce.bayes.BayesClassifierMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect'
'org.apache.mahout.classifier.bayes.mapreduce.bayes.BayesClassifierMapper.configure','org.apache.hadoop.mapred.JobConf.get'
'org.apache.mahout.classifier.bayes.mapreduce.bayes.BayesClassifierReducer.reduce','org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect'
'org.visitante.mr.bda.BayesDiscriminator.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.visitante.mr.bda.BayesDiscriminator.main','org.apache.hadoop.util.ToolRunner.run'
'org.visitante.mr.bda.BayesDiscriminator.BayesDiscriminatorMapper.cleanup','org.apache.hadoop.io.Text.set org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.visitante.mr.bda.BayesDiscriminator.BayesDiscriminatorMapper.map','org.apache.hadoop.io.Text.toString'
'org.visitante.mr.bda.BayesDiscriminator.BayesDiscriminatorReducer.cleanup','org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.visitante.mr.bda.BayesDiscriminator.BayesDiscriminatorReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.apache.mahout.classifier.bayes.BayesFeatureMapReduceTest.runMapReduce','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.classifier.naivebayes.test.BayesTestMapper.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.classifier.naivebayes.test.BayesTestMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.classifier.bayes.mapreduce.common.BayesTfIdfDriver.runJob','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.toString org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.fromString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configurable.setConf org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.mahout.classifier.bayes.mapreduce.common.BayesTfIdfMapper.map','org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect org.apache.hadoop.mapred.Reporter.setStatus'
'org.apache.mahout.classifier.bayes.mapreduce.common.BayesTfIdfMapper.configure','org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.toString org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.fromString'
'org.apache.mahout.classifier.bayes.mapreduce.bayes.BayesThetaNormalizerDriver.runJob','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.toString org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.fromString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.<init> org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.toString org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.fromString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.fromString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configurable.setConf org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.mahout.classifier.naivebayes.BayesUtils.readModelFromDir','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.naivebayes.BayesUtils.writeLabelIndex','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.classifier.bayes.mapreduce.common.BayesWeightSummerDriver.runJob','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configurable.setConf org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.mahout.classifier.bayes.mapreduce.common.BayesWeightSummerMapper.map','org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect'
'org.apache.mahout.classifier.bayes.mapreduce.common.BayesWeightSummerOutputFormat.getBaseRecordWriter','org.apache.hadoop.mapred.SequenceFileOutputFormat<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.io.Writable>.<init> org.apache.hadoop.mapred.SequenceFileOutputFormat<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.io.Writable>.getRecordWriter'
'org.apache.mahout.classifier.bayes.mapreduce.common.BayesWeightSummerReducer.reduce','org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.common.StringTuple,org.apache.hadoop.io.DoubleWritable>.collect'
'com.digitalpebble.behemoth.BehemothConfiguration.create','org.apache.hadoop.conf.Configuration.<init>'
'com.digitalpebble.behemoth.BehemothConfiguration.addBehemothResources','org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource'
'com.digitalpebble.behemoth.BehemothDocument.getMetadata','org.apache.hadoop.io.MapWritable.<init>'
'com.digitalpebble.behemoth.BehemothDocument.readFields','org.apache.hadoop.io.VersionMismatchException.<init> org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.MapWritable.readFields org.apache.hadoop.io.Text.readString'
'com.digitalpebble.behemoth.BehemothDocument.writeCommon','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.MapWritable.write'
'com.digitalpebble.behemoth.BehemothDocument.writeAnnotations','org.apache.hadoop.io.Text.writeString'
'com.digitalpebble.behemoth.BehemothDocument.writeAnnotation','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.write org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.write org.apache.hadoop.io.WritableUtils.writeString'
'com.digitalpebble.behemoth.BehemothDocument.readAnnotationFields','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.WritableUtils.readString'
'com.digitalpebble.behemoth.BehemothDocument.toString','org.apache.hadoop.io.MapWritable.entrySet'
'com.digitalpebble.behemoth.mahout.BehemothDocumentProcessor.tokenizeDocuments','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.digitalpebble.behemoth.mahout.BehemothDocumentProcessor.dumpLabels','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.digitalpebble.behemoth.BehemothMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,com.digitalpebble.behemoth.BehemothDocument>.collect'
'org.apache.nutch.tools.Benchmark.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.tools.Benchmark.createSeeds','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.nutch.tools.Benchmark.benchmark','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toString'
'com.nearinfinity.blur.store.BenchmarkDirectory.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'com.nearinfinity.blur.store.BenchmarkDirectoryNrt.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.BenchmarkerForNamenode','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.showParameter','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.countOperation','org.apache.hadoop.conf.Configuration.getInt'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.generateOperations','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.prepareOperations','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.prepareOrExecuteOperations','org.apache.hadoop.conf.Configuration.getInt'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.showFiles','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getReplication org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.analyzeResult','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setFloat'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.showResult','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.getResultForMail','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.sendMail','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.close','org.apache.hadoop.fs.FileSystem.close'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationThread.run','org.apache.hadoop.util.StringUtils.stringifyException'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.Operation.Operation','org.apache.hadoop.conf.Configuration.getInt'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.Operation.prepare','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.Operation.cleanup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationAppend.prepareInternal','org.apache.hadoop.fs.FileSystem.create'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationAppend.excuteInternal','org.apache.hadoop.fs.FileSystem.append org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationCreate.excuteInternal','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationRename.prepareInternal','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationRename.excuteInternal','org.apache.hadoop.fs.FileSystem.rename'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationDelete.prepareInternal','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationDelete.excuteInternal','org.apache.hadoop.fs.FileSystem.delete'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationMkdirs.excuteInternal','org.apache.hadoop.fs.FileSystem.mkdirs'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationSetReplication.prepareInternal','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationSetReplication.excuteInternal','org.apache.hadoop.fs.FileSystem.setReplication'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationGetListing.prepareInternal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.listStatus'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationGetListing.excuteInternal','org.apache.hadoop.fs.FileSystem.listStatus'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationGetBlockLocations.prepareInternal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.FileSystem.getFileBlockLocations'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationGetBlockLocations.excuteInternal','org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.FileSystem.getFileBlockLocations'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationGetFileInfo.prepareInternal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.taobao.adfs.benchmark.BenchmarkerForNamenode.OperationGetFileInfo.excuteInternal','org.apache.hadoop.fs.FileSystem.getFileStatus'
'co.nubetech.apache.hadoop.BigDecimalSplitter.split','org.apache.hadoop.conf.Configuration.getInt'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequency.MyMapper.map','org.apache.hadoop.io.Text.toString'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequency.MyCombiner.reduce','org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequency.MyReducer.reduce','org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequency.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequency.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequency.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequencyTuple.MyMapper.map','org.apache.hadoop.io.Text.toString'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequencyTuple.MyCombiner.reduce','org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequencyTuple.MyReducer.reduce','org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequencyTuple.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequencyTuple.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.bigram.BigramRelativeFrequencyTuple.main','org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.util.shared.BinaryComparableWithOffset.compareTo','org.apache.hadoop.io.BinaryComparable.getBytes org.apache.hadoop.io.BinaryComparable.getLength org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.BinaryComparable.getBytes org.apache.hadoop.io.BinaryComparable.getLength org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.WritableComparator.compareBytes'
'com.nesscomputing.hbase.spill.BinaryConverter.PutToBinary.apply','org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.KeyValue.getBuffer'
'com.nesscomputing.hbase.spill.BinaryConverter.StreamToPut.apply','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.KeyValue.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.KeyValue.<init> org.apache.hadoop.hbase.client.Put.add'
'org.apache.accumulo.core.util.format.BinaryFormatter.appendText','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'tap.formats.avro.BinaryKeyPartitioner.configure','org.apache.hadoop.mapred.JobConf.get'
'hipi.imagebundle.mapreduce.output.BinaryOutputFormat.BinaryRecordWriter.write','org.apache.hadoop.io.Writable.write org.apache.hadoop.io.Writable.write'
'hipi.imagebundle.mapreduce.output.BinaryOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.beginField','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.endFields','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.initializeSkipStream','org.apache.hadoop.io.DataInputBuffer.mark org.apache.hadoop.io.DataInputBuffer.readByte org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.mark'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.readFieldId','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.readVInt','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.readVLong','org.apache.hadoop.io.WritableUtils.readVLong'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.skipTextBytes','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.skipVInt','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.skipVLong','org.apache.hadoop.io.WritableUtils.readVLong'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.writeVInt','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.rpc.base.shared.BinaryProtocol.writeVLong','org.apache.hadoop.io.WritableUtils.writeVLong'
'com.cloudera.science.matching.graph.BipartiteMatchingVertexOutputFormat.BipartiteMatchingVertexWriter.writeVertex','org.apache.hadoop.io.Text.<init>'
'bixi.query.coprocessor.BixiImplementation.copQueryNeighbor4LS1','org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.copQueryPoint4LS1','org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.copQueryNeighbor4LS2','org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.copQueryPoint4LS2','org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.copGetTotalUsage4S3','org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.giveAvailableBikes','org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.processKV','org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.query.coprocessor.BixiImplementation.giveTotalUsage','org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.getAvailableBikesFromAPoint','org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.query.coprocessor.BixiImplementation.getTotalUsage_Schema2','org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.getAvailableBikesFromAPoint_Schema2','org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.query.coprocessor.BixiImplementation.getStationsNearPoint_Schema2','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'bixi.hbase.query.location.BixiLocationQueryS1.copQueryAvailableNear','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.getStartRow org.apache.hadoop.hbase.client.Scan.getStopRow'
'bixi.hbase.query.location.BixiLocationQueryS1.BixiCallBack.update','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS1.scanQueryAvailableNear','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS1.copQueryPoint','org.apache.hadoop.hbase.client.Scan.getStartRow org.apache.hadoop.hbase.client.Scan.getStopRow'
'bixi.hbase.query.location.BixiLocationQueryS1.scanQueryPoint','org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS1.debugColumnVersion','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS1.scanQueryAvailableKNN','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS1S1.copQueryAvailableNear','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.getStartRow org.apache.hadoop.hbase.client.Scan.getStopRow'
'bixi.hbase.query.location.BixiLocationQueryS1S1.BixiCallBack.update','org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS1S1.scanQueryAvailableNear','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS2.copQueryAvailableNear','org.apache.hadoop.hbase.client.Scan.getStartRow org.apache.hadoop.hbase.client.Scan.getStopRow'
'bixi.hbase.query.location.BixiLocationQueryS2.BixiCallBack.update','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS2.scanQueryAvailableNear','org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS2.copQueryPoint','org.apache.hadoop.hbase.client.Scan.getStartRow org.apache.hadoop.hbase.client.Scan.getStopRow'
'bixi.hbase.query.location.BixiLocationQueryS2.scanQueryPoint','org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS2.scanQueryAvailableKNN','org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.location.BixiLocationQueryS2.debugColumnVersion','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Result.getBytes org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'org.hackreduce.mappers.BixiMapper.configureJob','org.apache.hadoop.mapreduce.Job.setInputFormatClass'
'bixi.hbase.query.BixiQuery.queryAvgUsageByTimeSlot4Stations','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.BixiQuery.queryAvgUsageByTimeSlot4StationsWithScan','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'bixi.hbase.query.BixiQuery.Test_queryAvgUsageByTimeSlot4StationsWithScan','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.filter.RegexStringComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.getFilter org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'bixi.hbase.query.BixiQuery.queryAvailableByTimeStamp4Point','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'bixi.hbase.query.BixiQuery.queryAvailableByClusters','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'bixi.hbase.query.BixiQuery.queryAvailableByTimestampAndStations','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.containsColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.BixiQuery.askAvgUsageByTimeSlot4Stations','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'bixi.hbase.query.BixiQuery.askAvailableByTimeStamp4Point','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.BixiQuery.getEmptyDocks','org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'bixi.hbase.query.BixiQuerySchema3.queryAvgUsageByTimeSlot4Stations','org.apache.hadoop.hbase.client.coprocessor.BixiClient.<init> org.apache.hadoop.hbase.client.coprocessor.BixiClient.copGetAvgUsageForPeriod4S3'
'bixi.dataset.cluster.BixiReaderToTable.BixiReaderToTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init>'
'bixi.dataset.cluster.BixiReaderToTable.addStationToTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'org.hackreduce.models.BixiRecord.BixiRecord','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.sqoop.lib.BlobRef.BlobRef','org.apache.hadoop.io.BytesWritable.<init>'
'org.apache.sqoop.lib.BlobRef.getInternalSource','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'org.apache.sqoop.lib.BlobRef.getInternalData','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'org.apache.sqoop.lib.BlobRef.deepCopyData','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.<init>'
'org.apache.sqoop.lib.BlobRef.readFieldsInternal','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.readFields'
'org.lilyproject.server.modules.repository.BlobStoreConfig.get','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.lilyproject.repository.impl.test.BlobStoreTest.setUpBeforeClass','org.apache.hadoop.fs.Path.<init>'
'org.lilyproject.repository.impl.primitivevaluetype.BlobValueType.fromBytes','org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.repository.impl.primitivevaluetype.BlobValueType.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'com.nearinfinity.blur.store.blockcache.BlockCacheTest.testBlockCache','org.apache.hadoop.conf.Configuration.<init>'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileInputFormat<com.twitter.elephanttwin.retrieval.K,com.twitter.elephanttwin.retrieval.V>.getSplits org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.equals org.apache.hadoop.mapreduce.lib.input.FileSplit.getLocations'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.setOptions','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.getIndexDir','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.foundIndexFile','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.verifyInputFileCheckSum','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileChecksum'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.getFilterQualifiedBlocks','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.<init> org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.getPartition org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.listIndexFiles','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.accept','org.apache.hadoop.fs.Path.getName'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.getRealRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileInputFormat<com.twitter.elephanttwin.retrieval.K,com.twitter.elephanttwin.retrieval.V>.createRecordReader'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.noFilterCondition','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.getFilterCondition','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.twitter.elephanttwin.retrieval.BlockIndexedFileInputFormat.isIndexingJob','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.commoncrawl.service.directory.BlockingClient.requestComplete','org.apache.hadoop.record.Buffer.<init>'
'org.apache.pig.builtin.Bloom.exec','org.apache.hadoop.util.bloom.Key.<init> org.apache.hadoop.util.bloom.BloomFilter.membershipTest'
'org.apache.pig.builtin.Bloom.init','org.apache.hadoop.util.bloom.BloomFilter.<init> org.apache.hadoop.util.bloom.BloomFilter.readFields'
'org.apache.pig.builtin.Bloom.setFilter','org.apache.hadoop.util.bloom.BloomFilter.<init> org.apache.hadoop.util.bloom.BloomFilter.readFields'
'com.cloudera.util.bloom.BloomSet.BloomSet','org.apache.hadoop.util.bloom.BloomFilter.<init>'
'com.cloudera.util.bloom.BloomSet.deserialize','org.apache.hadoop.util.bloom.BloomFilter.<init> org.apache.hadoop.util.bloom.BloomFilter.readFields'
'com.cloudera.util.bloom.BloomSet.addInt','org.apache.hadoop.util.bloom.Key.<init> org.apache.hadoop.util.bloom.BloomFilter.add'
'com.cloudera.util.bloom.BloomSet.getBytes','org.apache.hadoop.util.bloom.BloomFilter.write'
'com.cloudera.util.bloom.BloomSet.and','org.apache.hadoop.util.bloom.BloomFilter.and'
'org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilter.BloomTokenFilter','org.apache.hadoop.util.bloom.Key.<init>'
'org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilter.incrementToken','org.apache.hadoop.util.bloom.Key.set org.apache.hadoop.util.bloom.Filter.membershipTest'
'com.nearinfinity.blur.analysis.BlurAnalyzer.create','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'com.nearinfinity.blur.mapreduce.example.BlurExampleIndexerRebuild.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.io.compress.DefaultCodec.getName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.nearinfinity.blur.mapreduce.example.BlurExampleIndexerUpdate.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.nearinfinity.blur.mapreduce.example.BlurExampleMapper.map','org.apache.hadoop.io.Text.toString'
'com.nearinfinity.blur.mapreduce.lib.BlurInputFormat.getSplits','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getInputPaths org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.nearinfinity.blur.mapreduce.lib.BlurInputFormat.findAllSegments','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.nearinfinity.blur.mapred.BlurInputFormat.getSplits','org.apache.hadoop.mapred.FileInputFormat.getInputPaths'
'com.nearinfinity.blur.mapred.BlurInputFormat.getRecordReader','org.apache.hadoop.mapred.InputSplit.toString org.apache.hadoop.mapred.Reporter.setStatus'
'com.nearinfinity.blur.store.lock.BlurLockFactory.BlurLockFactory','org.apache.hadoop.fs.Path.getFileSystem'
'com.nearinfinity.blur.store.lock.BlurLockFactory.makeLock','org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.store.lock.BlurLockFactory.obtain','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'com.nearinfinity.blur.store.lock.BlurLockFactory.release','org.apache.hadoop.fs.FileSystem.delete'
'com.nearinfinity.blur.store.lock.BlurLockFactory.isLocked','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataInputStream.close'
'com.nearinfinity.blur.store.lock.BlurLockFactory.clearLock','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.nearinfinity.blur.manager.writer.BlurNRTIndex.init','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.manager.writer.BlurNRTIndexTest.setup','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.mapreduce.lib.BlurRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.nearinfinity.blur.mapreduce.lib.BlurRecordReader.readDocument','org.apache.hadoop.io.Text.set'
'com.nearinfinity.blur.mapred.BlurRecordReader.readDocument','org.apache.hadoop.io.Text.set'
'com.nearinfinity.blur.mapred.BlurRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'com.nearinfinity.blur.mapreduce.lib.BlurRecordWriter.BlurRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.mapreduce.lib.BlurRecordWriter.write','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.set'
'com.nearinfinity.blur.mapreduce.lib.BlurRecordWriterTest.testBlurRecordWriter','org.apache.hadoop.mapreduce.JobID.<init> org.apache.hadoop.mapreduce.TaskID.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.mapreduce.BlurReducer.reduce','org.apache.hadoop.mapreduce.Counter.increment'
'com.nearinfinity.blur.mapreduce.BlurReducer.index','org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue'
'com.nearinfinity.blur.mapreduce.BlurReducer.getInstance','org.apache.hadoop.conf.Configurable.setConf'
'com.nearinfinity.blur.mapreduce.BlurReducer.remove','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'com.nearinfinity.blur.mapreduce.BlurReducer.convert','org.apache.hadoop.mapreduce.Counter.increment'
'com.nearinfinity.blur.utils.BlurUtil.setupFileSystem','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.utils.BlurUtil.validateShardCount','org.apache.hadoop.fs.FileSystem.listStatus'
'com.nearinfinity.blur.utils.BlurUtil.createPath','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'com.nearinfinity.blur.utils.BlurUtil.configure','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configurable.setConf'
'com.nearinfinity.blur.utils.BlurUtil.removeIndexFiles','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'de.tuberlin.dima.aim3.assignment1.BookAndAuthorBroadcastJoin.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'de.tuberlin.dima.aim3.assignment1.BookAndAuthorJoinTest.testJoin','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.Tool.setConf org.apache.hadoop.util.Tool.run'
'de.tuberlin.dima.aim.exercises.two.BookAndAuthorJoinTest.testJoin','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.Tool.setConf org.apache.hadoop.util.Tool.run'
'de.tuberlin.dima.aim3.assignment1.BookAndAuthorReduceSideJoin.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.giraph.aggregators.BooleanAndAggregator.aggregate','org.apache.hadoop.io.BooleanWritable.get'
'org.apache.giraph.aggregators.BooleanAndAggregator.createInitialValue','org.apache.hadoop.io.BooleanWritable.<init>'
'org.apache.expreval.expr.compare.BooleanCompare.getValue','org.apache.hadoop.hbase.hbql.client.HBqlException.<init>'
'org.apache.expreval.expr.compare.BooleanCompare.getFilter','org.apache.hadoop.hbase.hbql.filter.RecordFilterList.getOperator org.apache.hadoop.hbase.hbql.filter.RecordFilterList.addFilter org.apache.hadoop.hbase.hbql.util.Lists.newArrayList org.apache.hadoop.hbase.hbql.filter.RecordFilterList.<init> org.apache.hadoop.hbase.hbql.filter.RecordFilterList.getOperator org.apache.hadoop.hbase.hbql.filter.RecordFilterList.addFilter org.apache.hadoop.hbase.hbql.util.Lists.newArrayList org.apache.hadoop.hbase.hbql.filter.RecordFilterList.<init> org.apache.hadoop.hbase.hbql.client.HBqlException.<init>'
'org.apache.expreval.expr.compare.BooleanCompare.BooleanComparable.compareTo','org.apache.hadoop.hbase.hbql.io.IO.getSerialization org.apache.hadoop.hbase.hbql.client.HBqlException.printStackTrace org.apache.hadoop.hbase.hbql.impl.Utils.logException'
'com.nearinfinity.hbase.dsl.types.BooleanConverter.fromBytes','org.apache.hadoop.hbase.util.Bytes.toBoolean'
'com.nearinfinity.hbase.dsl.types.BooleanConverter.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.expreval.expr.function.BooleanFunction.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'org.apache.expreval.expr.function.BooleanFunction.getValue','org.apache.hadoop.hbase.hbql.parser.ParserUtil.parseWhereExpression org.apache.hadoop.hbase.hbql.impl.HConnectionImpl.mappingExists org.apache.hadoop.hbase.hbql.impl.HConnectionImpl.tableExists org.apache.hadoop.hbase.hbql.impl.HConnectionImpl.tableEnabled org.apache.hadoop.hbase.hbql.impl.HConnectionImpl.tableAvailable org.apache.hadoop.hbase.hbql.impl.HConnectionImpl.familyExistsForTable org.apache.hadoop.hbase.hbql.impl.HConnectionImpl.familyExistsForMapping org.apache.hadoop.hbase.hbql.impl.HConnectionImpl.indexExistsForTable org.apache.hadoop.hbase.hbql.impl.HConnectionImpl.indexExistsForMapping org.apache.hadoop.hbase.hbql.client.AsyncExecutorManager.asyncExecutorExists org.apache.hadoop.hbase.hbql.client.QueryExecutorPoolManager.queryExecutorPoolExists org.apache.hadoop.hbase.hbql.client.HBqlException.<init>'
'org.goldenorb.types.message.BooleanMessage.BooleanMessage','org.apache.hadoop.io.BooleanWritable.<init>'
'org.goldenorb.types.message.BooleanMessage.get','org.apache.hadoop.io.BooleanWritable.get'
'org.goldenorb.types.message.BooleanMessage.set','org.apache.hadoop.io.BooleanWritable.set'
'org.apache.giraph.aggregators.BooleanOverwriteAggregator.aggregate','org.apache.hadoop.io.BooleanWritable.get'
'org.apache.giraph.aggregators.BooleanOverwriteAggregator.createInitialValue','org.apache.hadoop.io.BooleanWritable.<init>'
'edu.umd.cloud9.example.ir.BooleanRetrieval.BooleanRetrieval','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'edu.umd.cloud9.example.ir.BooleanRetrieval.fetchPostings','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'edu.umd.cloud9.example.ir.BooleanRetrieval.fetchLine','org.apache.hadoop.fs.FSDataInputStream.seek'
'edu.umd.cloud9.example.ir.BooleanRetrieval.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.lilyproject.repository.impl.primitivevaluetype.BooleanValueType.fromBytes','org.apache.hadoop.hbase.util.Bytes.toBoolean'
'org.lilyproject.repository.impl.primitivevaluetype.BooleanValueType.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.mahout.classifier.df.BreimanExample.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.classifier.df.BreimanExample.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.hasOutput','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.set','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.save','org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.getSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readString'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.getDataSourceRepository','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.getVariableTable','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.setupTask','org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.commitTask','org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.abortTask','org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.doCleanupTask','org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.setupJob','org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.cleanOutput','org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.commitJob','org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.setTransactionInfo','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.getRaw org.apache.hadoop.conf.Configuration.getRaw org.apache.hadoop.conf.Configuration.getRaw org.apache.hadoop.conf.Configuration.getRaw org.apache.hadoop.conf.Configuration.getRaw org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobName org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.fs.FileSystem.makeQualified'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.setCommitted','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.fs.FileSystem.makeQualified'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.isCommitted','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.abortJob','org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.rollforward','org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.cleanup','org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.getTransactionInfoPath','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.get'
'com.asakusafw.runtime.stage.output.BridgeOutputFormat.BridgeOutputCommitter.getCommitMarkPath','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.get'
'ivory.lsh.eval.BruteForcePwsim.MyMapperDocVectors.configure','org.apache.hadoop.mapred.JobConf.getFloat org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.lsh.eval.BruteForcePwsim.MyMapperDocVectors.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.pair.PairOfFloatInt>.collect'
'ivory.lsh.eval.BruteForcePwsim.MyMapperTermDocVectors.configure','org.apache.hadoop.mapred.JobConf.getFloat org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.lsh.eval.BruteForcePwsim.MyMapperTermDocVectors.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.pair.PairOfFloatInt>.collect'
'ivory.lsh.eval.BruteForcePwsim.MyMapperSignature.configure','org.apache.hadoop.mapred.JobConf.getFloat org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.lsh.eval.BruteForcePwsim.MyMapperSignature.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.pair.PairOfFloatInt>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'ivory.lsh.eval.BruteForcePwsim.MyReducer.configure','org.apache.hadoop.mapred.JobConf.getInt'
'ivory.lsh.eval.BruteForcePwsim.MyReducer.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfInts,org.apache.hadoop.io.Text>.collect'
'ivory.lsh.eval.BruteForcePwsim.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob'
'ivory.lsh.eval.BruteForcePwsim.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.giraph.BspCase.getSinglePartFileStatus','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.giraph.BspCase.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete'
'org.apache.giraph.BspCase.removeAndSetOutput','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.apache.giraph.BspCase.remove','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.giraph.BspCase.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.giraph.BspCase.setupConfiguration','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.giraph.BspCase.getTempPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.giraph.BspCase.getSinglePartFileStatus','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.giraph.BspCase.getNumResults','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'org.apache.giraph.BspCase.cleanupTemporaryFiles','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.giraph.BspCase.removeAndSetOutput','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.apache.giraph.bsp.BspOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.giraph.bsp.BspOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.giraph.graph.BspService.BspService','org.apache.hadoop.fs.FileSystem.get'
'org.apache.giraph.graph.BspService.getCheckpoint','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'org.apache.giraph.graph.BspUtils.getGraphPartitionerClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createGraphPartitioner','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getVertexInputFormatClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createVertexInputFormat','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getVertexOutputFormatClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createVertexOutputFormat','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getAggregatorWriterClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createAggregatorWriter','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getVertexCombinerClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createVertexCombiner','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getVertexResolverClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createVertexResolver','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getWorkerContextClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createWorkerContext','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getMasterComputeClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createMasterCompute','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getVertexClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createVertex','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.graph.BspUtils.getVertexIdClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.getVertexValueClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createVertexValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.graph.BspUtils.getEdgeValueClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createEdgeValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.graph.BspUtils.getMessageValueClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.graph.BspUtils.createMessageValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.BtMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getUniqueFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.mahout.math.hadoop.stochasticsvd.SparseRowBlockWritable>.<init> org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.BtMapper.cleanup','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.BtMapper.outputQRow','org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.OuterProductReducer.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.OuterProductReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.mahout.math.VectorWritable>.collect org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.OuterProductReducer.cleanup','org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable>.collect org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.mahout.math.VectorWritable>.collect'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setMinInputSplitSize org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.BtMapper.cleanup','org.apache.hadoop.mapred.lib.MultipleOutputs.close'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.BtMapper.outputQRow','org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.BtMapper.map','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.BtMapper.setup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getUniqueFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.BtJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setMinInputSplitSize org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'com.odiago.flumebase.exec.BucketedAggregationElement.BucketedAggregationElement','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt'
'com.sematext.hbase.hut.BufferedHutPutWriter.write','org.apache.hadoop.hbase.client.Put.getRow'
'com.sematext.hbase.hut.BufferedHutPutWriter.processGroupAndWrite','org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.getRow'
'com.sematext.hbase.hut.BufferedHutPutWriter.writeInternal','org.apache.hadoop.hbase.client.HTable.put'
'org.apache.pig.builtin.BuildBloom.Initial.exec','org.apache.hadoop.util.bloom.Key.<init> org.apache.hadoop.util.bloom.BloomFilter.<init>'
'edu.umd.cloud9.collection.clue.BuildClueWarcForwardIndex.MyMapRunner.configure','org.apache.hadoop.mapred.JobConf.get'
'edu.umd.cloud9.collection.clue.BuildClueWarcForwardIndex.MyMapRunner.run','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.collection.clue.ClueWarcRecord>.getPos org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.collection.clue.ClueWarcRecord>.next org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.collection.clue.ClueWarcRecord>.getPos org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.collection.clue.BuildClueWarcForwardIndex.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.collection.clue.BuildClueWarcForwardIndex.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.findCounter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeUTF org.apache.hadoop.fs.FSDataOutputStream.writeUTF org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.writeShort org.apache.hadoop.util.LineReader.close org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.collection.clue.BuildClueWarcForwardIndex.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.df.mapreduce.BuildForest.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapreduce.BuildForest.buildForest','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapreduce.BuildForest.loadData','org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.df.mapreduce.BuildForest.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.df.mapred.BuildForest.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapred.BuildForest.buildForest','org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.df.mapred.BuildForest.loadData','org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.df.mapred.BuildForest.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ivory.core.index.BuildIPInvertedIndexDocSorted.MyMapper.map','org.apache.hadoop.io.IntWritable.get'
'ivory.core.index.BuildIPInvertedIndexDocSorted.MyReducer.reduce','org.apache.hadoop.io.IntWritable.set'
'ivory.core.index.BuildIPInvertedIndexDocSorted.MyReducer.cleanup','org.apache.hadoop.io.IntWritable.set'
'ivory.core.index.BuildIPInvertedIndexDocSorted.runTool','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'ivory.core.preprocess.BuildIntDocVectors.MyMapper.setup','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.core.preprocess.BuildIntDocVectors.runTool','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'ivory.core.preprocess.BuildIntDocVectorsForwardIndex.MyMapper.run','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.LongWritable.set'
'ivory.core.preprocess.BuildIntDocVectorsForwardIndex.MyReducer.setup','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.writeInt'
'ivory.core.preprocess.BuildIntDocVectorsForwardIndex.MyReducer.reduce','org.apache.hadoop.fs.FSDataOutputStream.writeLong'
'ivory.core.preprocess.BuildIntDocVectorsForwardIndex.MyReducer.cleanup','org.apache.hadoop.fs.FSDataOutputStream.close'
'ivory.core.preprocess.BuildIntDocVectorsForwardIndex.runTool','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.ir.BuildInvertedIndex.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,edu.umd.cloud9.io.pair.PairOfInts>.collect'
'edu.umd.cloud9.example.ir.BuildInvertedIndex.MyReducer.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,edu.umd.cloud9.io.pair.PairOfWritables<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.io.pair.PairOfInts>>>.collect'
'edu.umd.cloud9.example.ir.BuildInvertedIndex.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.ir.BuildInvertedIndex.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.example.ir.BuildInvertedIndex.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.pagerank.BuildPageRankRecords.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.example.pagerank.BuildPageRankRecords.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.pagerank.BuildPageRankRecords.run','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.pagerank.BuildPageRankRecords.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.webgraph.BuildReverseWebGraph.Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect'
'edu.umd.cloud9.webgraph.BuildReverseWebGraph.runTool','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapred.SequenceFileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobClient.runJob'
'ivory.core.preprocess.BuildTermDocVectors.MyMapper.setup','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.conf.Configuration.get'
'ivory.core.preprocess.BuildTermDocVectors.MyMapper.map','org.apache.hadoop.io.IntWritable.set'
'ivory.core.preprocess.BuildTermDocVectors.MyMapper.cleanup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.toString'
'ivory.core.preprocess.BuildTermDocVectors.DocLengthDataWriterMapper.run','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.util.LineReader.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.close'
'ivory.core.preprocess.BuildTermDocVectors.runTool','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'ivory.core.preprocess.BuildTranslatedTermDocVectors.MyMapperTrans.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getFloat org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'ivory.core.preprocess.BuildTranslatedTermDocVectors.MyMapperTrans.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.map.HMapSFW>.collect'
'ivory.core.preprocess.BuildTranslatedTermDocVectors.runTool','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobClient.runJob'
'ivory.core.preprocess.BuildTranslatedTermDocVectors.createTranslatedDFFile','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobClient.runJob'
'ivory.core.preprocess.BuildTranslatedTermDocVectors.DataWriterMapper.run','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'ivory.core.preprocess.BuildWeightedTermDocVectors.MyMapper.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'ivory.core.preprocess.BuildWeightedTermDocVectors.MyMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.map.HMapSFW>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'ivory.core.preprocess.BuildWeightedTermDocVectors.runTool','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaDocnoMapping.MyMapper.configure','org.apache.hadoop.mapred.JobConf.getBoolean'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaDocnoMapping.MyMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaDocnoMapping.MyReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaDocnoMapping.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaDocnoMapping.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaLinkGraph.MyMapper1.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfStringInt,org.apache.hadoop.io.Text>.collect org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfStringInt,org.apache.hadoop.io.Text>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfStringInt,org.apache.hadoop.io.Text>.collect'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaLinkGraph.MyReducer1.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaLinkGraph.MyMapper2.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaLinkGraph.MyReducer2.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text>.collect'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaLinkGraph.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaLinkGraph.task1','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.TextOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaLinkGraph.task2','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.TextInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.TextOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.collection.wikipedia.BuildWikipediaLinkGraph.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.df.mapreduce.Builder.Builder','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.df.mapreduce.Builder.getNumMaps','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.mahout.df.mapreduce.Builder.isOutput','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.df.mapreduce.Builder.isOobEstimate','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.df.mapreduce.Builder.setOobEstimate','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.mahout.df.mapreduce.Builder.getRandomSeed','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.df.mapreduce.Builder.setRandomSeed','org.apache.hadoop.conf.Configuration.setLong'
'org.apache.mahout.df.mapreduce.Builder.getTreeBuilder','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.df.mapreduce.Builder.setTreeBuilder','org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.df.mapreduce.Builder.getNbTrees','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.mahout.df.mapreduce.Builder.setNbTrees','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.mahout.df.mapreduce.Builder.getOutputPath','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapreduce.Builder.getDistributedCacheFile','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapreduce.Builder.runJob','org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.df.mapreduce.Builder.build','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.mahout.df.mapreduce.Builder.compare','org.apache.hadoop.mapreduce.InputSplit.getLength org.apache.hadoop.mapreduce.InputSplit.getLength'
'org.apache.accumulo.server.test.randomwalk.concurrent.BulkImport.RFileBatchWriter.RFileBatchWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.fs.FileSystem.create'
'org.apache.accumulo.server.test.randomwalk.concurrent.BulkImport.visit','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.accumulo.server.test.BulkImportDirectory.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs'
'bulkimport.BulkImportJobExample.VerboseInputSampler.writePartitionFile','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getInputFormatClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.Job.getNumReduceTasks org.apache.hadoop.mapreduce.Job.getGroupingComparator org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.getPartitionFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.getMapOutputKeyClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.RawComparator<bulkimport.K>.compare'
'bulkimport.BulkImportJobExample.VerboseInputSampler.VerboseRandomSampler.getSample','org.apache.hadoop.mapreduce.InputFormat.getSplits org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.InputFormat.createRecordReader org.apache.hadoop.mapreduce.RecordReader<bulkimport.K,bulkimport.V>.initialize org.apache.hadoop.mapreduce.RecordReader<bulkimport.K,bulkimport.V>.nextKeyValue org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.RecordReader<bulkimport.K,bulkimport.V>.getCurrentKey org.apache.hadoop.util.ReflectionUtils.copy org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.RecordReader<bulkimport.K,bulkimport.V>.getCurrentKey org.apache.hadoop.util.ReflectionUtils.copy org.apache.hadoop.mapreduce.RecordReader<bulkimport.K,bulkimport.V>.close'
'bulkimport.BulkImportJobExample.DeliciousRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.<init> org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'bulkimport.BulkImportJobExample.DeliciousRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentKey'
'bulkimport.BulkImportJobExample.DeliciousRecordReader.getProgress','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress'
'bulkimport.BulkImportJobExample.DeliciousRecordReader.close','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'bulkimport.BulkImportJobExample.BulkImportMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.mapreduce.Counter.increment'
'bulkimport.BulkImportJobExample.createSubmittableJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.setOutputPath org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.setCompressOutput org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.setPartitionFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.createSymlink'
'bulkimport.BulkImportJobExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.accumulo.server.client.BulkImporter.bulkLoad','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.client.BulkImporter.importFiles','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.client.BulkImporter.run','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.client.BulkImporter.estimateSizes','org.apache.hadoop.fs.FileSystem.getContentSummary'
'org.apache.accumulo.server.client.BulkImporter.findOverlappingTablets','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append'
'org.apache.accumulo.server.client.BulkImporterTest.testFindOverlappingTablets','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.randomwalk.shard.BulkInsert.SeqfileBatchWriter.SeqfileBatchWriter','org.apache.hadoop.fs.Path.<init>'
'org.apache.accumulo.server.test.randomwalk.shard.BulkInsert.visit','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.accumulo.server.test.randomwalk.shard.BulkInsert.bulkImport','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.accumulo.server.test.randomwalk.shard.BulkInsert.sort','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.util.ToolRunner.run'
'org.apache.cassandra.hadoop.BulkOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.cassandra.hadoop.BulkOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.accumulo.server.test.randomwalk.bulk.BulkPlusOne.bulkLoadLots','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.delete'
'org.apache.cassandra.hadoop.BulkRecordWriter.BulkRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.BulkRecordWriter.getOutputLocation','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.BulkRecordWriter.prepareWriter','org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.server.test.functional.BulkSplitOptimizationTest.cleanup','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.accumulo.server.test.functional.BulkSplitOptimizationTest.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.mapred.segmenter.BundleKeyPartitioner.configure','org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.oozie.command.bundle.BundleStartXCommand.startCoordJobs','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.bundle.BundleStartXCommand.mergeConfig','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.bundle.BundleStartXCommand.startCoordJobs','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.bundle.BundleStartXCommand.mergeConfig','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.submit','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.mergeDefaultConfig','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.readAndValidateXml','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.readDefinition','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.storeToDB','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.submit','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.mergeDefaultConfig','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.readAndValidateXml','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.readDefinition','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.command.bundle.BundleSubmitXCommand.storeToDB','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'mia.clustering.ch12.twitter.ByKeyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.sap.hadoop.ds.list.ByteBasedListTest.test1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getSerializedClass org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize'
'com.sap.hadoop.ds.list.ByteBasedListTest.test2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getSerializedClass org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize'
'org.apache.expreval.expr.betweenstmt.ByteBetweenStmt.ByteBetweenComparable.compareTo','org.apache.hadoop.hbase.hbql.io.IO.getSerialization org.apache.hadoop.hbase.hbql.client.HBqlException.printStackTrace org.apache.hadoop.hbase.hbql.impl.Utils.logException'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testSignedIntegerCompare','org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testSignedFloatCompare','org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testCollatorStringCompare','org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testUtf8StringCompare','org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testAsciiFoldingStringCompare','org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testDecimalCompare','org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testDateCompare','org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testTimeCompare','org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testTimeNoMillisCompare','org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testDateTimeCompare','org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.lilyproject.hbaseindex.test.ByteComparisonTest.testDateTimeNoMillisCompare','org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.wikimedia.wikihadoop.ByteMatcher.getPos','org.apache.hadoop.fs.Seekable.getPos'
'org.wikimedia.wikihadoop.ByteMatcher.readUntilMatch','org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.fs.Seekable.getPos'
'org.gora.util.ByteUtils.vintToBytes','org.apache.hadoop.io.WritableUtils.getVIntSize'
'org.gora.util.ByteUtils.bytesToVlong','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.gora.util.ByteUtils.bytesToVint','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.gora.util.ByteUtils.readVLong','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.apache.gora.util.ByteUtils.vintToBytes','org.apache.hadoop.io.WritableUtils.getVIntSize'
'org.apache.gora.util.ByteUtils.bytesToVlong','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.apache.gora.util.ByteUtils.bytesToVint','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.apache.gora.util.ByteUtils.readVLong','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.apache.hama.util.Bytes.readByteArray','org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.hama.util.Bytes.writeByteArray','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt'
'org.apache.hama.util.Bytes.vintToBytes','org.apache.hadoop.io.WritableUtils.getVIntSize'
'org.apache.hama.util.Bytes.bytesToVint','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.apache.hama.util.Bytes.readVLong','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.apache.hama.util.Bytes.hashCode','org.apache.hadoop.io.WritableComparator.hashBytes'
'org.apache.hama.util.Bytes.binarySearch','org.apache.hadoop.io.RawComparator<byte[]>.compare'
'cascading.tuple.hadoop.util.BytesComparator.compare','org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.WritableComparator.compareBytes'
'.BytesWritableTest.test','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.util.StringUtils.byteToHexString org.apache.hadoop.io.BytesWritable.setCapacity org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.getBytes'
'com.twitter.elephantbird.pig.store.Bz2PigStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.compress.BZip2Codec.getName'
'org.apache.hama.bsp.message.compress.Bzip2Compressor.compressBundle','org.apache.hadoop.io.compress.BZip2Codec.createOutputStream org.apache.hadoop.io.compress.CompressionOutputStream.close'
'org.apache.hama.bsp.message.compress.Bzip2Compressor.decompressBundle','org.apache.hadoop.io.compress.BZip2Codec.createInputStream org.apache.hadoop.io.compress.CompressionInputStream.close'
'org.apache.tools.bzip2r.CBZip2InputStream.CBZip2InputStream','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.tools.bzip2r.CBZip2InputStream.initBlock','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.tools.bzip2r.CBZip2InputStream.complete','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.tools.bzip2r.CBZip2InputStream.readBs','org.apache.hadoop.fs.FSDataInputStream.read'
'org.apache.crunch.io.text.CBZip2InputStream.CBZip2InputStream','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.crunch.io.text.CBZip2InputStream.initBlock','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.crunch.io.text.CBZip2InputStream.complete','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.crunch.io.text.CBZip2InputStream.readBs','org.apache.hadoop.fs.FSDataInputStream.read'
'org.apache.mahout.classifier.bayes.mapreduce.cbayes.CBayesThetaNormalizerDriver.runJob','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.toString org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Double>>.fromString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.<init> org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.toString org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.fromString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.io.DefaultStringifier<java.lang.Double>.fromString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configurable.setConf org.apache.hadoop.mapred.JobClient.runJob'
'org.creativecommons.nutch.CCIndexingFilter.filter','org.apache.hadoop.io.Text.toString'
'org.creativecommons.nutch.CCParseFilter.Walker.walk','org.apache.hadoop.conf.Configuration.getBoolean'
'org.creativecommons.nutch.CCParseFilter.Walker.walk','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.ga.watchmaker.cd.CDFitnessEvaluator.CDFitnessEvaluator','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.ga.watchmaker.cd.CDFitnessEvaluator.evaluate','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.ga.watchmaker.cd.tool.CDInfosTool.gatherInfos','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.ga.watchmaker.cd.tool.CDInfosTool.configureJob','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.mahout.ga.watchmaker.cd.tool.CDInfosTool.importDescriptions','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.ga.watchmaker.cd.tool.CDInfosTool.loadDescriptors','org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.ga.watchmaker.cd.tool.CDInfosTool.storeDescriptions','org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.ga.watchmaker.cd.tool.CDInfosTool.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.ga.watchmaker.cd.hadoop.CDMahoutEvaluatorTest.testEvaluate','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.ga.watchmaker.cd.hadoop.CDMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.ga.watchmaker.cd.hadoop.CDMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'org.apache.mahout.ga.watchmaker.cd.hadoop.CDMapperTest.testMap','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.get'
'org.archive.wayback.hadoop.CDXCanonicalizingMapper.mapGlobal','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.archive.wayback.hadoop.CDXCanonicalizingMapper.mapFull','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.archive.wayback.hadoop.CDXCanonicalizingMapper.setMapMode','org.apache.hadoop.conf.Configuration.setInt'
'org.archive.wayback.hadoop.CDXCanonicalizingMapper.setConf','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get'
'org.archive.hadoop.cdx.CDXCluster.CDXCluster','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init>'
'org.archive.hadoop.cdx.CDXCluster.loadBlock','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'org.archive.hadoop.cdx.CDXConverterTool.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.archive.hadoop.mapreduce.CDXMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.archive.hadoop.mapreduce.CDXMapper.setConf','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.cdbw.CDbwEvaluator.CDbwEvaluator','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.cdbw.CDbwEvaluator.CDbwEvaluator','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.iterator.CIReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'ivory.core.util.CLIRUtilsTest.testGIZA','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'ivory.core.util.CLIRUtilsTest.testGIZA2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'ivory.core.util.CLIRUtilsTest.testBerkeleyAligner','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'ivory.core.util.CLIRUtilsTest.testBerkeleyAligner2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'ivory.core.util.CLIRUtilsTest.testHooka','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'ivory.core.util.CLIRUtilsTest.testHooka2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'edu.umd.hooka.CLI_Int2Words.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open'
'client.CRUDExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.cloudera.recordbreaker.analyzer.CSVDataDescriptor.isCSV','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.open'
'org.apache.solr.hadoop.CSVDocumentConverter.convert','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.MapWritable.entrySet'
'example.CSVFileImporter.main','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.HBaseConfiguration.set org.apache.hadoop.hbase.HBaseConfiguration.set org.apache.hadoop.hbase.HBaseConfiguration.setInt org.apache.hadoop.hbase.HBaseConfiguration.setInt org.apache.hadoop.hbase.client.HTablePool.<init>'
'example.CSVFileImporter.process','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.client.HTablePool.putTable'
'org.apache.pig.piggybank.storage.CSVLoader.getNext','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentValue org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.pig.piggybank.storage.CSVLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'com.manning.hip.ch3.csv.CSVOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'com.manning.hip.ch3.csv.CSVOutputFormat.CSVRecordWriter.writeObject','org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Writable.toString'
'com.bizo.hive.serde.csv.CSVSerde.initialize','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector'
'com.bizo.hive.serde.csv.CSVSerde.serialize','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.bizo.hive.serde.csv.CSVSerde.deserialize','org.apache.hadoop.io.Text.toString org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.bizo.hive.serde.csv.CSVSerdeTest.testDeserialize','org.apache.hadoop.io.Text.<init>'
'com.bizo.hive.serde.csv.CSVSerdeTest.testDeserializeCustomSeparators','org.apache.hadoop.io.Text.<init>'
'com.bizo.hive.serde.csv.CSVSerdeTest.testDeserializeCustomEscape','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.getNumTerms','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.calculatePerplexity','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.getParent org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.getParent'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.DualDoubleSumReducer.run','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.DoubleWritable.set'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.readPerplexity','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.writeTopicModel','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.submit'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.writeDocTopicInference','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.filecache.DistributedCache.setCacheFiles org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.submit'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.modelPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.stage1OutputPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.perplexityPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.getCurrentIterationNumber','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.runIteration','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.setModelPaths','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.conf.Configuration.setStrings'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.getModelPaths','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.lda.cvb.CVB0Driver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.thundergate.runtime.cache.mapreduce.CacheBuildClient.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'com.asakusafw.thundergate.runtime.cache.mapreduce.CacheBuildClient.update','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.FileUtil.copy'
'com.asakusafw.thundergate.runtime.cache.mapreduce.CacheBuildClient.create','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.FileUtil.copy'
'com.asakusafw.thundergate.runtime.cache.mapreduce.CacheBuildClient.getNextDirectory','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.mapreduce.CacheBuildClient.getNextProperties','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.mapreduce.CacheBuildClient.getNextContents','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.mapreduce.CacheBuildClient.getEscapeDir','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.bulkloader.cache.CacheBuildTest.setUp','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.bulkloader.cache.CacheBuildTest.collect','org.apache.hadoop.fs.FileSystem.globStatus'
'com.asakusafw.bulkloader.cache.CacheBuildTest.collectContent','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileStatus.getPath'
'test.modelgen.table.model.CacheFiles.setFilePath','org.apache.hadoop.io.Text.modify'
'org.commoncrawl.service.listcrawler.CacheManager.CacheManager','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.listcrawler.CacheManager.loadHDFSIndexFiles','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.listcrawler.CacheManager.loadCacheItemFromDisk','org.apache.hadoop.record.Buffer.<init>'
'org.commoncrawl.service.listcrawler.CacheManager.readVLongFromByteBuffer','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.commoncrawl.service.listcrawler.CacheManager.main','org.apache.hadoop.record.Buffer.<init> org.apache.hadoop.record.Buffer.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.CacheStorage','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.getConfiguration','org.apache.hadoop.fs.FileSystem.getConf'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.deleteHead','org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.deletePatch','org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.getCacheInfo','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.putCacheInfo','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.deleteAll','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.getHeadDirectory','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.getHeadProperties','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.getHeadContents','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.getPatchDirectory','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.getPatchProperties','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorage.getPatchContents','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorageTest.deleteHead','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'com.asakusafw.thundergate.runtime.cache.CacheStorageTest.deletePatch','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'com.asakusafw.thundergate.runtime.cache.CacheStorageTest.deleteAll','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'com.asakusafw.thundergate.runtime.cache.CacheStorageTest.deleteAll_missing','org.apache.hadoop.conf.Configuration.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorageTest.putPatchCacheInfo','org.apache.hadoop.conf.Configuration.<init>'
'com.asakusafw.thundergate.runtime.cache.CacheStorageTest.putHeadCacheInfo','org.apache.hadoop.conf.Configuration.<init>'
'tap.util.CacheUtils.addMapToCache','org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'tap.util.CacheUtils.getFromCache','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'org.commoncrawl.service.listcrawler.CacheWriterThread.run','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.apache.mahout.clustering.lda.cvb.CachingCVB0PerplexityMapper.setup','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat'
'org.apache.mahout.clustering.lda.cvb.CachingCVB0PerplexityMapper.map','org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.DoubleWritable.set'
'org.apache.mahout.math.stats.entropy.CalculateEntropyMapper.map','org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.math.stats.entropy.CalculateEntropyReducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.set'
'org.apache.mahout.math.stats.entropy.CalculateSpecificConditionalEntropyMapper.map','org.apache.hadoop.io.NullWritable.get'
'org.apache.oozie.service.CallableQueueService.init','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getStringCollection org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.CallbackService.createCallBackUrl','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.CallbackService.createCallBackUrl','org.apache.hadoop.conf.Configuration.get'
'mapreduce.CampaignMR1.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'mapreduce.CampaignMR1.MyReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init>'
'mapreduce.CampaignMR1.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'mapreduce.CampaignMR1.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.canopy.CanopyClusterer.configure','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'clusterer.CanopyClusterer.CanopyClusterer','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'clusterer.CanopyClusterer.loadClusters','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.mahout.clustering.canopy.CanopyClusterer.configure','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.canopy.CanopyClusterer.emitPointToClosestCanopy','org.apache.hadoop.io.IntWritable.<init>'
'mia.clustering.ch09.CanopyClustering.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.CanopyDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.canopy.CanopyDriver.run','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.canopy.CanopyDriver.buildClustersSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.CanopyDriver.buildClustersMR','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.canopy.CanopyDriver.clusterData','org.apache.hadoop.fs.Path.<init>'
'mapreduce.CanopyDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'lsh.mahout.clustering.CanopyTest.loadVectors','org.apache.hadoop.io.LongWritable.<init>'
'lsh.mahout.clustering.CanopyTest.getSequenceFileReader','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified'
'com.senseidb.indexing.hadoop.demo.CarMapInputConverter.getJsonInput','org.apache.hadoop.io.Text.toString'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.init','org.apache.hadoop.mapred.RunningJob.getID org.apache.hadoop.mapred.RunningJob.getJobName'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.getMapProgress','org.apache.hadoop.mapred.RunningJob.mapProgress'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.getReduceProgress','org.apache.hadoop.mapred.RunningJob.reduceProgress'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.isJobComplete','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.isJobSuccessful','org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.getFailureInfo','org.apache.hadoop.mapred.RunningJob.getFailureInfo'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.getStatus','org.apache.hadoop.mapred.RunningJob.getJobState org.apache.hadoop.mapred.JobStatus.getJobRunState'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.isJobCompleteNoTS','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.wso2.carbon.mapred.reporting.CarbonJobReporter.getCounters','org.apache.hadoop.mapred.RunningJob.getCounters'
'backtype.support.CascadingUtils.markSuccessfulOutputDir','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.initialize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.conf.Configuration.getLong'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.makeAbsolute','org.apache.hadoop.fs.Path.isAbsolute org.apache.hadoop.fs.Path.<init>'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.mkdirs','org.apache.hadoop.fs.Path.getParent'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.mkdir','org.apache.hadoop.fs.viewfs.InodeTree.INode.<init> org.apache.hadoop.fs.viewfs.InodeTree.INode.isFile'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.isFile','org.apache.hadoop.fs.viewfs.InodeTree.INode.isFile'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.checkFile','org.apache.hadoop.fs.viewfs.InodeTree.INode.isDirectory'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.listStatus','org.apache.hadoop.fs.viewfs.InodeTree.INode.isFile org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.equals org.apache.hadoop.fs.Path.makeQualified'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.create','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FSDataOutputStream.<init>'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.open','org.apache.hadoop.fs.FSDataInputStream.<init>'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.rename','org.apache.hadoop.fs.viewfs.InodeTree.INode.isDirectory org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.viewfs.InodeTree.INode.isFile'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.renameRecursive','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.delete','org.apache.hadoop.fs.viewfs.InodeTree.INode.isFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.getFileBlockLocations','org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.viewfs.InodeTree.INode.getBlocks'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.getFileStatus','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.makeQualified'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.CassandraFileStatus.CassandraFileStatus','org.apache.hadoop.fs.viewfs.InodeTree.INode.isDirectory'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.CassandraFileStatus.findLength','org.apache.hadoop.fs.viewfs.InodeTree.INode.isDirectory org.apache.hadoop.fs.viewfs.InodeTree.INode.getBlocks'
'org.apache.cassandra.hadoop.fs.CassandraFileSystem.CassandraFileStatus.findBlocksize','org.apache.hadoop.fs.viewfs.InodeTree.INode.getBlocks'
'org.apache.cassandra.hadoop.fs.CassandraFileSystemTest.testFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.BlockLocation.getHosts org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.initConsistencyLevels','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.storeINode','org.apache.hadoop.fs.Path.toUri'
'org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.getParentForIndex','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.toUri'
'org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.getPathKey','org.apache.hadoop.fs.Path.toUri'
'org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listSubPaths','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.getBlockLocation','org.apache.hadoop.fs.BlockLocation.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.createDatabase','org.apache.hadoop.hive.metastore.api.Database.getName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getDatabase','org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName org.apache.hadoop.hive.metastore.api.NoSuchObjectException.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getDatabases','org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Database.getName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.alterDatabase','org.apache.hadoop.hive.metastore.api.Table.deepCopy org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Table.setDbName org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.MetaException.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.dropDatabase','org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.createTable','org.apache.hadoop.hive.metastore.api.Table.getDbName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getTable','org.apache.hadoop.hive.metastore.api.Table.<init> org.apache.hadoop.hive.metastore.api.Table.setTableName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getTables','org.apache.hadoop.hive.metastore.api.Table.<init> org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Table.getTableName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.alterTable','org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Table.<init> org.apache.hadoop.hive.metastore.api.Table.setDbName org.apache.hadoop.hive.metastore.api.Table.setTableName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.updateTableComponents','org.apache.hadoop.hive.metastore.api.Partition.deepCopy org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Partition.setDbName org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Partition.setTableName org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Index.deepCopy org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Index.setDbName org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Index.setOrigTableName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.dropTable','org.apache.hadoop.hive.metastore.api.Table.<init> org.apache.hadoop.hive.metastore.api.Table.setDbName org.apache.hadoop.hive.metastore.api.Table.setTableName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.addIndex','org.apache.hadoop.hive.metastore.api.Index.getParameters org.apache.hadoop.hive.metastore.api.Index.getParameters org.apache.hadoop.hive.metastore.api.Index.getDbName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getIndex','org.apache.hadoop.hive.metastore.api.Index.<init> org.apache.hadoop.hive.metastore.api.Index.setDbName org.apache.hadoop.hive.metastore.api.Index.setIndexName org.apache.hadoop.hive.metastore.api.Index.setOrigTableName org.apache.hadoop.hive.metastore.api.MetaException.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getIndexes','org.apache.hadoop.hive.metastore.api.Index.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.listIndexNames','org.apache.hadoop.hive.metastore.api.Index.getIndexName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.dropIndex','org.apache.hadoop.hive.metastore.api.Index.<init> org.apache.hadoop.hive.metastore.api.Index.setDbName org.apache.hadoop.hive.metastore.api.Index.setOrigTableName org.apache.hadoop.hive.metastore.api.Index.setIndexName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.addPartition','org.apache.hadoop.hive.metastore.api.Partition.getDbName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getPartition','org.apache.hadoop.hive.metastore.api.Partition.<init> org.apache.hadoop.hive.metastore.api.Partition.setDbName org.apache.hadoop.hive.metastore.api.Partition.setTableName org.apache.hadoop.hive.metastore.api.Partition.setValues org.apache.hadoop.hive.metastore.api.NoSuchObjectException.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getPartitions','org.apache.hadoop.hive.metastore.api.Partition.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.listPartitionNames','org.apache.hadoop.hive.metastore.api.Partition.getSd'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.alterPartition','org.apache.hadoop.hive.metastore.api.Partition.getValues org.apache.hadoop.hive.metastore.api.NoSuchObjectException.getMessage org.apache.hadoop.hive.metastore.api.InvalidObjectException.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.dropPartition','org.apache.hadoop.hive.metastore.api.Partition.<init> org.apache.hadoop.hive.metastore.api.Partition.setDbName org.apache.hadoop.hive.metastore.api.Partition.setTableName org.apache.hadoop.hive.metastore.api.Partition.setValues'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getRole','org.apache.hadoop.hive.metastore.api.NoSuchObjectException.<init>'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.getType','org.apache.hadoop.hive.metastore.api.Type.<init> org.apache.hadoop.hive.metastore.api.Type.setName'
'org.apache.cassandra.hadoop.hive.metastore.CassandraHiveMetaStore.dropType','org.apache.hadoop.hive.metastore.api.Type.<init> org.apache.hadoop.hive.metastore.api.Type.setName'
'org.gora.cassandra.query.CassandraPartitionQuery.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'org.gora.cassandra.query.CassandraPartitionQuery.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide','org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentValue'
'org.apache.cassandra.hadoop.pig.CassandraStorage.getNext','org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentValue'
'org.apache.cassandra.hadoop.pig.CassandraStorage.setLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.cassandra.hadoop.pig.CassandraStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.cassandra.hadoop.pig.CassandraStorage.writeMutations','org.apache.hadoop.mapreduce.RecordWriter<java.nio.ByteBuffer,java.util.List<org.apache.cassandra.thrift.Mutation>>.write'
'org.apache.cassandra.hadoop.pig.CassandraStorage.getNext','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentKey org.apache.hadoop.mapreduce.RecordReader.getCurrentValue'
'org.apache.cassandra.hadoop.pig.CassandraStorage.setLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.cassandra.hadoop.pig.CassandraStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.cassandra.hadoop.pig.CassandraStorage.putNext','org.apache.hadoop.mapreduce.RecordWriter.write'
'org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide','org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentValue'
'org.apache.cassandra.hadoop.pig.CassandraStorage.getNext','org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<java.nio.ByteBuffer,java.util.Map<java.nio.ByteBuffer,org.apache.cassandra.db.IColumn>>.getCurrentValue'
'org.apache.cassandra.hadoop.pig.CassandraStorage.setLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.cassandra.hadoop.pig.CassandraStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.cassandra.hadoop.pig.CassandraStorage.writeMutations','org.apache.hadoop.mapreduce.RecordWriter<java.nio.ByteBuffer,java.util.List<org.apache.cassandra.thrift.Mutation>>.write'
'org.pingles.cascading.cassandra.CassandraTap.sourceInit','org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.FileInputFormat.addInputPaths org.apache.hadoop.mapred.JobConf.setInputFormat'
'org.pingles.cascading.cassandra.CassandraTap.endpointInit','org.apache.hadoop.mapred.JobConf.get'
'org.pingles.cascading.cassandra.CassandraTap.getPath','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.example.direct.seqfile.jobflow.format.CategorySummarySeqFileFormat.createKeyObject','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.example.direct.seqfile.jobflow.format.CategorySummarySeqFileFormat.copyFromModel','org.apache.hadoop.io.Text.set'
'org.apache.whirr.service.cdh.integration.Cdh3HBaseServiceTest.test','org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.<init> org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.<init> org.apache.hadoop.hbase.thrift.generated.Mutation.<init> org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.whirr.service.cdh.integration.CdhHadoopServiceTest.testJobExecution','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init>'
'org.apache.whirr.service.cdh.integration.CdhHadoopServiceTest.checkHadoop','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.whirr.service.cdh.integration.CdhHadoopServiceTest.getConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.whirr.service.cdh.integration.CdhHadoopServiceTest.waitForTaskTrackers','org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers'
'ivory.core.data.stat.CfTableArray.CfTableArray','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.fs.FSDataInputStream.close'
'com.nearinfinity.blur.store.hdfs.ChangeFileExt.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.ChangeIDHandler.get','org.apache.hadoop.fs.FileStatus.getModificationTime'
'org.apache.accumulo.server.master.balancer.ChaoticLoadBalancerTest.makeExtent','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.balancer.ChaoticLoadBalancerTest.toText','org.apache.hadoop.io.Text.<init>'
'edu.jhu.thrax.hadoop.features.CharacterCountDifferenceFeature.score','org.apache.hadoop.io.IntWritable.<init>'
'org.commoncrawl.util.CharsetUtils.detectCharacterEncoding','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'client.CheckAndDeleteExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.checkAndDelete org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.checkAndDelete org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.checkAndDelete'
'org.apache.accumulo.server.util.CheckForMetadataProblems.checkMetadataTableEntries','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.server.util.CheckForMetadataProblems.main','org.apache.hadoop.fs.FileSystem.get'
'org.goldenorb.io.output.checkpoint.CheckPointDataOutput.CheckPointDataOutput','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getStatistics'
'org.goldenorb.io.checkpoint.CheckPointDataTest.setUpCluster','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem'
'org.goldenorb.io.checkpoint.CheckPointDataTest.testCheckpointOutput','org.apache.hadoop.hdfs.MiniDFSCluster.getNameNodePort org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.write org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.write org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.write'
'org.goldenorb.io.checkpoint.CheckPointDataTest.testCheckpointInput','org.apache.hadoop.hdfs.MiniDFSCluster.getNameNodePort org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.readFields org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.FloatWritable.get'
'org.goldenorb.io.checkpoint.CheckPointDataTest.tearDownCluster','org.apache.hadoop.fs.FileSystem.close'
'org.apache.accumulo.examples.simple.filedata.ChunkInputStreamTest.addData','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.james.mailbox.hbase.io.ChunkOutputStream.writeData','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.head org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'org.apache.mahout.text.ChunkedWriter.ChunkedWriter','org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.text.ChunkedWriter.getPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.text.ChunkedWriter.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getBytes'
'com.nearinfinity.hadoop.patent.CitationHistogram.MapClass.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set'
'com.nearinfinity.hadoop.patent.CitationHistogram.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'com.nearinfinity.hadoop.patent.CitationHistogram.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.nearinfinity.hadoop.patent.CitationHistogram.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.sqoop.orm.ClassWriter.rpcGetterForType','org.apache.hadoop.io.BytesWritable.getName'
'org.apache.sqoop.orm.ClassWriter.rpcSetterForType','org.apache.hadoop.io.BytesWritable.getName'
'org.apache.sqoop.orm.ClassWriter.generateCloneMethod','org.apache.hadoop.io.BytesWritable.getName'
'org.apache.mahout.classifier.df.mapreduce.Classifier.Classifier','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.df.mapreduce.Classifier.configureJob','org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass'
'org.apache.mahout.classifier.df.mapreduce.Classifier.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.classifier.df.mapreduce.Classifier.parseOutput','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeChars org.apache.hadoop.fs.FSDataOutputStream.writeChar'
'org.apache.mahout.classifier.df.mapreduce.Classifier.CMapper.setup','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.df.mapreduce.Classifier.CMapper.map','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.Text.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.Text.set'
'org.apache.mahout.pig.Classifier.Classifier','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.mahout.pig.Classifier.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'org.apache.mahout.pig.Classifier.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'com.asakusafw.operation.tools.hadoop.fs.Clean.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.Tool.setConf org.apache.hadoop.util.Tool.run'
'com.asakusafw.operation.tools.hadoop.fs.Clean.parseOptions','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.operation.tools.hadoop.fs.Clean.remove','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getModificationTime'
'com.asakusafw.operation.tools.hadoop.fs.Clean.Context.isSymlink','org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.operation.tools.hadoop.fs.Clean.Context.isSymlink0','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.makeQualified'
'com.asakusafw.operation.tools.hadoop.fs.Clean.Context.canDelete','org.apache.hadoop.fs.FileStatus.getModificationTime'
'org.megalon.Client.put','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'org.megalon.Client.deleteColumn','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumn'
'org.megalon.Client.batch','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.batch org.apache.hadoop.hbase.client.HTablePool.putTable'
'com.basho.riak.hadoop.config.ClientFactoryTest.getClusterClient_die','org.apache.hadoop.conf.Configuration.<init>'
'com.cloudera.hadoop.hdfs.nfs.rpc.ClientInputHandler.ClientInputHandler','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'com.cloudera.hadoop.hdfs.nfs.rpc.ClientInputHandler.run','org.apache.hadoop.io.IOUtils.closeSocket'
'org.apache.accumulo.core.client.ClientSideIteratorScanner.ScannerTranslator.seek','org.apache.hadoop.io.Text.<init>'
'com.taobao.adfs.distributed.rpc.Client.setPingInterval','org.apache.hadoop.conf.Configuration.setInt'
'com.taobao.adfs.distributed.rpc.Client.getPingInterval','org.apache.hadoop.conf.Configuration.getInt'
'com.taobao.adfs.distributed.rpc.Client.Call.processResult','org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields'
'com.taobao.adfs.distributed.rpc.Client.Connection.setupIOstreams','org.apache.hadoop.net.NetUtils.getInputStream org.apache.hadoop.net.NetUtils.getOutputStream'
'com.taobao.adfs.distributed.rpc.Client.Connection.writeHeader','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData'
'com.taobao.adfs.distributed.rpc.Client.Connection.sendParam','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.IOUtils.closeStream'
'com.taobao.adfs.distributed.rpc.Client.Connection.close','org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'com.taobao.adfs.distributed.rpc.Client.Client','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.net.NetUtils.getDefaultSocketFactory'
'com.bah.culvert.Client.getIndices','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getClass'
'com.bah.culvert.Client.getIndicesForTable','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.get'
'com.bah.culvert.Client.addIndex','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.set'
'com.bah.culvert.Client.setDatabase','org.apache.hadoop.conf.Configuration.set'
'com.bah.culvert.Client.getDatabaseAdapter','org.apache.hadoop.conf.Configuration.getClass'
'com.bah.culvert.Client.setDatabaseAdapter','org.apache.hadoop.conf.Configuration.setClass'
'com.linkedin.mr_kluj.ClojureCombiner.setup','org.apache.hadoop.conf.Configuration.get'
'com.linkedin.mr_kluj.ClojureReducer.setup','org.apache.hadoop.conf.Configuration.get'
'org.cloudata.core.common.conf.CloudataConf.CloudataConf','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.core.common.conf.CloudataConf.setJobConf','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.core.parallel.hadoop.CloudataMapReduceUtil.initMapReduce','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addArchiveToClassPath'
'org.cloudata.core.parallel.hadoop.CloudataMapReduceUtil.uploadFile','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toUri'
'org.cloudata.core.parallel.hadoop.CloudataMapReduceUtil.makeJarToHDFS','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.cloudata.core.parallel.hadoop.CloudataMapReduceUtil.getMapNum','org.apache.hadoop.mapred.JobConf.get'
'edu.umd.cloud9.collection.clue.ClueCollectionPathConstants.addEnglishTestFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath'
'edu.umd.cloud9.collection.clue.ClueCollectionPathConstants.addEnglishTinyCollection','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath'
'edu.umd.cloud9.collection.clue.ClueCollectionPathConstants.addEnglishSmallCollection','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath'
'edu.umd.cloud9.collection.clue.ClueCollectionPathConstants.addEnglishCompleteCollection','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath'
'edu.umd.cloud9.collection.clue.ClueCollectionPathConstants.addEnglishCollectionPart','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath'
'edu.umd.cloud9.webgraph.ClueExtractLinks.Map.configure','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.get'
'edu.umd.cloud9.webgraph.ClueExtractLinks.Map.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect'
'edu.umd.cloud9.webgraph.ClueExtractLinks.Reduce.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect'
'edu.umd.cloud9.webgraph.ClueExtractLinks.runTool','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapred.SequenceFileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.collection.clue.ClueWarcDocnoMapping.loadMapping','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.util.LineReader.close'
'edu.umd.cloud9.collection.clue.ClueWarcDocnoMappingBuilder.build','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'edu.umd.cloud9.collection.clue.ClueWarcDocnoMappingBuilder.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.collection.clue.ClueWarcDocnoMappingBuilder.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.clue.ClueWarcDocnoMappingTest.testDocidToDocno','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.collection.clue.ClueWarcDocnoMappingTest.testDocnoToDocid','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.collection.clue.ClueWarcInputFormat.ClueWarcRecordReader.ClueWarcRecordReader','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream'
'edu.umd.cloud9.collection.clue.ClueWarcInputFormat.ClueWarcRecordReader.next','org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.LongWritable.set'
'edu.umd.cloud9.collection.clue.ClueWarcInputFormat.ClueWarcRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'ivory.regression.sigir2011.Clue_VaryingTradeoff_FeaturePrune.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.mahout.clustering.classify.ClusterClassificationDriver.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.classify.ClusterClassificationDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.classify.ClusterClassificationDriver.finalClustersPath','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.clustering.classify.ClusterClassificationDriver.selectCluster','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.classify.ClusterClassificationDriver.write','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.classify.ClusterClassificationDriver.classifyClusterMR','org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.vmware.bdd.cli.commands.ClusterCommands.targetCluster','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.vmware.bdd.cli.commands.ClusterCommands.setFsURL','org.apache.hadoop.conf.Configuration.set'
'com.vmware.bdd.cli.commands.ClusterCommands.setJobTrackerURL','org.apache.hadoop.conf.Configuration.set'
'edu.duke.starfish.profile.profileinfo.ClusterConfiguration.ClusterConfiguration','org.apache.hadoop.mapred.JobTracker.getAddress org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getMaxMapTasks org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.ClusterStatus.getMaxReduceTasks org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.ClusterStatus.getActiveTrackerNames'
'org.sleuthkit.hadoop.ClusterDocumentsJob.runPipeline','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.utils.clustering.ClusterDumper.ClusterDumper','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.utils.clustering.ClusterDumper.run','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.utils.clustering.ClusterDumper.printClusters','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.utils.clustering.ClusterDumper.init','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.evaluation.ClusterEvaluator.ClusterEvaluator','org.apache.hadoop.conf.Configuration.get'
'com.mozilla.hadoop.ClusterHealth.testThrift','org.apache.hadoop.hbase.thrift.generated.IOError.getMessage'
'com.mozilla.hadoop.ClusterHealth.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hdfs.DFSClient.<init> org.apache.hadoop.hdfs.DFSClient.datanodeReport org.apache.hadoop.hdfs.protocol.DatanodeInfo.getDfsUsed org.apache.hadoop.hdfs.protocol.DatanodeInfo.getNonDfsUsed org.apache.hadoop.hdfs.protocol.DatanodeInfo.getCapacity org.apache.hadoop.hdfs.protocol.DatanodeInfo.getHostName org.apache.hadoop.hdfs.protocol.DatanodeInfo.getXceiverCount org.apache.hadoop.hdfs.DFSClient.datanodeReport org.apache.hadoop.hdfs.protocol.DatanodeInfo.getHostName org.apache.hadoop.hdfs.DFSClient.close org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus org.apache.hadoop.hbase.ClusterStatus.getRegionsCount org.apache.hadoop.hbase.ClusterStatus.getRequestsCount org.apache.hadoop.hbase.ClusterStatus.getServerInfo org.apache.hadoop.hbase.HServerInfo.getLoad org.apache.hadoop.hbase.HServerLoad.getUsedHeapMB org.apache.hadoop.hbase.HServerLoad.getMaxHeapMB org.apache.hadoop.hbase.HServerLoad.getNumberOfRegions org.apache.hadoop.hbase.HServerLoad.getNumberOfRequests org.apache.hadoop.hbase.HServerInfo.getHostname org.apache.hadoop.hbase.HServerLoad.getUsedHeapMB org.apache.hadoop.hbase.HServerLoad.getMaxHeapMB org.apache.hadoop.hbase.HServerLoad.getNumberOfRegions org.apache.hadoop.hbase.HServerLoad.getNumberOfRequests org.apache.hadoop.hbase.ClusterStatus.getDeadServers org.apache.hadoop.hbase.ClusterStatus.getDeadServerNames'
'org.sleuthkit.hadoop.ClusterJSONBuilder.buildReport','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'org.sleuthkit.hadoop.ClusterJSONBuilder.writeFileToStream','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataInputStream.read'
'org.apache.mahout.utils.vectors.lucene.ClusterLabels.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.ClusterMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorDriver.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorDriver.postProcessSeq','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorDriver.postProcessMR','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorDriver.movePartFilesToRespectiveDirectories','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorDriver.renameFile','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.Writable.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest.testTopDownClustering','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest.getVectorsInCluster','org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest.assertBottomLevelCluster','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.ClusterStatus.write','org.apache.hadoop.io.WritableUtils.writeEnum'
'org.apache.hama.bsp.ClusterStatus.readFields','org.apache.hadoop.io.WritableUtils.readEnum'
'com.stumbleupon.hbaseadmin.ClusterUtils.updateStatus','org.apache.hadoop.hbase.client.HBaseAdmin.getConfiguration org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus org.apache.hadoop.hbase.client.HBaseAdmin.getConnection org.apache.hadoop.hbase.ClusterStatus.getServers org.apache.hadoop.hbase.ServerName.getHostname org.apache.hadoop.hbase.ServerName.getPort org.apache.hadoop.hbase.client.HConnection.getHRegionConnection org.apache.hadoop.hbase.ServerName.getHostAndPort org.apache.hadoop.hbase.ServerName.getServerName org.apache.hadoop.hbase.ipc.HRegionInterface.getOnlineRegions org.apache.hadoop.hbase.ipc.HRegionInterface.getOnlineRegions org.apache.hadoop.hbase.ServerName.getHostname'
'com.stumbleupon.hbaseadmin.ClusterUtils.isRegionLive','org.apache.hadoop.hbase.HRegionInfo.getStartKey org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.HRegionInfo.getRegionName org.apache.hadoop.hbase.HRegionInfo.getTableName org.apache.hadoop.hbase.client.Scan.setBatch org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HBaseAdmin.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'com.stumbleupon.hbaseadmin.ClusterUtils.getServerHostingRegion','org.apache.hadoop.hbase.client.HBaseAdmin.getConnection org.apache.hadoop.hbase.HRegionInfo.isRootRegion org.apache.hadoop.hbase.client.HConnection.getZooKeeperWatcher org.apache.hadoop.hbase.Abortable.<init> org.apache.hadoop.hbase.zookeeper.RootRegionTracker.<init> org.apache.hadoop.hbase.zookeeper.RootRegionTracker.start org.apache.hadoop.hbase.zookeeper.RootRegionTracker.isLocationAvailable org.apache.hadoop.hbase.zookeeper.RootRegionTracker.getRootRegionLocation org.apache.hadoop.hbase.zookeeper.RootRegionTracker.stop org.apache.hadoop.hbase.ServerName.getHostname org.apache.hadoop.hbase.ServerName.getPort org.apache.hadoop.hbase.client.HBaseAdmin.getConfiguration org.apache.hadoop.hbase.HRegionInfo.isMetaRegion org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.HRegionInfo.getRegionName org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.stumbleupon.hbaseadmin.ClusterUtils.moveRegion','org.apache.hadoop.hbase.HRegionInfo.getEncodedNameAsBytes org.apache.hadoop.hbase.client.HBaseAdmin.move'
'com.inmobi.databus.Cluster.Cluster','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.inmobi.databus.Cluster.getUnqaulifiedFinalDestDirRoot','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'com.inmobi.databus.Cluster.getTrashPath','org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.Cluster.getTrashPathWithDate','org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.Cluster.getTrashPathWithDateHour','org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.Cluster.getDataDir','org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.Cluster.getConsumePath','org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.Cluster.getMirrorConsumePath','org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.Cluster.getTmpPath','org.apache.hadoop.fs.Path.<init>'
'tv.floe.caduceus.mahout.clustering.kmeans.ClusteringDemoPointFileWriter.writePointsToSequenceFileInHDFS','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init>'
'tv.floe.caduceus.mahout.clustering.kmeans.ClusteringDemoPointFileWriter.writeClustersAndPointsToHDFS','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.ClusteringTestUtils.writePointsToFile','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.clustering.display.ClustersFilter.accept','org.apache.hadoop.fs.Path.toString'
'com.odiago.flumebase.client.CmdLineClient.CmdLineClient','org.apache.hadoop.conf.Configuration.<init>'
'com.odiago.flumebase.client.CmdLineClient.connect','org.apache.hadoop.conf.Configuration.getInt'
'com.odiago.flumebase.client.CmdLineClient.setProperty','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get'
'com.odiago.flumebase.client.CmdLineClient.run','org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.tool.CodeGenTool.run','org.apache.hadoop.util.StringUtils.stringifyException'
'.CoherencyModelTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem'
'.CoherencyModelTest.tearDown','org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'.CoherencyModelTest.fileExistsImmediatelyAfterCreation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'.CoherencyModelTest.fileContentIsNotVisibleAfterFlush','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.delete'
'.CoherencyModelTest.fileContentIsVisibleAfterFlushAndSync','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.sync org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.delete'
'.CoherencyModelTest.fileContentIsVisibleAfterClose','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.delete'
'com.datasalt.pangool.utils.test.CollectionInputFormat.createRecordReader','org.apache.hadoop.mapreduce.RecordReader<com.datasalt.pangool.utils.test.K,com.datasalt.pangool.utils.test.V>.<init>'
'com.urbanairship.datacube.CollectionWritableTest.basicTest','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.urbanairship.datacube.CollectionWritableTest.emptyCollection','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.mahout.vectorizer.collocations.llr.CollocDriver.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.vectorizer.collocations.llr.CollocDriver.run','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.vectorizer.collocations.llr.CollocDriver.generateCollocations','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'org.apache.mahout.vectorizer.collocations.llr.CollocDriver.computeNGramsPruneByLLR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.vectorizer.collocations.llr.CollocDriver.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.vectorizer.collocations.llr.CollocDriver.run','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.vectorizer.collocations.llr.CollocDriver.generateCollocations','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'org.apache.mahout.vectorizer.collocations.llr.CollocDriver.computeNGramsPruneByLLR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.vectorizer.collocations.llr.CollocMapper.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest.testCollectNgrams','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.Counters.Counter.increment'
'org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest.testCollectNgramsWithUnigrams','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapred.Counters.Counter.increment'
'org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest.testCollectNgrams','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.Counters.Counter.increment'
'org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest.testCollectNgramsWithUnigrams','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapred.Counters.Counter.increment'
'org.apache.mahout.vectorizer.collocations.llr.CollocMapper.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.vectorizer.collocations.llr.CollocReducer.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.data.Column.compareBytes','org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.accumulo.core.data.Column.hash','org.apache.hadoop.io.WritableComparator.hashBytes'
'org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init>'
'org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getRecordReader','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapreduce.TaskAttemptID.forName org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.cassandra.hadoop.ColumnFamilyInputFormat.progress','org.apache.hadoop.mapred.Reporter.progress'
'org.apache.cassandra.hadoop.ColumnFamilyInputFormatTest.testSlicePredicate','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.ColumnFamilyRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong'
'org.apache.accumulo.core.iterators.system.ColumnFilterTest.nk','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.ColumnMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'meetup.beeno.filter.ColumnMatchFilter.filterKeyValue','org.apache.hadoop.hbase.KeyValue.matchingColumn org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.KeyValue.getValue'
'meetup.beeno.filter.ColumnMatchFilter.readFields','org.apache.hadoop.hbase.util.Bytes.readByteArray'
'meetup.beeno.filter.ColumnMatchFilter.write','org.apache.hadoop.hbase.util.Bytes.writeByteArray'
'org.apache.accumulo.core.iterators.conf.ColumnSet.add','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.conf.ColumnSet.encode','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes'
'org.apache.accumulo.core.iterators.conf.ColumnSet.decode','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.iterators.conf.ColumnToClassMapping.addObject','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.security.ColumnVisibility.NodeComparator.compare','org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.sqoop.mapreduce.CombineFileRecordReader.initialize','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.initialize'
'org.apache.sqoop.mapreduce.CombineFileRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.nextKeyValue'
'org.apache.sqoop.mapreduce.CombineFileRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.getCurrentKey'
'org.apache.sqoop.mapreduce.CombineFileRecordReader.getCurrentValue','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.getCurrentValue'
'org.apache.sqoop.mapreduce.CombineFileRecordReader.close','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.close'
'org.apache.sqoop.mapreduce.CombineFileRecordReader.getProgress','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.getProgress org.apache.hadoop.mapreduce.lib.input.CombineFileSplit.getLength org.apache.hadoop.mapreduce.lib.input.CombineFileSplit.getLength'
'org.apache.sqoop.mapreduce.CombineFileRecordReader.initNextRecordReader','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.close org.apache.hadoop.mapreduce.lib.input.CombineFileSplit.getLength org.apache.hadoop.mapreduce.lib.input.CombineFileSplit.getNumPaths org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.CombineFileSplit.getPath org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.lib.input.CombineFileSplit.getOffset org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.mapreduce.lib.input.CombineFileSplit.getLength org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.initialize'
'edu.isi.mavuno.extract.CombineGlobalStats.run','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'net.ripe.hadoop.pcap.io.reader.CombinePcapRecordReader.CombinePcapRecordReader','org.apache.hadoop.mapred.lib.CombineFileSplit.getPath org.apache.hadoop.mapred.lib.CombineFileSplit.getLength'
'edu.isi.mavuno.score.CombineScores.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.DoubleWritable.set'
'edu.isi.mavuno.score.CombineScores.MyReducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.set'
'edu.isi.mavuno.score.CombineScores.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.util.CombineSequenceFiles.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.util.CombineSequenceFiles.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.sqoop.mapreduce.CombineShimRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,java.lang.Object>.initialize'
'org.apache.sqoop.mapreduce.CombineShimRecordReader.getProgress','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,java.lang.Object>.getProgress'
'org.apache.sqoop.mapreduce.CombineShimRecordReader.close','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,java.lang.Object>.close'
'org.apache.sqoop.mapreduce.CombineShimRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,java.lang.Object>.getCurrentKey'
'org.apache.sqoop.mapreduce.CombineShimRecordReader.getCurrentValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,java.lang.Object>.getCurrentValue'
'org.apache.sqoop.mapreduce.CombineShimRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,java.lang.Object>.nextKeyValue'
'org.apache.sqoop.mapreduce.CombineShimRecordReader.createChildReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance'
'edu.isi.mavuno.extract.CombineSplits.MyReducer.setup','org.apache.hadoop.conf.Configuration.get'
'edu.isi.mavuno.extract.CombineSplits.run','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.accumulo.core.iterators.user.CombinerTest.nk','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.CombinerTest.nr','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.common.CommandLineUtil.printHelpWithGenericOptions','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init>'
'org.pentaho.hadoop.shim.common.CommonHadoopShim.getHadoopVersion','org.apache.hadoop.util.VersionInfo.getVersion'
'org.pentaho.hadoop.shim.common.CommonHadoopShim.getHiveJdbcDriver','org.apache.hadoop.hive.jdbc.HiveDriver.<init>'
'org.pentaho.hadoop.shim.common.CommonHadoopShim.getFileSystem','org.apache.hadoop.fs.FileSystem.get'
'org.pentaho.hadoop.shim.common.CommonHadoopShim.getNamenodeConnectionInfo','org.apache.hadoop.fs.FileSystem.getDefaultUri'
'org.pentaho.hadoop.shim.common.CommonHadoopShim.getJobtrackerConnectionInfo','org.apache.hadoop.mapred.JobTracker.getAddress'
'org.pentaho.hadoop.shim.common.CommonHadoopShim.submitJob','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.submitJob'
'org.pentaho.hadoop.shim.common.CommonHadoopShimTest.getHadoopVersion','org.apache.hadoop.util.VersionInfo.getVersion'
'com.manning.hip.common.CommonLogEntry.write','org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString'
'com.manning.hip.common.CommonLogEntry.readFields','org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString'
'com.manning.hip.common.CommonLogInputFormat.isSplitable','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'com.manning.hip.common.CommonLogInputFormat.CommonLogRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize'
'com.manning.hip.common.CommonLogInputFormat.CommonLogRecordReader.close','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'com.manning.hip.common.CommonLogInputFormat.CommonLogRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentKey'
'com.manning.hip.common.CommonLogInputFormat.CommonLogRecordReader.getProgress','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress'
'com.manning.hip.common.CommonLogInputFormat.CommonLogRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue'
'org.pentaho.hadoop.shim.common.CommonSnappyShim.getSnappyInputStream','org.apache.hadoop.io.compress.SnappyCodec.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.compress.SnappyCodec.setConf org.apache.hadoop.io.compress.SnappyCodec.createInputStream'
'org.pentaho.hadoop.shim.common.CommonSnappyShim.getSnappyOutputStream','org.apache.hadoop.io.compress.SnappyCodec.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.compress.SnappyCodec.setConf org.apache.hadoop.io.compress.SnappyCodec.createOutputStream'
'org.wso2.carbon.hadoop.hive.jdbc.storage.utils.Commons.getObjectFromWritable','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.hive.serde2.io.ShortWritable.get org.apache.hadoop.hive.serde2.io.ByteWritable.get org.apache.hadoop.io.BooleanWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.Writable.toString'
'org.apache.accumulo.server.test.randomwalk.concurrent.Compact.visit','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.yahoo.omid.client.regionserver.Compacter.start','org.apache.hadoop.hbase.CoprocessorEnvironment.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'com.yahoo.omid.client.regionserver.Compacter.preCompact','org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.getEnvironment'
'com.yahoo.omid.client.regionserver.Compacter.CompacterScanner.next','org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getTimestamp'
'com.yahoo.omid.client.regionserver.Compacter.CompacterScanner.close','org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'org.softlang.company.Company.setName','org.apache.hadoop.io.Text.<init>'
'org.softlang.company.Company.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields'
'org.softlang.company.Company.write','org.apache.hadoop.io.Text.write'
'com.datasalt.pangool.tuplemr.mapred.ComparatorsBaseTest.assertPositive','org.apache.hadoop.io.RawComparator<com.datasalt.pangool.io.ITuple>.compare'
'com.datasalt.pangool.tuplemr.mapred.ComparatorsBaseTest.assertNegative','org.apache.hadoop.io.RawComparator<com.datasalt.pangool.io.ITuple>.compare'
'org.apache.accumulo.core.file.rfile.bcfile.CompareUtils.BytesComparator.compare','org.apache.hadoop.io.RawComparator<java.lang.Object>.compare'
'org.apache.accumulo.core.file.rfile.bcfile.CompareUtils.MemcmpRawComparator.compare','org.apache.hadoop.io.WritableComparator.compareBytes'
'com.sap.hadoop.metadata.CompositeDataType.CompositeDataType','org.apache.hadoop.io.Text.<init>'
'com.sap.hadoop.metadata.CompositeDataType.write','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'com.sap.hadoop.metadata.CompositeDataType.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'com.sap.hadoop.metadata.CompositeDataType.define','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.toString'
'com.sap.hadoop.metadata.CompositeDataType.CompositeWritableComparator.CompositeWritableComparator','org.apache.hadoop.io.DataInputBuffer.<init>'
'com.sap.hadoop.metadata.CompositeDataType.CompositeWritableComparator.setConf','org.apache.hadoop.conf.Configuration.get'
'com.sap.hadoop.metadata.CompositeDataType.CompositeWritableComparator.compare','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.crunch.io.CompositePathIterable.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.crunch.io.CompositePathIterable.create','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.crunch.io.CompositePathIterable.iterator','org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.crunch.io.CompositePathIterable.hasNext','org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.crunch.io.CompositePathIterableTest.testCreate_FilePresent','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.CompositePathIterableTest.testCreate_DirectoryPresentButNoFiles','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.CompositePathIterableTest.testCreate_DirectoryNotPresent','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'com.sap.hadoop.metadata.CompositeSerialization.CompositeDeserializer.CompositeDeserializer','org.apache.hadoop.conf.Configuration.get'
'com.sap.hadoop.metadata.CompositeSerialization.CompositeDeserializer.deserialize','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields'
'gov.llnl.ontology.mapreduce.stats.CompoundTokenCountMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.CompoundTokenCountMR.setupConfiguration','org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'gov.llnl.ontology.mapreduce.stats.CompoundTokenCountMR.setupReducer','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'gov.llnl.ontology.mapreduce.stats.CompoundTokenCountMR.CompoundTokenCountMapper.setup','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'gov.llnl.ontology.mapreduce.stats.CompoundTokenCountMR.CompoundTokenCountMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write org.apache.hadoop.mapreduce.Mapper.Context.getCounter'
'com.manning.hip.ch5.CompressedFileWrite.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'org.commoncrawl.util.CompressedIndex.Builder.BlockCompressor.BlockCompressor','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.io.compress.GzipCodec.setConf org.apache.hadoop.io.compress.GzipCodec.createCompressor'
'org.commoncrawl.util.CompressedIndex.Builder.BlockCompressor.reset','org.apache.hadoop.io.compress.Compressor.reset org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init>'
'org.commoncrawl.util.CompressedIndex.Builder.BlockCompressor.addItem','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.util.CompressedIndex.Builder.BlockCompressor.flush','org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.compress.GzipCodec.createOutputStream org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.compress.CompressionOutputStream.write org.apache.hadoop.io.compress.CompressionOutputStream.flush org.apache.hadoop.io.compress.CompressionOutputStream.close org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.FSDataOutputStream.writeByte org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush'
'org.commoncrawl.util.CompressedIndex.Builder.Builder','org.apache.hadoop.conf.Configuration.<init>'
'org.commoncrawl.util.CompressedIndex.Builder.close','org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FSDataOutputStream.close'
'com.manning.hip.ch5.CompressedMapReduce.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.hadoop.compression.snappy.CompressionTests.testSnappyCompression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.compress.CompressionOutputStream.write org.apache.hadoop.io.compress.CompressionOutputStream.flush org.apache.hadoop.io.compress.CompressionInputStream.available org.apache.hadoop.io.compress.CompressionInputStream.read'
'edu.isi.mavuno.app.distsim.ComputeContextScores.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'edu.isi.mavuno.app.distsim.ComputeContextScores.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixPairs.MyMapper.map','org.apache.hadoop.io.Text.toString'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixPairs.MyReducer.reduce','org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixPairs.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixPairs.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixPairs.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixStripes.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixStripes.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixStripes.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.cooccur.ComputeCooccurrenceMatrixStripes.main','org.apache.hadoop.util.ToolRunner.run'
'ivory.core.preprocess.ComputeGlobalTermStatistics.MyMapper.map','org.apache.hadoop.io.Text.set'
'ivory.core.preprocess.ComputeGlobalTermStatistics.runTool','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter'
'ivory.lsh.projection.ComputeSignaturesMinhash.MyMapper.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.lsh.projection.ComputeSignaturesMinhash.MyMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,ivory.lsh.data.MinhashSignature>.collect'
'ivory.lsh.projection.ComputeSignaturesMinhash.runTool','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob'
'ivory.lsh.projection.ComputeSignaturesRandom.MyMapper.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.util.LineReader.close'
'ivory.lsh.projection.ComputeSignaturesRandom.MyMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,ivory.lsh.data.NBitSignature>.collect'
'ivory.lsh.projection.ComputeSignaturesRandom.runTool','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.webgraph.ComputeWeight.Map.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfInts,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfInts,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect'
'edu.umd.cloud9.webgraph.ComputeWeight.Reduce.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect'
'edu.umd.cloud9.webgraph.ComputeWeight.Reduce.close','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.array.ArrayListWritable<edu.umd.cloud9.webgraph.data.AnchorText>>.collect'
'edu.umd.cloud9.webgraph.ComputeWeight.runTool','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapred.SequenceFileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobClient.runJob'
'me.saac.i.ConComs.CCMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set'
'me.saac.i.ConComs.CCReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set'
'me.saac.i.ConComs.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counter.getDisplayName org.apache.hadoop.mapreduce.Counter.getValue'
'org.apache.mahout.math.stats.entropy.ConditionalEntropy.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.stats.entropy.ConditionalEntropy.prepareArguments','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.stats.entropy.ConditionalEntropy.groupAndCountByKeyAndValue','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'org.apache.mahout.math.stats.entropy.ConditionalEntropy.calculateSpecificConditionalEntropy','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.math.stats.entropy.ConditionalEntropy.calculateConditionalEntropy','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.math.stats.entropy.ConditionalEntropyTest.testConditionalEntropy','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.conf.Configuration.<init>'
'com.taobao.adfs.performance.test.ConfTest.main','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get'
'com.manning.hip.ch1.ConfigComparer.getConfigFinalProperties','org.apache.hadoop.conf.Configuration.getDeclaredField'
'com.manning.hip.ch1.ConfigComparer.loadConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource'
'com.manning.hip.ch1.ConfigComparer.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.writeXml'
'org.apache.cassandra.hadoop.ConfigHelper.setInputColumnFamily','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.setOutputColumnFamily','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.setRangeBatchSize','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.cassandra.hadoop.ConfigHelper.getRangeBatchSize','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.cassandra.hadoop.ConfigHelper.setInputSplitSize','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.cassandra.hadoop.ConfigHelper.getInputSplitSize','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.cassandra.hadoop.ConfigHelper.setInputSlicePredicate','org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.getInputSlicePredicate','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.setInputRange','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.getInputKeyRange','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getInputKeyspace','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputKeyspace','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getInputKeyspaceUserName','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getInputKeyspacePassword','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputKeyspaceUserName','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputKeyspacePassword','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getInputColumnFamily','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getInputIsWide','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputColumnFamily','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getReadConsistencyLevel','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getWriteConsistencyLevel','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getInputRpcPort','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.setInputRpcPort','org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.getInputInitialAddress','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.setInputInitialAddress','org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.setInputPartitioner','org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.getInputPartitioner','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputRpcPort','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.setOutputRpcPort','org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputInitialAddress','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.setOutputInitialAddress','org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.setOutputPartitioner','org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputPartitioner','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputCompressionClass','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.getOutputCompressionChunkLength','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.hadoop.ConfigHelper.setOutputCompressionClass','org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.ConfigHelper.setOutputCompressionChunkLength','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.util.ConfigUtils.getWithDeprecatedCheck','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.springframework.data.hadoop.configuration.ConfigurationFactoryBean.afterPropertiesSet','org.apache.hadoop.conf.Configuration.setClassLoader org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.size org.apache.hadoop.security.UserGroupInformation.setConfiguration org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>'
'org.springframework.data.hadoop.configuration.ConfigurationFactoryBean.createConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.springframework.data.hadoop.configuration.ConfigurationFactoryBean.getObjectType','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.sqoop.config.ConfigurationHelper.setJobNumMaps','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.config.ConfigurationHelper.getJobNumMaps','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.sqoop.config.ConfigurationHelper.getNumMapOutputRecords','org.apache.hadoop.mapreduce.Job.getCounters'
'org.apache.sqoop.config.ConfigurationHelper.getNumMapInputRecords','org.apache.hadoop.mapreduce.Job.getCounters'
'org.apache.sqoop.config.ConfigurationHelper.getConfNumMaps','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.sqoop.config.ConfigurationHelper.setJobMapSpeculativeExecution','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.config.ConfigurationHelper.setJobReduceSpeculativeExecution','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.config.ConfigurationHelper.setJobtrackerAddr','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.config.ConfigurationHelper.parseGenericOptions','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'org.apache.sqoop.config.ConfigurationHelper.getInstances','org.apache.hadoop.conf.Configuration.getClasses org.apache.hadoop.util.ReflectionUtils.newInstance'
'.ConfigurationPrinter.main','org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.runtime.util.hadoop.ConfigurationProviderTest.putConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml'
'com.asakusafw.runtime.util.hadoop.ConfigurationProviderTest.isLoaded','org.apache.hadoop.conf.Configuration.get'
'org.apache.ivory.entity.store.ConfigurationStoreTest.cleanup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.toConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.toProperties','org.apache.hadoop.conf.Configuration.iterator'
'org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.mergeConf','org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.getLocalFSProperties','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.getClass'
'com.bizo.hive.gdata.ConfigurationUtils.readCredentials','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'com.bizo.hive.gdata.ConfigurationUtils.missingProperties','org.apache.hadoop.mapred.JobConf.get'
'com.bizo.hive.gdata.ConfigurationUtils.spreadsheetName','org.apache.hadoop.mapred.JobConf.get'
'com.bizo.hive.gdata.ConfigurationUtils.worksheetName','org.apache.hadoop.mapred.JobConf.get'
'org.springframework.data.hadoop.configuration.ConfigurationUtils.addProperties','org.apache.hadoop.conf.Configuration.set'
'org.springframework.data.hadoop.configuration.ConfigurationUtils.createFrom','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init>'
'org.springframework.data.hadoop.configuration.ConfigurationUtils.merge','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.hbaseindex.Conjunction.next','org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.apache.sqoop.ConnFactory.instantiateFactories','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getClassByName org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.sqoop.ConnFactory.addManager','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.ConnFactory.addManagersFromFile','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.sqoop.ConnFactory.loadManagersFromConfDir','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.setClassLoader'
'org.apache.sqoop.manager.ConnManager.toJavaType','org.apache.hadoop.io.BytesWritable.getName'
'org.apache.giraph.examples.ConnectedComponentsVertex.compute','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.apache.accumulo.core.Constants.getDataVersionLocation','org.apache.hadoop.fs.Path.<init>'
'com.bah.culvert.constraints.Constraint.readFromStream','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.ObjectWritable.setConf org.apache.hadoop.io.ObjectWritable.readFields org.apache.hadoop.io.ObjectWritable.get'
'com.bah.culvert.constraints.Constraint.write','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.write'
'org.apache.nutch.protocol.Content.readFieldsCompressed','org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.VersionMismatchException.<init>'
'org.apache.nutch.protocol.Content.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.VersionMismatchException.<init>'
'org.apache.nutch.protocol.Content.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.protocol.Content.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.close'
'com.digitalpebble.behemoth.util.ContentExtractor.main','org.apache.hadoop.util.ToolRunner.run'
'com.digitalpebble.behemoth.util.ContentExtractor.generateDocs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Reader.next org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.io.SequenceFile.Reader.close'
'com.digitalpebble.behemoth.util.ContentExtractor.createArchive','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.digitalpebble.behemoth.util.ContentExtractor.addToArchive','org.apache.hadoop.fs.FSDataOutputStream.flush'
'ca.openparliament.qmiyc.ContextCounterJob.ContextCounterReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'ca.openparliament.qmiyc.ContextCounterJob.ContextCounterMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'ca.openparliament.qmiyc.ContextCounterJob.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'ca.openparliament.qmiyc.ContextCounterJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.isi.mavuno.util.ContextPatternWritableScorePair.hashCode','org.apache.hadoop.io.DoubleWritable.get'
'edu.isi.mavuno.app.distsim.ContextToPattern.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set'
'edu.isi.mavuno.app.distsim.ContextToPattern.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'fr.insarennes.fafdti.hadoop.ContinuousAttrLabelPair.ContinuousAttrLabelPair','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'fr.insarennes.fafdti.hadoop.ContinuousAttrLabelPair.getLabelIndex','org.apache.hadoop.io.IntWritable.get'
'fr.insarennes.fafdti.hadoop.ContinuousAttrLabelPair.getContinuousValue','org.apache.hadoop.io.DoubleWritable.get'
'fr.insarennes.fafdti.hadoop.ContinuousAttrLabelPair.readFields','org.apache.hadoop.io.DoubleWritable.readFields org.apache.hadoop.io.IntWritable.readFields'
'fr.insarennes.fafdti.hadoop.ContinuousAttrLabelPair.write','org.apache.hadoop.io.DoubleWritable.write org.apache.hadoop.io.IntWritable.write'
'org.apache.accumulo.server.test.continuous.ContinuousBatchWalker.addRow','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'org.apache.accumulo.server.test.continuous.ContinuousBatchWalker.getBatch','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.continuous.ContinuousIngest.initVisibilities','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.accumulo.server.test.continuous.ContinuousIngest.genMutation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.continuous.ContinuousMoru.CMapper.map','org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.accumulo.server.test.continuous.ContinuousMoru.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'org.apache.accumulo.server.test.continuous.ContinuousMoru.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.test.continuous.ContinuousQuery.main','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.examples.simple.shard.ContinuousQuery.findRandomTerms','org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.server.test.continuous.ContinuousStatsCollector.StatsCollectionTask.getTabletStats','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.continuous.ContinuousStatsCollector.StatsCollectionTask.getFSStats','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getContentSummary org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getContentSummary org.apache.hadoop.fs.ContentSummary.getLength org.apache.hadoop.fs.ContentSummary.getDirectoryCount org.apache.hadoop.fs.ContentSummary.getFileCount org.apache.hadoop.fs.ContentSummary.getLength org.apache.hadoop.fs.ContentSummary.getDirectoryCount org.apache.hadoop.fs.ContentSummary.getFileCount'
'org.apache.accumulo.server.test.continuous.ContinuousStatsCollector.getMRStats','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getMapTasks org.apache.hadoop.mapred.ClusterStatus.getMaxMapTasks org.apache.hadoop.mapred.ClusterStatus.getReduceTasks org.apache.hadoop.mapred.ClusterStatus.getMaxReduceTasks org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.ClusterStatus.getBlacklistedTrackers'
'util.Convert.toString','org.apache.hadoop.io.IntWritable.toString'
'util.Convert.toInt','org.apache.hadoop.io.IntWritable.get'
'edu.ucla.sspace.hadoop.CooccurrenceReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.apache.oozie.command.coord.CoordActionInputCheckCommand.pathExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.command.coord.CoordActionInputCheckCommand.pathExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.command.coord.CoordActionInputCheckXCommand.pathExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.command.coord.CoordActionInputCheckXCommand.pathExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.command.coord.CoordActionNotification.call','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordActionNotificationXCommand.execute','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordActionNotificationXCommand.execute','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordActionNotification.call','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordActionStartXCommand.mergeConfig','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.CoordActionStartXCommand.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordActionStartXCommand.mergeConfig','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.CoordActionStartXCommand.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.coord.CoordELFunctions.isPathAvailable','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.service.CoordMaterializeTriggerService.init','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.CoordMaterializeTriggerService.init','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.command.coord.CoordRerunCommand.cleanupOutputEvents','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.command.coord.CoordRerunCommand.cleanupOutputEvents','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.command.coord.CoordSubmitXCommand.readAndValidateXml','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordSubmitXCommand.mergeDefaultConfig','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordSubmitXCommand.readDefinition','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.command.coord.CoordSubmitXCommand.storeToDB','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordSubmitXCommand.readAndValidateXml','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordSubmitXCommand.mergeDefaultConfig','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.coord.CoordSubmitXCommand.readDefinition','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.command.coord.CoordSubmitXCommand.storeToDB','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.coord.CoordUtils.getHadoopConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.coord.CoordUtils.getHadoopConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'nl.vu.datalayer.hbase.coprocessor.CoprocessorBulkLoad.createResourceToTripleJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath'
'nl.vu.datalayer.hbase.coprocessor.CoprocessorBulkLoad.runResourceToTripleJob','org.apache.hadoop.mapreduce.Job.waitForCompletion'
'nl.vu.datalayer.hbase.coprocessor.CoprocessorBulkLoad.runCoprocessors','org.apache.hadoop.hbase.client.HTable.setOperationTimeout org.apache.hadoop.hbase.client.coprocessor.Batch.forMethod org.apache.hadoop.hbase.client.HTable.coprocessorExec'
'nl.vu.datalayer.hbase.coprocessor.CoprocessorBulkLoad.main','org.apache.hadoop.fs.Path.<init>'
'com.memonews.hbase.CopyColumnFamily.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'com.memonews.hbase.hadoop.CopyColumnFamilyData.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.setScannerCaching org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.memonews.hbase.hadoop.CopyColumnFamilyData.ResultToPutIdentityReducer.setup','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.memonews.hbase.hadoop.CopyColumnFamilyData.ResultToPutIdentityReducer.reduce','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.io.NullWritable.get'
'org.oclc.firefly.hadoop.backup.CopyRegionMapper.setup','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'org.oclc.firefly.hadoop.backup.CopyRegionMapper.cleanup','org.apache.hadoop.fs.FileSystem.close'
'org.oclc.firefly.hadoop.backup.CopyRegionMapper.map','org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.mkdirs'
'org.oclc.firefly.hadoop.backup.CopyRegionMapper.failRegion','org.apache.hadoop.fs.FileSystem.delete'
'com.memonews.hbase.CopyRow.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'com.memonews.hbase.CopyTable.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'org.apache.accumulo.server.test.randomwalk.multitable.CopyTable.visit','org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.flume.collector.CopyToHdfs.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileUtil.copy'
'lsh.hadoop.CornerDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'lsh.hadoop.CornerMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'lsh.hadoop.CornerMapper.map','org.apache.hadoop.io.Text.toString'
'lsh.hadoop.CornerMapper.doOneCorner','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'lsh.hadoop.CornerMapper.doAllCorners','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'babel.prep.corpus.CorpusGenMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,babel.content.pages.Page>.collect'
'babel.prep.corpus.CorpusGenerator.createJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'babel.prep.corpus.CorpusGenerator.main','org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.FileOutputFormat.getOutputPath'
'gov.llnl.ontology.mapreduce.CorpusTableMR.setupReducer','org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'gov.llnl.ontology.mapreduce.CorpusTableMR.addToDistrubutedCache','org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'gov.llnl.ontology.mapreduce.CorpusTableMR.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.mapreduce.Job.waitForCompletion'
'gov.llnl.ontology.mapreduce.CorpusTableMR.CorpusTableMapper.setup','org.apache.hadoop.mapreduce.Mapper.Context.setStatus org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Mapper.Context.setStatus'
'gov.llnl.ontology.mapreduce.CorpusTableMR.CorpusTableMapper.loadWordList','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'edu.umd.hooka.CorpusVocabNormalizerAndNumberizer.BitextCompilerMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getBoolean'
'edu.umd.hooka.CorpusVocabNormalizerAndNumberizer.BitextCompilerMapper.close','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'edu.umd.hooka.CorpusVocabNormalizerAndNumberizer.BitextCompilerMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,edu.umd.hooka.PhrasePair>.collect'
'edu.umd.hooka.CorpusVocabNormalizerAndNumberizer.XMLInput.configure','org.apache.hadoop.io.compress.CompressionCodecFactory.<init>'
'edu.umd.hooka.CorpusVocabNormalizerAndNumberizer.XMLInput.isSplitable','org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'edu.umd.hooka.CorpusVocabNormalizerAndNumberizer.XMLInput.getRecordReader','org.apache.hadoop.mapred.InputSplit.toString org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.streaming.StreamXmlRecordReader.<init>'
'edu.umd.hooka.CorpusVocabNormalizerAndNumberizer.preprocessAndNumberizeFiles','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setJar org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.hooka.CorpusVocabNormalizerAndNumberizer.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init>'
'de.jungblut.recommendation.cosine.CosineMapReduceJob.CosineMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'de.jungblut.recommendation.cosine.CosineMapReduceJob.CosineMapper.map','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'de.jungblut.recommendation.cosine.CosineMapReduceJob.CosineMapper.reopen','org.apache.hadoop.fs.FileSystem.get'
'de.jungblut.recommendation.cosine.CosineMapReduceJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'de.jungblut.recommendation.cosine.CosineMapReduceJob.generateInput','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.IntWritable.<init>'
'org.lwes.journaller.CountDeJournaller.processSequenceFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.BytesWritable.getBytes'
'com.hbasebook.hush.Counters.incrementUsage','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Increment.<init> org.apache.hadoop.hbase.client.Increment.addColumn org.apache.hadoop.hbase.client.HTable.increment'
'com.hbasebook.hush.Counters.addIncrement','org.apache.hadoop.hbase.client.Increment.addColumn org.apache.hadoop.hbase.client.Increment.addColumn org.apache.hadoop.hbase.client.Increment.addColumn'
'com.hbasebook.hush.Counters.getQualifier','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'com.hbasebook.hush.Counters.getDailyStatistics','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong'
'org.apache.hama.bsp.Counters.Counter.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.WritableUtils.readVLong'
'org.apache.hama.bsp.Counters.Counter.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.WritableUtils.writeVLong'
'org.apache.hama.bsp.Counters.Group.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.WritableUtils.writeVInt'
'org.apache.hama.bsp.Counters.Group.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.hama.bsp.Counters.write','org.apache.hadoop.io.Text.writeString'
'org.apache.hama.bsp.Counters.readFields','org.apache.hadoop.io.Text.readString'
'org.apache.hama.bsp.Counters.getBlock','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.util.StringUtils.findNext org.apache.hadoop.util.StringUtils.findNext org.apache.hadoop.io.IntWritable.set'
'org.apache.hama.bsp.Counters.fromEscapedCompactString','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.hama.bsp.Counters.escape','org.apache.hadoop.util.StringUtils.escapeString'
'org.apache.hama.bsp.Counters.unescape','org.apache.hadoop.util.StringUtils.unEscapeString'
'hipi.examples.covariance.Covariance.MeanMap.map','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'hipi.examples.covariance.Covariance.MeanReduce.reduce','org.apache.hadoop.mapreduce.Mapper.Context.write'
'hipi.examples.covariance.Covariance.CovarianceMap.setup','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FSDataInputStream.skip'
'hipi.examples.covariance.Covariance.CovarianceMap.map','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'hipi.examples.covariance.Covariance.CovarianceReduce.reduce','org.apache.hadoop.mapreduce.Mapper.Context.write'
'hipi.examples.covariance.Covariance.rmdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'hipi.examples.covariance.Covariance.mkdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'hipi.examples.covariance.Covariance.runMeanCompute','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'hipi.examples.covariance.Covariance.runCovariance','org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'hipi.examples.covariance.Covariance.main','org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.CrawlDBStatsCollectorStep.runStep','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.crawl.CrawlDBTestUtil.createCrawlDb','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.crawl.CrawlDBTestUtil.createConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource'
'org.apache.nutch.crawl.CrawlDBTestUtil.generateSeedList','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'org.commoncrawl.util.CrawlDatum.CrawlDatum','org.apache.hadoop.io.MapWritable.<init>'
'org.commoncrawl.util.CrawlDatum.getMetaData','org.apache.hadoop.io.MapWritable.<init>'
'org.commoncrawl.util.CrawlDatum.readFields','org.apache.hadoop.io.VersionMismatchException.<init> org.apache.hadoop.io.MapWritable.clear org.apache.hadoop.io.MapWritable.readFields'
'org.commoncrawl.util.CrawlDatum.write','org.apache.hadoop.io.MapWritable.size org.apache.hadoop.io.MapWritable.write'
'org.commoncrawl.util.CrawlDatum.set','org.apache.hadoop.io.MapWritable.<init>'
'org.commoncrawl.util.CrawlDatum.toString','org.apache.hadoop.io.MapWritable.toString'
'org.commoncrawl.util.CrawlDatum.equals','org.apache.hadoop.io.MapWritable.size org.apache.hadoop.io.MapWritable.equals'
'org.commoncrawl.util.CrawlDatum.hashCode','org.apache.hadoop.io.MapWritable.hashCode'
'org.apache.nutch.crawl.CrawlDatum.readFields','org.apache.hadoop.io.VersionMismatchException.<init>'
'org.apache.nutch.crawl.CrawlDbFilter.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.get'
'org.apache.nutch.crawl.CrawlDbFilter.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.collect'
'org.apache.nutch.crawl.CrawlDbMerger.Merger.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.CrawlDatum>.collect'
'org.apache.nutch.crawl.CrawlDbMerger.merge','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.apache.nutch.crawl.CrawlDbMerger.createMergeJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.apache.nutch.crawl.CrawlDbMerger.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.crawl.CrawlDbMerger.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.crawl.CrawlDbReader.openReaders','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getReaders'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDatumCsvOutputFormat.LineRecordWriter.write','org.apache.hadoop.io.Text.toString'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDatumCsvOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbStatMapper.configure','org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbStatMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbStatCombiner.reduce','org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbStatReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbTopNMapper.configure','org.apache.hadoop.mapred.JobConf.getLong'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbTopNMapper.map','org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.FloatWritable,org.apache.hadoop.io.Text>.collect'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbTopNReducer.reduce','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.FloatWritable,org.apache.hadoop.io.Text>.collect'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbTopNReducer.configure','org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getNumReduceTasks'
'org.apache.nutch.crawl.CrawlDbReader.processStatJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.CrawlDbReader.get','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.lib.HashPartitioner<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getEntry'
'org.apache.nutch.crawl.CrawlDbReader.processDumpJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbDumpMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.apache.nutch.crawl.CrawlDbReader.CrawlDbDumpMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.collect'
'org.apache.nutch.crawl.CrawlDbReader.processTopNJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.CrawlDbReducer.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.nutch.crawl.CrawlDbReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.CrawlDatum>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.CrawlDatum>.collect'
'org.apache.nutch.crawl.CrawlDb.update','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.CrawlDb.createJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setBoolean'
'org.apache.nutch.crawl.CrawlDb.install','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getFs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.crawl.CrawlDb.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.crawl.CrawlDb.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.crawl.common.internal.CrawlEnvironment.getDefaultFileSystem','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'org.commoncrawl.service.listcrawler.CrawlList.CrawlList','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.service.listcrawler.CrawlList.compareRaw','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong'
'org.commoncrawl.service.listcrawler.CrawlList.updateItemState','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData'
'org.commoncrawl.service.listcrawler.CrawlList.queueUnCrawledItems','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.service.listcrawler.CrawlList.requeueFailedItems','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.service.listcrawler.CrawlList.calculateStringCRC','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.service.listcrawler.CrawlList.writeInitialOnDiskItem','org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.service.listcrawler.CrawlList.loadOnDiskItemForURLFP','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'org.commoncrawl.service.listcrawler.CrawlList.getHistoryItemFromOnDiskItem','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.service.listcrawler.CrawlList.testmain','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.commoncrawl.service.listcrawler.CrawlList.getOffsetForSubDomainData','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.DataInputBuffer.readInt'
'org.commoncrawl.service.listcrawler.CrawlList.writeSubDomainMetadataToDisk','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.service.listcrawler.CrawlList.writeInitialSubDomainMetadataToDisk','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeInt'
'org.commoncrawl.service.listcrawler.CrawlList.resetSubDomainCounts','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData'
'org.commoncrawl.service.listcrawler.CrawlList.loadSubDomainMetadataFromDisk','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.service.listcrawler.CrawlList.getSubDomainItemCount','org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.service.listcrawler.CrawlList.getSubDomainList','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'org.commoncrawl.service.listcrawler.CrawlList.dumpUnCrawledItems','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.service.crawler.CrawlList.call','org.apache.hadoop.record.Buffer.get'
'org.commoncrawl.util.CrawlLogSplitter.compare','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'org.commoncrawl.util.CrawlLogSplitter.buildIncrementalPathGivenPathAndIndex','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.util.CrawlLogSplitter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.compress.SnappyCodec.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.compress.SnappyCodec.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.commoncrawl.mapred.pipelineV3.CrawlPipelineTask.addTaskDependency','org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.mapred.pipelineV3.CrawlPipelineTask.finalStepComplete','org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.commoncrawl.mapred.pipelineV3.CrawlPipelineTask.getLatestDatabaseTimestamp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.pipelineV3.CrawlPipelineTask.getOutputDirForStep','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.pipelineV3.CrawlPipelineTask.getTaskIdentityPath','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.pipelineV3.CrawlPipelineTask.getTempDirForStep','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.pipelineV3.CrawlPipelineTask.runStep','org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.mapred.pipelineV3.CrawlPipelineTask.runTask','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.preAnalyze','org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext.getHive org.apache.hadoop.hive.ql.parse.SemanticException.<init> org.apache.hadoop.hive.ql.parse.ASTNode.getChildCount org.apache.hadoop.hive.ql.parse.ASTNode.getChild org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getUnescapedName org.apache.hadoop.hive.ql.parse.ASTNode.getChild org.apache.hadoop.hive.ql.parse.ASTNode.getToken org.apache.hadoop.hive.ql.metadata.Hive.getDatabasesByPattern org.apache.hadoop.hive.ql.parse.SemanticException.<init>'
'org.apache.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.postAnalyze','org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext.getConf'
'org.apache.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.authorizeDDLWork','org.apache.hadoop.hive.ql.plan.DDLWork.getCreateDatabaseDesc org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getName org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getComment org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getLocationUri org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getDatabaseProperties org.apache.hadoop.hive.metastore.api.Database.<init>'
'org.apache.sqoop.tool.CreateHiveTableTool.run','org.apache.hadoop.util.StringUtils.stringifyException'
'mia.clustering.ch12.CreateLastfmDataset.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.accumulo.server.test.CreateRandomRFile.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init>'
'tv.floe.caduceus.hadoop.compression.lzo.create_gzip_seq_file.CreateSeqFileJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setCompressMapOutput org.apache.hadoop.mapred.TextOutputFormat.setOutputCompressorClass org.apache.hadoop.mapred.TextOutputFormat.setCompressOutput org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'tv.floe.caduceus.hadoop.compression.lzo.create_gzip_seq_file.CreateSeqFileJob.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'tv.floe.caduceus.hadoop.compression.lzo.create_gzip_seq_file.CreateSeqFileJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.master.tableOps.PopulateMetadata.call','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.tableOps.CreateDir.undo','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'admin.CreateTableExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.isTableAvailable'
'admin.CreateTableWithRegionsExample.printTableRegions','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getStartEndKeys org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toStringBinary'
'admin.CreateTableWithRegionsExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.setName org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.CreateTimeHandler.get','org.apache.hadoop.fs.FileStatus.getModificationTime'
'org.apache.oozie.action.hadoop.CredentialForTest.addtoJobConf','org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.action.hadoop.CredentialsProvider.createCredentialObject','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.oozie.action.hadoop.CredentialsProvider.createCredentialObject','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.datasalt.pangool.tuplemr.Criteria.SortElement.equals','org.apache.hadoop.io.RawComparator<?>.equals'
'org.commoncrawl.mapred.pipelineV3.domainmeta.fuzzydedupe.CrossDomainDupes.runStep','org.apache.hadoop.mapred.JobClient.runJob'
'org.sleuthkit.hadoop.scoring.CrossImageJSONOutputBuilder.buildReport','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'org.sleuthkit.hadoop.scoring.CrossImageJSONOutputBuilder.writeFileToStream','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataInputStream.read'
'org.sleuthkit.hadoop.scoring.CrossImageScoreReducer.reduce','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.<init>'
'org.sleuthkit.hadoop.scoring.CrossImageScorerJob.runPipeline','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.RegexStringComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.sleuthkit.hadoop.scoring.CrossImageScorerJob.convertScanToString','org.apache.hadoop.hbase.client.Scan.write org.apache.hadoop.hbase.util.Base64.encodeBytes'
'org.apache.crunch.impl.mr.exec.CrunchJob.handleMultiPaths','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename'
'org.apache.crunch.impl.mr.exec.CrunchJob.getDestFile','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.impl.mr.exec.CrunchJob.getMinPartIndex','org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.crunch.impl.mr.exec.CrunchJob.checkRunningState','org.apache.hadoop.util.StringUtils.stringifyException'
'com.datasalt.pangool.benchmark.secondarysort.CrunchSecondarySort.run','org.apache.hadoop.util.GenericOptionsParser.printGenericCommandUsage org.apache.hadoop.conf.Configuration.set'
'com.datasalt.pangool.benchmark.secondarysort.CrunchSecondarySort.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.manning.hip.ch12.crunch.CrunchUtils.setContext','org.apache.hadoop.mapreduce.MapContext.getInputSplit org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'com.datasalt.pangool.benchmark.wordcount.CrunchWordCount.run','org.apache.hadoop.util.GenericOptionsParser.printGenericCommandUsage'
'com.datasalt.pangool.benchmark.wordcount.CrunchWordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.simple','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.types','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.attributes','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.header','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.implicit_field_name','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.file_name','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.line_number','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.record_number','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.ignore','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.fragmentation_attempt','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.directio.csv.driver.CsvFormatEmitterTest.fragmentation_restricted','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.simple','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.types','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.attributes','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.header','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.implicit_field_name','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.file_name','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.line_number','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.record_number','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.csv.driver.CsvSupportEmitterTest.ignore','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.bah.culvert.accumulo.CulvertAccumuloIT.beforeClass','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.bah.culvert.CulvertHBaseIT.create','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getMiniHBaseCluster'
'com.bah.culvert.CulvertHBaseIT.shutdown','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'com.bah.culvert.CulvertHiveIT.prepareEnvironment','org.apache.hadoop.conf.Configuration.addDefaultResource org.apache.hadoop.hive.ql.QTestUtil.<init>'
'com.bah.culvert.CulvertHiveIT.testHiveScript','org.apache.hadoop.hive.ql.QTestUtil.addFile org.apache.hadoop.hive.ql.QTestUtil.clearTestSideEffects org.apache.hadoop.hive.ql.QTestUtil.cliInit org.apache.hadoop.hive.ql.QTestUtil.executeClient org.apache.hadoop.hive.ql.QTestUtil.checkCliDriverResults'
'com.bah.culvert.CulvertHiveIT.cleanup','org.apache.hadoop.hive.ql.QTestUtil.cleanUp'
'com.bah.culvert.hive.CulvertInputFormat.getSplits','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.hive.ql.exec.Utilities.deserializeExpression org.apache.hadoop.hive.ql.index.IndexSearchCondition.getColumnDesc org.apache.hadoop.hive.ql.index.IndexSearchCondition.getComparisonOp org.apache.hadoop.hive.ql.index.IndexSearchCondition.getConstantDesc org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.<init> org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.initialize org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.evaluate org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.getName org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.getName org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.getName org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.getName org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.getName org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.getName org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.getName org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.getName'
'com.bah.culvert.hive.CulvertMetaHook.preCreateTable','org.apache.hadoop.hive.metastore.api.Table.getParameters org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hive.metastore.MetaStoreUtils.isExternalTable org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hive.metastore.api.MetaException.<init>'
'com.bah.culvert.hive.CulvertRecordReader.createKey','org.apache.hadoop.io.NullWritable.get'
'com.bah.culvert.hive.CulvertSerDe.initialize','org.apache.hadoop.conf.Configuration.<init>'
'com.bah.culvert.hive.CulvertSerDe.serialize','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.manning.hip.ch3.binary.CustomBinaryFileRead.write','org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.binary.CustomBinaryFileRead.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch3.binary.CustomBinaryInputFormat.getFileOffest','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.binary.CustomBinaryInputFormat.getSplits','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.equals org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getLocations org.apache.hadoop.mapreduce.lib.input.FileSplit.<init>'
'com.manning.hip.ch3.binary.CustomBinaryRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek'
'com.manning.hip.ch3.binary.CustomBinaryRecordReader.nextKeyValue','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.set'
'com.cloudera.flume.handlers.hdfs.CustomDfsSink.open','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.compress.CompressionCodec.createCompressor org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'com.cloudera.flume.handlers.hdfs.CustomDfsSink.getCodec','org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses org.apache.hadoop.conf.Configurable.setConf'
'com.cloudera.flume.handlers.hdfs.CustomDfsSink.close','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'com.cloudera.flume.handlers.hdfs.CustomDfsSink.deleteEmptyFile','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.delete'
'com.cloudera.flume.handlers.hdfs.CustomDfsSink.writeHiveMarker','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'com.cloudera.flume.handlers.hdfs.CustomDfsSink.open','org.apache.hadoop.io.compress.GzipCodec.<init> org.apache.hadoop.io.compress.CompressionCodec.createCompressor org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.compress.CompressionCodec.createCompressor org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream org.apache.hadoop.fs.Path.toString'
'com.cloudera.flume.handlers.hdfs.CustomDfsSink.open','org.apache.hadoop.io.compress.GzipCodec.<init> org.apache.hadoop.io.compress.CompressionCodec.createCompressor org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.compress.CompressionCodec.createCompressor org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream org.apache.hadoop.fs.Path.toString'
'org.apache.nutch.searcher.custom.CustomFieldQueryFilter.setConf','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.indexer.field.CustomFields.runConverter','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.indexer.field.CustomFields.runCollector','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.indexer.field.CustomFields.Converter.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getConfResourceAsInputStream org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.indexer.field.CustomFields.Converter.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.field.FieldWritable>.collect'
'org.apache.nutch.indexer.field.CustomFields.Converter.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.field.FieldWritable>.collect org.apache.hadoop.io.Text.toString'
'org.apache.nutch.indexer.field.CustomFields.Collector.map','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.ObjectWritable>.collect'
'org.apache.nutch.indexer.field.CustomFields.Collector.reduce','org.apache.hadoop.io.ObjectWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.field.FieldWritable>.collect'
'org.apache.nutch.indexer.field.CustomFields.createFields','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.indexer.field.CustomFields.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.indexer.field.CustomFields.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'filters.CustomFilter.filterKeyValue','org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.compareTo'
'filters.CustomFilter.write','org.apache.hadoop.hbase.util.Bytes.writeByteArray'
'filters.CustomFilter.readFields','org.apache.hadoop.hbase.util.Bytes.readByteArray'
'filters.CustomFilterExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'thiswillbereplaced.CustomGenericUDF.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.getOrdinal org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category.toString org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter'
'thiswillbereplaced.CustomGenericUDF.evaluate','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.get org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.findText org.apache.hadoop.io.IntWritable.set'
'CustomKey.CustomIntPairReducer.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapreduce.Reducer.Context.write'
'com.nearinfinity.hbase.dsl.CustomQueryOperatorTest.setUpBeforeClass','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster'
'com.nearinfinity.hbase.dsl.CustomQueryOperatorTest.tearDownAfterClass','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'com.nearinfinity.hbase.dsl.CustomQueryOperatorTest.myFuncMethod','org.apache.hadoop.hbase.client.HTable.<init>'
'thiswillbereplaced.CustomUDF.evaluate','org.apache.hadoop.io.Text.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.set'
'com.jaccson.DBCollection.indexBSON','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.jaccson.DBCollection.createIndex','org.apache.hadoop.util.ToolRunner.run'
'com.twitter.maple.jdbc.db.DBConfiguration.configureDB','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.twitter.maple.jdbc.db.DBConfiguration.getConnection','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.twitter.maple.jdbc.db.DBConfiguration.getInputTableName','org.apache.hadoop.conf.Configuration.get'
'com.twitter.maple.jdbc.db.DBConfiguration.setInputTableName','org.apache.hadoop.conf.Configuration.set'
'com.twitter.maple.jdbc.db.DBConfiguration.getInputFieldNames','org.apache.hadoop.conf.Configuration.getStrings'
'com.twitter.maple.jdbc.db.DBConfiguration.setInputFieldNames','org.apache.hadoop.conf.Configuration.setStrings'
'com.twitter.maple.jdbc.db.DBConfiguration.getInputConditions','org.apache.hadoop.conf.Configuration.get'
'com.twitter.maple.jdbc.db.DBConfiguration.setInputConditions','org.apache.hadoop.conf.Configuration.set'
'com.twitter.maple.jdbc.db.DBConfiguration.getInputOrderBy','org.apache.hadoop.conf.Configuration.get'
'com.twitter.maple.jdbc.db.DBConfiguration.setInputOrderBy','org.apache.hadoop.conf.Configuration.set'
'com.twitter.maple.jdbc.db.DBConfiguration.getInputQuery','org.apache.hadoop.conf.Configuration.get'
'com.twitter.maple.jdbc.db.DBConfiguration.setInputQuery','org.apache.hadoop.conf.Configuration.set'
'com.twitter.maple.jdbc.db.DBConfiguration.getInputLimit','org.apache.hadoop.conf.Configuration.getLong'
'com.twitter.maple.jdbc.db.DBConfiguration.setInputLimit','org.apache.hadoop.conf.Configuration.setLong'
'com.twitter.maple.jdbc.db.DBConfiguration.getInputCountQuery','org.apache.hadoop.conf.Configuration.get'
'com.twitter.maple.jdbc.db.DBConfiguration.setInputCountQuery','org.apache.hadoop.conf.Configuration.set'
'com.twitter.maple.jdbc.db.DBConfiguration.getInputClass','org.apache.hadoop.conf.Configuration.getClass'
'com.twitter.maple.jdbc.db.DBConfiguration.setInputClass','org.apache.hadoop.conf.Configuration.setClass'
'com.twitter.maple.jdbc.db.DBConfiguration.getOutputTableName','org.apache.hadoop.conf.Configuration.get'
'com.twitter.maple.jdbc.db.DBConfiguration.setOutputTableName','org.apache.hadoop.conf.Configuration.set'
'com.twitter.maple.jdbc.db.DBConfiguration.getOutputFieldNames','org.apache.hadoop.conf.Configuration.getStrings'
'com.twitter.maple.jdbc.db.DBConfiguration.setOutputFieldNames','org.apache.hadoop.conf.Configuration.setStrings'
'com.twitter.maple.jdbc.db.DBConfiguration.getOutputUpdateFieldNames','org.apache.hadoop.conf.Configuration.getStrings'
'com.twitter.maple.jdbc.db.DBConfiguration.setOutputUpdateFieldNames','org.apache.hadoop.conf.Configuration.setStrings'
'com.twitter.maple.jdbc.db.DBConfiguration.getBatchStatementsNum','org.apache.hadoop.conf.Configuration.getInt'
'com.twitter.maple.jdbc.db.DBConfiguration.setBatchStatementsNum','org.apache.hadoop.conf.Configuration.setInt'
'com.twitter.maple.jdbc.db.DBConfiguration.getMaxConcurrentReadsNum','org.apache.hadoop.conf.Configuration.getInt'
'com.twitter.maple.jdbc.db.DBConfiguration.setMaxConcurrentReadsNum','org.apache.hadoop.conf.Configuration.setInt'
'cascading.dbmigrate.hadoop.DBConfiguration.getConnection','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'cascading.dbmigrate.hadoop.DBConfiguration.getInputTableName','org.apache.hadoop.mapred.JobConf.get'
'cascading.dbmigrate.hadoop.DBConfiguration.setInputTableName','org.apache.hadoop.mapred.JobConf.set'
'cascading.dbmigrate.hadoop.DBConfiguration.getInputColumnNames','org.apache.hadoop.mapred.JobConf.getStrings'
'cascading.dbmigrate.hadoop.DBConfiguration.setInputColumnNames','org.apache.hadoop.mapred.JobConf.setStrings'
'cascading.dbmigrate.hadoop.DBConfiguration.getPrimaryKeyColumn','org.apache.hadoop.mapred.JobConf.get'
'cascading.dbmigrate.hadoop.DBConfiguration.setPrimaryKeyColumn','org.apache.hadoop.mapred.JobConf.set'
'cascading.dbmigrate.hadoop.DBConfiguration.setNumChunks','org.apache.hadoop.mapred.JobConf.setInt'
'cascading.dbmigrate.hadoop.DBConfiguration.getNumChunks','org.apache.hadoop.mapred.JobConf.getInt'
'cascading.dbmigrate.hadoop.DBConfiguration.setMinId','org.apache.hadoop.mapred.JobConf.setLong'
'cascading.dbmigrate.hadoop.DBConfiguration.getMinId','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getLong'
'cascading.dbmigrate.hadoop.DBConfiguration.setMaxId','org.apache.hadoop.mapred.JobConf.setLong'
'cascading.dbmigrate.hadoop.DBConfiguration.getMaxId','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getLong'
'co.nubetech.apache.hadoop.DBConfiguration.configureDB','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DBConfiguration.getConnection','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.apache.hadoop.DBConfiguration.getInputTableName','org.apache.hadoop.conf.Configuration.get'
'co.nubetech.apache.hadoop.DBConfiguration.setInputTableName','org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DBConfiguration.getInputFieldNames','org.apache.hadoop.conf.Configuration.getStrings'
'co.nubetech.apache.hadoop.DBConfiguration.setInputFieldNames','org.apache.hadoop.conf.Configuration.setStrings'
'co.nubetech.apache.hadoop.DBConfiguration.getInputConditions','org.apache.hadoop.conf.Configuration.get'
'co.nubetech.apache.hadoop.DBConfiguration.setInputConditions','org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DBConfiguration.getInputOrderBy','org.apache.hadoop.conf.Configuration.get'
'co.nubetech.apache.hadoop.DBConfiguration.setInputOrderBy','org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DBConfiguration.getInputQuery','org.apache.hadoop.conf.Configuration.get'
'co.nubetech.apache.hadoop.DBConfiguration.setInputQuery','org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DBConfiguration.getInputCountQuery','org.apache.hadoop.conf.Configuration.get'
'co.nubetech.apache.hadoop.DBConfiguration.setInputCountQuery','org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DBConfiguration.setInputBoundingQuery','org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DBConfiguration.getInputBoundingQuery','org.apache.hadoop.conf.Configuration.get'
'co.nubetech.apache.hadoop.DBConfiguration.getInputClass','org.apache.hadoop.conf.Configuration.getClass'
'co.nubetech.apache.hadoop.DBConfiguration.setInputClass','org.apache.hadoop.conf.Configuration.setClass'
'co.nubetech.apache.hadoop.DBConfiguration.getOutputTableName','org.apache.hadoop.conf.Configuration.get'
'co.nubetech.apache.hadoop.DBConfiguration.setOutputTableName','org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DBConfiguration.getOutputFieldNames','org.apache.hadoop.conf.Configuration.getStrings'
'co.nubetech.apache.hadoop.DBConfiguration.setOutputFieldNames','org.apache.hadoop.conf.Configuration.setStrings'
'co.nubetech.apache.hadoop.DBConfiguration.setOutputFieldCount','org.apache.hadoop.conf.Configuration.setInt'
'co.nubetech.apache.hadoop.DBConfiguration.getOutputFieldCount','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.sqoop.mapreduce.db.DBConfiguration.configureDB','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getConnection','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getFetchSize','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setFetchSize','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getInputTableName','org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setInputTableName','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getInputFieldNames','org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setInputFieldNames','org.apache.hadoop.conf.Configuration.setStrings'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getInputConditions','org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setInputConditions','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getInputOrderBy','org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setInputOrderBy','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getInputQuery','org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setInputQuery','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getInputCountQuery','org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setInputCountQuery','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setInputBoundingQuery','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getInputBoundingQuery','org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getInputClass','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setInputClass','org.apache.hadoop.conf.Configuration.setClass'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getOutputTableName','org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setOutputTableName','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getOutputFieldNames','org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setOutputFieldNames','org.apache.hadoop.conf.Configuration.setStrings'
'org.apache.sqoop.mapreduce.db.DBConfiguration.setOutputFieldCount','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.sqoop.mapreduce.db.DBConfiguration.getOutputFieldCount','org.apache.hadoop.conf.Configuration.getInt'
'cascading.jdbc.db.DBConfiguration.configureDB','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'cascading.jdbc.db.DBConfiguration.getConnection','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'cascading.jdbc.db.DBConfiguration.getInputTableName','org.apache.hadoop.mapred.JobConf.get'
'cascading.jdbc.db.DBConfiguration.setInputTableName','org.apache.hadoop.mapred.JobConf.set'
'cascading.jdbc.db.DBConfiguration.getInputFieldNames','org.apache.hadoop.mapred.JobConf.getStrings'
'cascading.jdbc.db.DBConfiguration.setInputFieldNames','org.apache.hadoop.mapred.JobConf.setStrings'
'cascading.jdbc.db.DBConfiguration.getInputConditions','org.apache.hadoop.mapred.JobConf.get'
'cascading.jdbc.db.DBConfiguration.setInputConditions','org.apache.hadoop.mapred.JobConf.set'
'cascading.jdbc.db.DBConfiguration.getInputOrderBy','org.apache.hadoop.mapred.JobConf.get'
'cascading.jdbc.db.DBConfiguration.setInputOrderBy','org.apache.hadoop.mapred.JobConf.set'
'cascading.jdbc.db.DBConfiguration.getInputQuery','org.apache.hadoop.mapred.JobConf.get'
'cascading.jdbc.db.DBConfiguration.setInputQuery','org.apache.hadoop.mapred.JobConf.set'
'cascading.jdbc.db.DBConfiguration.getInputLimit','org.apache.hadoop.mapred.JobConf.getLong'
'cascading.jdbc.db.DBConfiguration.setInputLimit','org.apache.hadoop.mapred.JobConf.setLong'
'cascading.jdbc.db.DBConfiguration.getInputCountQuery','org.apache.hadoop.mapred.JobConf.get'
'cascading.jdbc.db.DBConfiguration.setInputCountQuery','org.apache.hadoop.mapred.JobConf.set'
'cascading.jdbc.db.DBConfiguration.getInputClass','org.apache.hadoop.mapred.JobConf.getClass'
'cascading.jdbc.db.DBConfiguration.setInputClass','org.apache.hadoop.mapred.JobConf.setClass'
'cascading.jdbc.db.DBConfiguration.getOutputTableName','org.apache.hadoop.mapred.JobConf.get'
'cascading.jdbc.db.DBConfiguration.setOutputTableName','org.apache.hadoop.mapred.JobConf.set'
'cascading.jdbc.db.DBConfiguration.getOutputFieldNames','org.apache.hadoop.mapred.JobConf.getStrings'
'cascading.jdbc.db.DBConfiguration.setOutputFieldNames','org.apache.hadoop.mapred.JobConf.setStrings'
'cascading.jdbc.db.DBConfiguration.getOutputUpdateFieldNames','org.apache.hadoop.mapred.JobConf.getStrings'
'cascading.jdbc.db.DBConfiguration.setOutputUpdateFieldNames','org.apache.hadoop.mapred.JobConf.setStrings'
'cascading.jdbc.db.DBConfiguration.getBatchStatementsNum','org.apache.hadoop.mapred.JobConf.getInt'
'cascading.jdbc.db.DBConfiguration.setBatchStatementsNum','org.apache.hadoop.mapred.JobConf.setInt'
'cascading.jdbc.db.DBConfiguration.getMaxConcurrentReadsNum','org.apache.hadoop.mapred.JobConf.getInt'
'cascading.jdbc.db.DBConfiguration.setMaxConcurrentReadsNum','org.apache.hadoop.mapred.JobConf.setInt'
'co.nubetech.hiho.mapreduce.DBInputAvroMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.avro.mapred.AvroValue<org.apache.avro.mapred.Pair>,org.apache.hadoop.io.NullWritable>.collect'
'co.nubetech.hiho.mapreduce.DBInputAvroMapper.getKeyRecord','org.apache.hadoop.io.LongWritable.get'
'com.twitter.maple.jdbc.db.DBInputFormat.DBRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'com.twitter.maple.jdbc.db.DBInputFormat.DBRecordReader.createValue','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.twitter.maple.jdbc.db.DBInputFormat.DBRecordReader.next','org.apache.hadoop.io.LongWritable.set'
'com.twitter.maple.jdbc.db.DBInputFormat.configure','org.apache.hadoop.mapred.lib.db.DBConfiguration.<init> org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputTableName org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputFieldNames org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputConditions org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputLimit org.apache.hadoop.mapred.lib.db.DBConfiguration.getMaxConcurrentReadsNum org.apache.hadoop.mapred.lib.db.DBConfiguration.getConnection'
'com.twitter.maple.jdbc.db.DBInputFormat.getRecordReader','org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputClass'
'com.twitter.maple.jdbc.db.DBInputFormat.getCountQuery','org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputCountQuery org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputCountQuery'
'com.twitter.maple.jdbc.db.DBInputFormat.setInput','org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.lib.db.DBConfiguration.<init> org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputClass org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputTableName org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputFieldNames org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputConditions org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputOrderBy org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputLimit org.apache.hadoop.mapred.lib.db.DBConfiguration.setMaxConcurrentReadsNum org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.lib.db.DBConfiguration.<init> org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputClass org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputQuery org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputCountQuery org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputLimit org.apache.hadoop.mapred.lib.db.DBConfiguration.setMaxConcurrentReadsNum'
'co.nubetech.apache.hadoop.DBInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'co.nubetech.apache.hadoop.DBInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'co.nubetech.apache.hadoop.DBInputFormat.setInput','org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.mapreduce.db.DBInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.sqoop.mapreduce.db.DBInputFormat.setInput','org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration'
'cascading.dbmigrate.tap.DBMigrateTap.DBMigrateScheme.sourceInit','org.apache.hadoop.mapred.FileInputFormat.setInputPaths'
'cascading.dbmigrate.tap.DBMigrateTap.getPath','org.apache.hadoop.fs.Path.<init>'
'co.nubetech.apache.hadoop.DBOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>'
'co.nubetech.apache.hadoop.DBOutputFormat.DBRecordWriter.close','org.apache.hadoop.util.StringUtils.stringifyException'
'co.nubetech.apache.hadoop.DBOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'co.nubetech.apache.hadoop.DBOutputFormat.setOutput','org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.mapreduce.db.DBOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>'
'org.apache.sqoop.mapreduce.db.DBOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.sqoop.mapreduce.db.DBOutputFormat.setOutput','org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.mapreduce.db.DBOutputFormat.DBRecordWriter.close','org.apache.hadoop.util.StringUtils.stringifyException'
'cascading.jdbc.db.DBOutputFormat.DBRecordWriter.rollBack','org.apache.hadoop.util.StringUtils.stringifyException'
'cascading.jdbc.db.DBOutputFormat.setOutput','org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setReduceSpeculativeExecution org.apache.hadoop.mapred.JobConf.setMapSpeculativeExecution'
'co.nubetech.hiho.mapreduce.lib.db.DBQueryInputFormat.createDBRecordReader','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.DefaultStringifier<java.util.ArrayList>.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.DefaultStringifier<java.util.ArrayList>.fromString'
'co.nubetech.hiho.mapreduce.lib.db.DBQueryInputFormat.setInput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.DefaultStringifier<java.util.ArrayList>.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.DefaultStringifier<java.util.ArrayList>.toString org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.DefaultStringifier<java.util.ArrayList>.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.DefaultStringifier<java.util.ArrayList>.toString org.apache.hadoop.mapreduce.Job.setInputFormatClass'
'org.apache.pig.piggybank.storage.DBStorage.MyDBOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.OutputCommitter.<init>'
'org.apache.pig.piggybank.storage.DBStorage.MyDBOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable>.<init>'
'org.lilyproject.repository.impl.DFSBlobStoreAccess.DFSBlobStoreAccess','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'org.lilyproject.repository.impl.DFSBlobStoreAccess.getOutputStream','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.lilyproject.repository.impl.DFSBlobStoreAccess.getInputStream','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.lilyproject.repository.impl.DFSBlobStoreAccess.delete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.lilyproject.repository.impl.DFSBlobStoreAccess.decode','org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toLong'
'com.cloudera.flume.handlers.hdfs.DFSEventSink.openWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.SequenceFile.createWriter'
'com.cloudera.flume.handlers.hdfs.DFSEventSink.append','org.apache.hadoop.io.SequenceFile.Writer.append'
'com.cloudera.flume.handlers.hdfs.DFSEventSink.close','org.apache.hadoop.io.SequenceFile.Writer.close'
'com.cloudera.flume.handlers.hdfs.DFSEventSink.openWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.SequenceFile.createWriter'
'com.cloudera.flume.handlers.hdfs.DFSEventSink.append','org.apache.hadoop.io.SequenceFile.Writer.append'
'com.cloudera.flume.handlers.hdfs.DFSEventSink.close','org.apache.hadoop.io.SequenceFile.Writer.close'
'tap.DFSStatTests.testSingleFile','org.apache.hadoop.mapred.JobConf.<init>'
'tap.DFSStatTests.testDirectory','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileStatus.isDir'
'tap.DFSStatTests.testRecursiveDirectory','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileStatus.isDir'
'tap.DFSStatTests.testMissing','org.apache.hadoop.mapred.JobConf.<init>'
'com.nexr.rhive.util.DFUtils.getInfoServer','org.apache.hadoop.net.NetUtils.getServerAddress'
'com.nexr.rhive.util.DFUtils.getFileInfo','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'org.apache.mahout.df.DFUtils.storeWritable','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.Writable.write org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.isi.mavuno.extract.DIRTExtractor.loadDependPairs','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append'
'edu.isi.mavuno.extract.DIRTExtractor.getContext','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.DNSAndCrawlStatsJoinStep.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.DNSAndCrawlStatsJoinStep.runStep','org.apache.hadoop.mapred.JobClient.runJob'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.DNSFailuresCollectorStep.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.DNSFailuresCollectorStep.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.DNSFailuresCollectorStep.reduce','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.DNSFailuresCollectorStep.runStep','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.parse.html.DOMContentUtils.setConf','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.nutch.parse.tika.DOMContentUtils.setConf','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.mahout.df.data.Data.extractLabels','org.apache.hadoop.fs.FileSystem.open'
'org.wso2.carbon.hdfs.dataaccess.DataAccessService.mountCurrentUserFileSystem','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get'
'org.wso2.carbon.hdfs.dataaccess.DataAccessService.mountFileSystem','org.apache.hadoop.fs.FileSystem.get'
'org.wso2.carbon.hdfs.dataaccess.DataAccessService.unmountFileSystem','org.apache.hadoop.fs.FileSystem.close'
'org.wso2.carbon.hdfs.dataaccess.DataAccessService.unmountAllFileSystems','org.apache.hadoop.fs.FileSystem.closeAll'
'com.infochimps.hbase.DataChunkToHFiles.TextToKeyValues.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get'
'com.infochimps.hbase.DataChunkToHFiles.TextToKeyValues.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.<init>'
'com.infochimps.hbase.DataChunkToHFiles.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.infochimps.hbase.DataChunkToHFiles.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.apache.hadoop.DataDrivenDBInputFormat.DataDrivenDBInputSplit.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'co.nubetech.apache.hadoop.DataDrivenDBInputFormat.DataDrivenDBInputSplit.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'co.nubetech.apache.hadoop.DataDrivenDBInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'co.nubetech.apache.hadoop.DataDrivenDBInputFormat.setBoundingQuery','org.apache.hadoop.conf.Configuration.set'
'co.nubetech.apache.hadoop.DataDrivenDBInputFormat.setInput','org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setInputFormatClass'
'org.apache.gora.avro.store.DataFileAvroStore.createFsInput','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.ga.watchmaker.cd.DataLineTest.testSet','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified'
'org.apache.mahout.classifier.df.data.DataLoader.loadData','org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.classifier.df.data.DataLoader.generateDataset','org.apache.hadoop.fs.FileSystem.open'
'com.inmobi.databus.purge.DataPurgerService.DataPurgerService','org.apache.hadoop.fs.FileSystem.get'
'com.inmobi.databus.purge.DataPurgerService.getTrashPathsToPurge','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.purge.DataPurgerService.getStreamsInCluster','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.purge.DataPurgerService.getStreamsPathToPurge','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.purge.DataPurgerService.purge','org.apache.hadoop.fs.FileSystem.delete'
'com.inmobi.databus.purge.DataPurgerService.getAllFilesInDir','org.apache.hadoop.fs.FileSystem.listStatus'
'edu.duke.starfish.whatif.data.DataSetModel.generateReduceShuffleSpecs','org.apache.hadoop.conf.Configuration.getInt'
'org.wso2.carbon.hive.data.source.access.util.DataSourceAccessUtil.getDataSourceProperties','org.apache.hadoop.hive.metastore.HiveContext.getCurrentContext'
'org.wso2.carbon.hive.data.source.access.util.DataSourceAccessUtil.getJdoConnectionUrl','org.apache.hadoop.conf.Configuration.getInt'
'org.commoncrawl.service.listcrawler.DataTransferAgent.hdfsCacheFileToLogFileLocation','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.service.listcrawler.DataTransferAgent.outOfOrderFileToLogFileLocation','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent'
'org.commoncrawl.service.listcrawler.DataTransferAgent.uploadSingeFile','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.listcrawler.DataTransferAgent.run','org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.hdfs.DistributedFileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hdfs.DistributedFileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hdfs.DistributedFileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.service.listcrawler.DataTransferAgent.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get'
'org.commoncrawl.service.listcrawler.DataTransferAgent.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'com.sap.hadoop.metadata.DataType.DataType','org.apache.hadoop.io.WritableComparator.get'
'com.sap.hadoop.metadata.DataType.compare','org.apache.hadoop.io.RawComparator<?>.compare'
'com.sap.hadoop.metadata.DataType.create','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.sap.hadoop.metadata.DataType.cloneInstance','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataInputBuffer.reset'
'com.sap.hadoop.metadata.DataType.getDataType','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory'
'com.sap.hadoop.metadata.DataType.initialValue','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init>'
'com.taobao.adfs.database.DatabaseExecutor.get','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'com.taobao.adfs.database.DatabaseExecutor.needMysqlServer','org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.database.DatabaseExecutorForHandlerSocket.DatabaseExecutorForHandlerSocket','org.apache.hadoop.conf.Configuration.<init>'
'com.taobao.adfs.database.DatabaseExecutorForHandlerSocketSimulator.DatabaseExecutorForHandlerSocketSimulator','org.apache.hadoop.conf.Configuration.<init>'
'com.taobao.adfs.database.DatabaseExecutorForHandlerSocketSimulator.createClient','org.apache.hadoop.conf.Configuration.getInt'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.MasterDatabaseIndex.MasterDatabaseIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.MasterDatabaseIndex.queryURLGivenURLFP','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.MasterDatabaseIndex.queryMetadataAndURLGivenFP','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.readFloat org.apache.hadoop.io.DataInputBuffer.readByte org.apache.hadoop.io.DataInputBuffer.readByte org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.getPosition'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.MasterDatabaseIndex.bulkQueryURLAndMetadataGivenInputStream','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.DataInputBuffer.reset'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.MasterDatabaseIndex.queryDomainMetadataGivenDomainId','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.MasterDatabaseIndex.queryDomainMetadataKeyAndIndex','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.MasterDatabaseIndex.collectAllTopLevelDomainRecordsByDomain','org.apache.hadoop.fs.FileSystem.toString org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.SlaveDatabaseIndex.queryURLList','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.SlaveDatabaseIndex.queryLinkDataByFP','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.SequenceFile.ValueBytes.writeUncompressedBytes org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.SlaveDatabaseIndex.queryDomainsGivenPattern','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.DataInputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.spillLinkDataIntoTempFileIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init>'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.compareRaw','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.readFloat org.apache.hadoop.io.DataInputBuffer.readFloat'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.compare','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readFloat org.apache.hadoop.io.DataInputBuffer.readFloat'
'org.commoncrawl.service.queryserver.index.DatabaseIndexV2.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'com.jaccson.DatabasesTable.removeCollection','org.apache.hadoop.io.Text.<init>'
'com.jaccson.DatabasesTable.removeDatabase','org.apache.hadoop.io.Text.<init>'
'com.inmobi.databus.Databus.main','org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled'
'com.inmobi.databus.DatabusTest.testDatabus','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.classifier.df.data.Dataset.load','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.classifier.df.data.Dataset.readFields','org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readStringArray'
'org.apache.mahout.classifier.df.data.Dataset.write','org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeStringArray'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.storeJobParameters','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.getSeed','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.getThreshold','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.isTraining','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.RndLineRecordReader.close','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.close'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.RndLineRecordReader.getProgress','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getProgress'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.RndLineRecordReader.initialize','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.initialize'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.RndLineRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentKey org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue org.apache.hadoop.io.Text.set'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplit.DatasetTextInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplitTest.MockReader.nextKeyValue','org.apache.hadoop.io.LongWritable.set'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplitTest.testTrainingTestingSets','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.ga.watchmaker.cd.hadoop.DatasetSplitTest.testStoreJobParameters','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.cf.taste.hadoop.als.DatasetSplitter.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.hadoop.als.DatasetSplitter.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.cf.taste.hadoop.als.DatasetSplitter.WritePrefsMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.utils.eval.DatasetSplitter.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.utils.eval.DatasetSplitter.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.utils.eval.DatasetSplitter.WritePrefsMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.df.data.Dataset.load','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.mahout.df.data.Dataset.readFields','org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readStringArray org.apache.hadoop.io.WritableUtils.readStringArray'
'org.apache.mahout.df.data.Dataset.write','org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeStringArray org.apache.hadoop.io.WritableUtils.writeStringArray'
'org.apache.expreval.expr.betweenstmt.DateBetweenStmt.DateBetweenComparable.compareTo','org.apache.hadoop.hbase.hbql.io.IO.getSerialization org.apache.hadoop.hbase.hbql.client.HBqlException.printStackTrace org.apache.hadoop.hbase.hbql.impl.Utils.logException'
'org.apache.expreval.expr.function.DateFunction.getValue','org.apache.hadoop.hbase.hbql.client.HBqlException.<init> org.apache.hadoop.hbase.hbql.client.HBqlException.<init>'
'com.inmobi.databus.utils.DatePathComparator.compare','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getParent'
'com.mozilla.pig.load.DateRangeLoader.getInputFormat','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init>'
'com.mozilla.pig.load.DateRangeLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'com.mozilla.pig.load.DateRangeLoader.getNext','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue org.apache.hadoop.io.Text.toString'
'co.nubetech.apache.hadoop.DateSplitter.split','org.apache.hadoop.conf.Configuration.getInt'
'org.lilyproject.repository.impl.primitivevaluetype.DateValueType.fromBytes','org.apache.hadoop.hbase.util.Bytes.toLong'
'org.lilyproject.repository.impl.primitivevaluetype.DateValueType.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'babel.prep.datedcorpus.DatedCorpusGenMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,babel.content.pages.PageVersion>.collect'
'com.datasalt.utils.io.DatumPairBase.setItem1','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.set'
'com.datasalt.utils.io.DatumPairBase.setItem2','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.set'
'com.datasalt.utils.io.DatumPairBase.write','org.apache.hadoop.io.BytesWritable.write org.apache.hadoop.io.BytesWritable.write'
'com.datasalt.utils.io.DatumPairBase.readFields','org.apache.hadoop.io.BytesWritable.readFields org.apache.hadoop.io.BytesWritable.readFields'
'com.datasalt.utils.io.DatumPairBase.equals','org.apache.hadoop.io.BytesWritable.equals org.apache.hadoop.io.BytesWritable.equals'
'com.datasalt.utils.io.DatumPairBase.compareTo','org.apache.hadoop.io.BytesWritable.compareTo org.apache.hadoop.io.BytesWritable.compareTo'
'com.datasalt.utils.io.DatumPairBase.hashCode','org.apache.hadoop.io.BytesWritable.hashCode org.apache.hadoop.io.BytesWritable.hashCode'
'com.datasalt.utils.io.DatumPairBase.ComparatorWithNoOrder.compare','org.apache.hadoop.io.WritableComparator.compareBytes'
'com.livingsocial.hive.udf.DayOfWeek.evaluate','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set'
'com.sematext.hbase.hut.DebugUtil.getContentAsText','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next'
'com.sematext.hbase.hut.DebugUtil.append','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'com.sematext.hbase.hut.DebugUtil.getHutRowKeyAsText','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.tail org.apache.hadoop.hbase.util.Bytes.head org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.tail org.apache.hadoop.hbase.util.Bytes.toLong'
'com.sematext.hbase.hut.DebugUtil.getText','org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'com.sematext.hbase.hut.DebugUtil.clean','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.ResultScanner.next'
'org.apache.mahout.df.DecisionForest.load','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.<init> org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.DeduperKey.setKey','org.apache.hadoop.io.LongWritable.set'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.JSONSetBuilder.reset','org.apache.hadoop.io.DataOutputBuffer.reset'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.JSONSetBuilder.flush','org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.SimhashMatcher.readLongComponent','org.apache.hadoop.io.DataOutputBuffer.getData'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.SimhashMatcher.textFromPackedLongInfo','org.apache.hadoop.io.DataOutputBuffer.getData'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.SimhashMatcher.emitMatches','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.DataOutputBuffer.size'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.SetUnionFinder.reset','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.reset'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.SetUnionFinder.insertItemGetId','org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataOutputBuffer.writeInt'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.SetUnionFinder.textFromPackedLongInfo','org.apache.hadoop.io.DataOutputBuffer.getData'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.SetUnionFinder.readLongComponent','org.apache.hadoop.io.DataOutputBuffer.getData'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.SetUnionFinder.emit','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.ec2.postprocess.deduper.DeduperUtils.main','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.<init> org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.<init> org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.<init>'
'org.apache.accumulo.core.util.format.DefaultFormatter.appendText','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'ivory.core.data.dictionary.DefaultFrequencySortedDictionary.DefaultFrequencySortedDictionary','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.dictionary.DefaultFrequencySortedDictionary.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'ivory.core.data.dictionary.DefaultFrequencySortedDictionaryTest.test1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.impl.builtin.DefaultIndexableLoader.initRightLoader','org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.testtools.inspect.DefaultInspector.inspect','org.apache.hadoop.io.Writable.getClass'
'org.apache.expreval.expr.literal.DefaultKeyword.getFilter','org.apache.hadoop.hbase.hbql.impl.InvalidServerFilterException.<init>'
'org.apache.hcatalog.mapreduce.DefaultOutputFormatContainer.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init>'
'org.apache.hcatalog.mapreduce.DefaultOutputFormatContainer.getOutputCommitter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getOutputCommitter'
'org.apache.hcatalog.mapreduce.DefaultOutputFormatContainer.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.hcatalog.mapreduce.DefaultRecordWriterContainer.DefaultRecordWriterContainer','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.hcatalog.mapreduce.DefaultRecordWriterContainer.write','org.apache.hadoop.hive.serde2.SerDe.serialize'
'org.cloudata.core.parallel.hadoop.DefaultTabletInputFormat.getInputTableInfos','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.apache.expreval.expr.calculation.DelegateCalculation.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'org.apache.expreval.expr.casestmt.DelegateCase.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'org.apache.expreval.expr.casestmt.DelegateCaseWhen.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'org.apache.expreval.expr.var.DelegateColumn.getValue','org.apache.hadoop.hbase.hbql.impl.InvalidColumnException.<init>'
'org.apache.expreval.expr.var.DelegateColumn.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidColumnException.<init>'
'org.apache.expreval.expr.var.DelegateColumn.setExpressionContext','org.apache.hadoop.hbase.hbql.impl.ColumnNotAllowedException.<init> org.apache.hadoop.hbase.hbql.mapping.ColumnAttrib.getFieldType org.apache.hadoop.hbase.hbql.mapping.ColumnAttrib.getFieldType org.apache.hadoop.hbase.hbql.client.HBqlException.<init>'
'org.apache.expreval.expr.ifthenstmt.DelegateIfThen.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'org.apache.expreval.expr.instmt.DelegateInStmt.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'com.datasalt.pangool.tuplemr.mapred.lib.input.DelegatingInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.InputFormat.getSplits'
'com.datasalt.pangool.tuplemr.mapred.lib.input.DelegatingRecordReader.DelegatingRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.InputFormat<com.datasalt.pangool.tuplemr.mapred.lib.input.K,com.datasalt.pangool.tuplemr.mapred.lib.input.V>.createRecordReader'
'com.datasalt.pangool.tuplemr.mapred.lib.input.DelegatingRecordReader.close','org.apache.hadoop.mapreduce.RecordReader<com.datasalt.pangool.tuplemr.mapred.lib.input.K,com.datasalt.pangool.tuplemr.mapred.lib.input.V>.close'
'com.datasalt.pangool.tuplemr.mapred.lib.input.DelegatingRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.RecordReader<com.datasalt.pangool.tuplemr.mapred.lib.input.K,com.datasalt.pangool.tuplemr.mapred.lib.input.V>.getCurrentKey'
'com.datasalt.pangool.tuplemr.mapred.lib.input.DelegatingRecordReader.getCurrentValue','org.apache.hadoop.mapreduce.RecordReader<com.datasalt.pangool.tuplemr.mapred.lib.input.K,com.datasalt.pangool.tuplemr.mapred.lib.input.V>.getCurrentValue'
'com.datasalt.pangool.tuplemr.mapred.lib.input.DelegatingRecordReader.getProgress','org.apache.hadoop.mapreduce.RecordReader<com.datasalt.pangool.tuplemr.mapred.lib.input.K,com.datasalt.pangool.tuplemr.mapred.lib.input.V>.getProgress'
'com.datasalt.pangool.tuplemr.mapred.lib.input.DelegatingRecordReader.initialize','org.apache.hadoop.mapreduce.RecordReader<com.datasalt.pangool.tuplemr.mapred.lib.input.K,com.datasalt.pangool.tuplemr.mapred.lib.input.V>.initialize'
'com.datasalt.pangool.tuplemr.mapred.lib.input.DelegatingRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.RecordReader<com.datasalt.pangool.tuplemr.mapred.lib.input.K,com.datasalt.pangool.tuplemr.mapred.lib.input.V>.nextKeyValue'
'com.asakusafw.bulkloader.cache.DeleteCacheStorageRemote.main','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.accumulo.core.util.shell.commands.DeleteCommand.execute','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.hcatalog.templeton.DeleteDelegator.run','org.apache.hadoop.security.UserGroupInformation.createRemoteUser org.apache.hadoop.mapred.JobTracker.getAddress org.apache.hadoop.mapred.TempletonJobTracker.<init> org.apache.hadoop.mapred.TempletonJobTracker.killJob org.apache.hadoop.mapred.TempletonJobTracker.killJob org.apache.hadoop.mapred.TempletonJobTracker.close'
'org.apache.hcatalog.templeton.DeleteDelegator.run','org.apache.hadoop.security.UserGroupInformation.createRemoteUser org.apache.hadoop.mapred.JobTracker.getAddress org.apache.hadoop.mapred.TempletonJobTracker.<init> org.apache.hadoop.mapred.TempletonJobTracker.killJob org.apache.hadoop.mapred.TempletonJobTracker.killJob org.apache.hadoop.mapred.TempletonJobTracker.close'
'org.apache.nutch.indexer.DeleteDuplicates.IndexDoc.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.MD5Hash.write org.apache.hadoop.io.Text.write'
'org.apache.nutch.indexer.DeleteDuplicates.IndexDoc.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.MD5Hash.readFields org.apache.hadoop.io.Text.readFields'
'org.apache.nutch.indexer.DeleteDuplicates.IndexDoc.compareTo','org.apache.hadoop.io.MD5Hash.equals org.apache.hadoop.io.MD5Hash.compareTo'
'org.apache.nutch.indexer.DeleteDuplicates.IndexDoc.equals','org.apache.hadoop.io.MD5Hash.equals org.apache.hadoop.io.Text.equals'
'org.apache.nutch.indexer.DeleteDuplicates.InputFormat.getSplits','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.FileSplit.<init>'
'org.apache.nutch.indexer.DeleteDuplicates.InputFormat.DDRecordReader.DDRecordReader','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileSplit.getPath'
'org.apache.nutch.indexer.DeleteDuplicates.InputFormat.DDRecordReader.next','org.apache.hadoop.io.Text.set'
'org.apache.nutch.indexer.DeleteDuplicates.InputFormat.DDRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.indexer.DeleteDuplicates.InputFormat.getRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.setStatus'
'org.apache.nutch.indexer.DeleteDuplicates.HashPartitioner.getPartition','org.apache.hadoop.io.MD5Hash.hashCode'
'org.apache.nutch.indexer.DeleteDuplicates.UrlsReducer.reduce','org.apache.hadoop.io.WritableUtils.cloneInto org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.MD5Hash,org.apache.nutch.indexer.DeleteDuplicates.IndexDoc>.collect org.apache.hadoop.io.WritableUtils.cloneInto org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.MD5Hash,org.apache.nutch.indexer.DeleteDuplicates.IndexDoc>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.MD5Hash,org.apache.nutch.indexer.DeleteDuplicates.IndexDoc>.collect'
'org.apache.nutch.indexer.DeleteDuplicates.HashReducer.configure','org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.indexer.DeleteDuplicates.HashReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.DeleteDuplicates.IndexDoc>.collect org.apache.hadoop.io.WritableUtils.cloneInto org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.indexer.DeleteDuplicates.IndexDoc>.collect org.apache.hadoop.io.WritableUtils.cloneInto'
'org.apache.nutch.indexer.DeleteDuplicates.setConf','org.apache.hadoop.fs.FileSystem.get'
'org.apache.nutch.indexer.DeleteDuplicates.map','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'org.apache.nutch.indexer.DeleteDuplicates.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.get'
'org.apache.nutch.indexer.DeleteDuplicates.getRecordWriter','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.<init>'
'org.apache.nutch.indexer.DeleteDuplicates.dedup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.indexer.DeleteDuplicates.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.indexer.DeleteDuplicates.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'client.DeleteExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.setTimestamp org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.close'
'client.DeleteListErrorExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.setTimestamp org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.close'
'client.DeleteListExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.setTimestamp org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.close'
'org.apache.accumulo.server.test.randomwalk.concurrent.DeleteRange.visit','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.DeleteRowsSplitTest.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.DeleteRowsSplitTest.generateRandomRange','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.apache.accumulo.server.master.tableOps.CleanUp.isReady','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.tableOps.CleanUp.call','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'client.DeleteTimestampExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete'
'org.apache.accumulo.server.test.randomwalk.shard.DeleteWord.visit','org.apache.hadoop.io.Text.<init>'
'goraci.Delete.run','org.apache.hadoop.conf.Configuration.<init>'
'goraci.Delete.main','org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.hiho.dedup.DelimitedLineRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'co.nubetech.hiho.dedup.DelimitedLineRecordReader.nextKeyValue','org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'co.nubetech.hiho.dedup.DelimitedLineRecordReader.close','org.apache.hadoop.util.LineReader.close'
'co.nubetech.hiho.dedup.DelimitedLineRecordReader.getColumn','org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.set'
'co.nubetech.hiho.dedup.DelimitedTextInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'co.nubetech.hiho.dedup.DelimitedTextInputFormat.isSplitable','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'co.nubetech.hiho.dedup.DelimitedTextInputFormat.setProperties','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.asakusafw.runtime.directio.util.DelimiterRangeInputStreamTest.readByte_random','org.apache.hadoop.io.InputBuffer.<init> org.apache.hadoop.io.InputBuffer.reset'
'com.asakusafw.runtime.directio.util.DelimiterRangeInputStreamTest.readArray_random','org.apache.hadoop.io.InputBuffer.<init> org.apache.hadoop.io.InputBuffer.reset'
'com.asakusafw.runtime.directio.util.DelimiterRangeInputStreamTest.copy','org.apache.hadoop.io.InputBuffer.reset org.apache.hadoop.io.InputBuffer.skip org.apache.hadoop.io.InputBuffer.reset org.apache.hadoop.io.InputBuffer.skip'
'edu.umd.cloud9.collection.aquaint2.DemoCountAquaint2Documents.MyMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.cloud9.collection.aquaint2.DemoCountAquaint2Documents.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.collection.aquaint2.DemoCountAquaint2Documents.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.collection.clue.DemoCountClueWarcRecords.MyMapper.configure','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal'
'edu.umd.cloud9.collection.clue.DemoCountClueWarcRecords.MyMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.cloud9.collection.clue.DemoCountClueWarcRecords.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.collection.clue.DemoCountClueWarcRecords.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.collection.clue.DemoCountClueWarcRecords.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.spinn3r.DemoCountSpinn3rEnglishPosts.MyMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.cloud9.collection.spinn3r.DemoCountSpinn3rEnglishPosts.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.collection.spinn3r.DemoCountSpinn3rEnglishPosts.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.collection.spinn3r.DemoCountSpinn3rEnglishPosts.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.line.DemoCountTextDocuments.MyMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.cloud9.collection.line.DemoCountTextDocuments.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.collection.line.DemoCountTextDocuments.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.collection.line.DemoCountTextDocuments.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.trec.DemoCountTrecDocuments.MyMapper.setup','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.getLocal'
'edu.umd.cloud9.collection.trec.DemoCountTrecDocuments.MyMapper.map','org.apache.hadoop.io.Text.set org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.collection.trec.DemoCountTrecDocuments.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counters.findCounter'
'edu.umd.cloud9.collection.trec.DemoCountTrecDocuments.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.wikipedia.DemoCountWikipediaPages.MyMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.cloud9.collection.wikipedia.DemoCountWikipediaPages.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.collection.wikipedia.DemoCountWikipediaPages.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.oozie.example.DemoMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.cloud9.example.simple.DemoMapredNullInput.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.example.simple.DemoMapreduceNullInput.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.memcached.demo.DemoMemcachedAccess.MyMapper.configure','org.apache.hadoop.mapred.JobConf.get'
'edu.umd.cloud9.example.memcached.demo.DemoMemcachedAccess.MyMapper.map','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.FloatWritable>.collect'
'edu.umd.cloud9.example.memcached.demo.DemoMemcachedAccess.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.example.simple.DemoPackTuples2.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.set'
'org.apache.oozie.example.DemoReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'org.apache.oozie.example.DemoReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.cloud9.example.simple.DemoWordCount.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'edu.umd.cloud9.example.simple.DemoWordCount.MyReducer.reduce','org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.example.simple.DemoWordCount.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.simple.DemoWordCount.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.simple.DemoWordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.simple.DemoWordCountTuple2.MapClass.map','org.apache.hadoop.io.Text.toString'
'edu.umd.cloud9.example.simple.DemoWordCountTuple2.ReduceClass.reduce','org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.example.simple.DemoWordCountTuple2.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.simple.DemoWordCountTuple2.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.simple.DemoWordCountTuple2.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.softlang.company.Department.setName','org.apache.hadoop.io.Text.<init>'
'org.softlang.company.Department.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields'
'org.softlang.company.Department.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.write org.apache.hadoop.io.Text.write'
'gov.llnl.ontology.mapreduce.stats.DependencyPathCountMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.DependencyPathCountMR.setupReducer','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'gov.llnl.ontology.mapreduce.stats.DependencyPathCountMR.DependencyPathCountMapper.setup','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.conf.Configuration.get'
'gov.llnl.ontology.mapreduce.stats.DependencyPathCountMR.DependencyPathCountMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'filters.DependentColumnFilterExample.filter','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.DependentColumnFilter.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.DependentColumnFilter.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setFilter org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'filters.DependentColumnFilterExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.BinaryPrefixComparator.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.BinaryPrefixComparator.<init> org.apache.hadoop.hbase.filter.RegexStringComparator.<init> org.apache.hadoop.hbase.filter.RegexStringComparator.<init>'
'uk.bl.wap.hadoop.mapreduce.lib.DereferencingArchiveToCDXRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize'
'uk.bl.wap.hadoop.mapreduce.lib.DereferencingArchiveToCDXRecordReader.close','org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'uk.bl.wap.hadoop.mapreduce.lib.DereferencingArchiveToCDXRecordReader.getProgress','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress'
'uk.bl.wap.hadoop.mapreduce.lib.DereferencingArchiveToCDXRecordReader.nextKeyValue','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'org.apache.mahout.classifier.df.tools.Describe.runTool','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.classifier.df.tools.Describe.generateDataset','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.classifier.df.tools.Describe.validateOutput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists'
'org.apache.mahout.df.tools.Describe.runTool','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.df.tools.Describe.generateDataset','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.df.tools.Describe.validateOutput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists'
'org.pentaho.hbase.shim.common.DeserializedBooleanComparator.getValue','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.pentaho.hbase.shim.common.DeserializedBooleanComparator.decodeBoolFromString','org.apache.hadoop.hbase.util.Bytes.toString'
'org.pentaho.hbase.shim.common.DeserializedBooleanComparator.decodeBoolFromNumber','org.apache.hadoop.hbase.util.Bytes.toShort org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toFloat org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toDouble'
'org.pentaho.hbase.shim.common.DeserializedNumericComparator.getValue','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.pentaho.hbase.shim.common.DeserializedNumericComparator.compareTo','org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toShort org.apache.hadoop.hbase.util.Bytes.toDouble org.apache.hadoop.hbase.util.Bytes.toFloat'
'ivory.lsh.data.DfTableArray.DfTableArray','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close'
'com.asakusafw.bulkloader.extractor.DfsFileImport.putCachePatch','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri'
'com.asakusafw.bulkloader.extractor.DfsFileImport.resolveLocation','org.apache.hadoop.conf.Configuration.<init>'
'com.asakusafw.bulkloader.extractor.DfsFileImport.write','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.bulkloader.extractor.DfsFileImport.getCompType','org.apache.hadoop.io.SequenceFile.CompressionType.valueOf'
'org.apache.accumulo.server.tabletserver.log.DfsLogger.DfsLogger','org.apache.hadoop.fs.Path.<init>'
'org.apache.accumulo.server.tabletserver.log.DfsLogger.open','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getDefaultReplication org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.accumulo.server.tabletserver.log.DfsLogger.getFileName','org.apache.hadoop.fs.Path.getName'
'org.apache.accumulo.server.tabletserver.log.DfsLogger.close','org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.accumulo.server.tabletserver.log.DfsLogger.defineTablet','org.apache.hadoop.fs.FSDataOutputStream.sync'
'com.mozilla.grouperfish.transforms.coclustering.text.Dictionary.loadDictionary','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'com.mozilla.grouperfish.transforms.coclustering.text.Dictionary.loadFeatureIndex','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'com.mozilla.grouperfish.transforms.coclustering.text.Dictionary.loadInvertedFeatureIndex','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'com.mozilla.grouperfish.transforms.coclustering.text.Dictionary.loadInvertedIndexWithKeys','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.vectorizer.DictionaryVectorizer.createTermFrequencyVectors','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.vectorizer.DictionaryVectorizer.createDictionaryChunks','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.vectorizer.DictionaryVectorizer.makePartialVectors','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.setCacheFiles org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.vectorizer.DictionaryVectorizer.startWordCounting','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.vectorizer.DictionaryVectorizerTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.vectorizer.DictionaryVectorizerTest.runTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.vectorizer.DictionaryVectorizer.createTermFrequencyVectors','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.vectorizer.DictionaryVectorizer.createDictionaryChunks','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.vectorizer.DictionaryVectorizer.makePartialVectors','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.setCacheFiles org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.vectorizer.DictionaryVectorizer.startWordCounting','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.mozilla.grouperfish.text.Dictionary.loadDictionary','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'com.mozilla.grouperfish.text.Dictionary.loadFeatureIndex','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'com.mozilla.grouperfish.text.Dictionary.loadInvertedFeatureIndex','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'com.mozilla.grouperfish.text.Dictionary.loadInvertedIndexWithKeys','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'org.sifarish.feature.DiffTypeSimilarity.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.sifarish.feature.DiffTypeSimilarity.main','org.apache.hadoop.util.ToolRunner.run'
'org.sifarish.feature.DiffTypeSimilarity.SimilarityMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.sifarish.feature.DiffTypeSimilarity.SimilarityMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.LongWritable.set'
'org.sifarish.feature.DiffTypeSimilarity.SimilarityReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.sifarish.feature.DiffTypeSimilarity.SimilarityReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'org.sifarish.feature.DiffTypeSimilarity.IdPairPartitioner.getPartition','org.apache.hadoop.io.LongWritable.get'
'org.sifarish.feature.DiffTypeSimilarity.IdPairGroupComprator.compare','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get'
'gr.ntua.cslab.distributed.dimension.DimFinderReducer.configure','org.apache.hadoop.mapred.JobConf.get'
'gr.ntua.cslab.distributed.dimension.DimFinderReducer.reduce','org.apache.hadoop.io.IntWritable.get'
'gr.ntua.cslab.distributed.dimension.DimFinderReducer.close','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'fr.insarennes.fafdti.builder.DirDeleter.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'com.asakusafw.directio.tools.DirectIoAbortTransaction.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.directio.tools.DirectIoApplyTransaction.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.directio.tools.DirectIoListTransactionTest.run','org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.directio.tools.DirectIoListTransactionTest.run_invalid','org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.testdriver.directio.DirectIoTestHelper.createConfiguration','org.apache.hadoop.conf.Configuration.addResource'
'com.asakusafw.testdriver.directio.DirectIoTestHelper.createFormat','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.asakusafw.runtime.directio.hadoop.DirectIoTransactionEditor.get','org.apache.hadoop.fs.Path.getFileSystem'
'com.asakusafw.runtime.directio.hadoop.DirectIoTransactionEditor.compare','org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getModificationTime'
'com.asakusafw.runtime.directio.hadoop.DirectIoTransactionEditor.toInfoObject','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'com.asakusafw.runtime.directio.hadoop.DirectIoTransactionEditor.doApply','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.runtime.directio.hadoop.DirectIoTransactionEditor.doAbort','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.runtime.directio.hadoop.DirectIoTransactionEditorTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.directio.hadoop.DirectIoTransactionEditorTest.indoubt','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create'
'com.cloudera.sqoop.manager.DirectMySQLExportTest.testAuthExport','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.create'
'skywriting.examples.skyhout.common.DirectOutputCollector.DirectOutputCollector','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.stage.directio.DirectOutputReducer.configure','org.apache.hadoop.conf.Configurable.setConf'
'com.asakusafw.runtime.stage.directio.DirectOutputSpec.createGroup','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.asakusafw.runtime.stage.directio.DirectOutputSpec.createOrder','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.service.directory.DirectoryServiceCmdLineTool.doImport','org.apache.hadoop.record.Buffer.<init>'
'org.commoncrawl.service.directory.DirectoryServiceServer.publish','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'org.commoncrawl.service.directory.DirectoryServiceServer.buildFullUserItemPath','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.directory.DirectoryServiceServer.loadSystemPaths','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.directory.DirectoryServiceServer.loadUserItems','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.directory.DirectoryServiceServer.loadItems','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.directory.DirectoryServiceServer.normalizeOutputPath','org.apache.hadoop.fs.Path.toString'
'org.commoncrawl.service.directory.DirectoryServiceServer.preLoadItemInfoFromPath','org.apache.hadoop.fs.Path.toString'
'org.commoncrawl.service.directory.DirectoryServiceTester.publishTestItem','org.apache.hadoop.record.Buffer.<init>'
'org.apache.mahout.clustering.dirichlet.DirichletClusterMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.dirichlet.DirichletClusterMapper.getClusters','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.dirichlet.DirichletClusterer.emitMostLikelyCluster','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.SequenceFile.Writer.append'
'org.apache.mahout.clustering.dirichlet.DirichletClusterer.emitAllClusters','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.SequenceFile.Writer.append'
'org.apache.mahout.clustering.dirichlet.DirichletDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.dirichlet.DirichletDriver.readPrototypeSize','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.clustering.dirichlet.DirichletDriver.buildClusters','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.dirichlet.DirichletDriver.clusterData','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.dirichlet.DirichletMapper.map','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.dirichlet.DirichletMapper.setup','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.dirichlet.DirichletMapper.getDirichletState','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.dirichlet.DirichletMapper.loadState','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.dirichlet.DirichletReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.dirichlet.DirichletReducer.setup','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'gov.llnl.ontology.mapreduce.ingest.DisambiguateMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.ingest.DisambiguateMR.setupConfiguration','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'gov.llnl.ontology.mapreduce.ingest.DisambiguateMR.DisambiguateMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.mycompany.hiaex.DisjointSelector.Mapper1.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init>'
'com.mycompany.hiaex.DisjointSelector.Reducer1.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init>'
'com.mycompany.hiaex.DisjointSelector.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.mycompany.hiaex.DisjointSelector.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.hama.bsp.DispatchTasksDirective.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeEnum'
'org.apache.hama.bsp.DispatchTasksDirective.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readEnum'
'org.apache.mahout.clustering.display.DisplayCanopy.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.display.DisplayClustering.plotClusteredSampleData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.display.DisplayClustering.writeSampleData','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.display.DisplayClustering.readClustersWritable','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.display.DisplayClustering.loadClustersWritable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.clustering.display.DisplayFuzzyKMeans.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.display.DisplayFuzzyKMeans.runSequentialFuzzyKClassifier','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.display.DisplayFuzzyKMeans.runSequentialFuzzyKClusterer','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.display.DisplayKMeans.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.display.DisplayKMeans.runSequentialKMeansClassifier','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.display.DisplayKMeans.runSequentialKMeansClusterer','org.apache.hadoop.fs.Path.<init>'
'com.mozilla.grouperfish.mahout.clustering.display.kmeans.DisplayKMeansBase.readClusteredPoints','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'com.mozilla.grouperfish.mahout.clustering.display.kmeans.DisplayKMeansBase.readClustersIteration','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.display.DisplayKMeans.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'com.mozilla.grouperfish.mahout.clustering.display.lda.DisplayLDABase.getTopWordsByTopic','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DoubleWritable.get'
'com.mozilla.grouperfish.mahout.clustering.display.lda.DisplayLDABase.getTopDocIdsByTopic','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.mahout.clustering.display.DisplayMeanShift.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.display.DisplaySpectralKMeans.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.display.DisplaySpectralKMeans.paint','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.display.DisplaySpectralKMeans.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init>'
'org.apache.crunch.util.DistCacheTest.setup','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.crunch.util.DistCacheTest.testAddJar','org.apache.hadoop.conf.Configuration.get'
'org.apache.crunch.util.DistCacheTest.testAddJarDirectory','org.apache.hadoop.conf.Configuration.get'
'org.springframework.data.hadoop.fs.DistCpTest.destroy','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.hadoop.DistcpActionExecutor.getLauncherMain','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.DistcpActionExecutor.getClassNamebyType','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings'
'com.inmobi.databus.distcp.DistcpBaseService.DistcpBaseService','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'com.inmobi.databus.distcp.DistcpBaseService.getDistCpOptions','org.apache.hadoop.tools.DistCpOptions.<init> org.apache.hadoop.tools.DistCpOptions.setBlocking'
'com.inmobi.databus.distcp.DistcpBaseService.executeDistCp','org.apache.hadoop.tools.DistCp.<init> org.apache.hadoop.tools.DistCp.execute'
'com.inmobi.databus.distcp.DistcpBaseService.doFinalCommit','org.apache.hadoop.fs.FileSystem.delete'
'com.inmobi.databus.distcp.DistcpBaseService.getDistCPInputFile','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.distcp.DistcpBaseService.readConsumePath','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.inmobi.databus.distcp.DistcpBaseService.getFinalPathForDistCP','org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.makeQualified'
'com.inmobi.databus.distcp.DistcpBaseService.createInputFileForDISCTP','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'sizzle.aggregators.DistinctAggregator.start','org.apache.hadoop.util.bloom.DynamicBloomFilter.<init>'
'sizzle.aggregators.DistinctAggregator.aggregate','org.apache.hadoop.util.bloom.Key.<init> org.apache.hadoop.util.bloom.Filter.membershipTest org.apache.hadoop.util.bloom.Filter.add'
'fr.eurecom.dsg.mapreduce.DistributedCacheJoin.run','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.eurecom.dsg.mapreduce.DistributedCacheJoin.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.DistributedFileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.ToolRunner.run'
'org.springframework.data.hadoop.fs.DistributedCacheTest.destroy','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.springframework.data.hadoop.fs.DistributedCacheTest.testClassPathArchives','org.apache.hadoop.filecache.DistributedCache.getArchiveClassPaths org.apache.hadoop.fs.Path.<init>'
'org.springframework.data.hadoop.fs.DistributedCacheTest.testClassPathFiles','org.apache.hadoop.filecache.DistributedCache.getFileClassPaths org.apache.hadoop.fs.Path.<init>'
'org.springframework.data.hadoop.fs.DistributedCacheTest.testCacheArchives','org.apache.hadoop.filecache.DistributedCache.getCacheArchives'
'org.springframework.data.hadoop.fs.DistributedCacheTest.testCacheFiles','org.apache.hadoop.filecache.DistributedCache.getCacheFiles'
'org.springframework.data.hadoop.fs.DistributedCacheTest.testLocalFiles','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.getName'
'org.springframework.data.hadoop.fs.DistributedCacheTest.testLocalArchives','org.apache.hadoop.filecache.DistributedCache.getLocalCacheArchives org.apache.hadoop.fs.Path.getName'
'org.springframework.data.hadoop.fs.DistributedCacheTest.testMisc','org.apache.hadoop.filecache.DistributedCache.getSymlink'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.getLockFileAt','org.apache.hadoop.fs.Path.<init>'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.isKettleEnvironmentInstalledAt','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.exists'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.installKettleEnvironment','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.stageBigDataPlugin','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.stagePluginsForCache','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init>'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.configureWithKettleEnvironment','org.apache.hadoop.fs.Path.<init>'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.addCachedFilesToClasspath','org.apache.hadoop.filecache.DistributedCache.createSymlink'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.addFileToClassPath','org.apache.hadoop.util.VersionInfo.getVersion org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.addCachedFiles','org.apache.hadoop.filecache.DistributedCache.createSymlink org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getName org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.disqualifyPath','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.stageForCache','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.setReplication'
'org.pentaho.hadoop.shim.common.DistributedCacheUtilImpl.findFiles','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.taobao.adfs.distributed.DistributedClient.DistributedClient','org.apache.hadoop.conf.Configuration.<init>'
'com.taobao.adfs.distributed.DistributedClient.configLogger','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.taobao.adfs.distributed.DistributedClient.createMetrics','org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.distributed.DistributedClient.getDistributedDataProtocols','org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.distributed.DistributedClient.getDistributedDataTypeName','org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.distributed.DistributedClient.createServerProxy','org.apache.hadoop.net.NetUtils.createSocketAddr'
'com.taobao.adfs.distributed.DistributedClient.invoke','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'com.taobao.adfs.distributed.DistributedClient.waitMaster','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong'
'com.taobao.adfs.distributed.DistributedClient.followConfSettings','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.taobao.adfs.distributed.DistributedClient.followLoggerLevels','org.apache.hadoop.conf.Configuration.getStrings'
'com.taobao.adfs.distributed.DistributedClient.DistributedLeaseThread.run','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.mahout.math.hadoop.solver.DistributedConjugateGradientSolver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.math.hadoop.solver.DistributedConjugateGradientSolver.loadInputVector','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.math.hadoop.solver.DistributedConjugateGradientSolver.saveOutputVector','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.math.hadoop.solver.DistributedConjugateGradientSolver.DistributedConjugateGradientSolverJob.run','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.solver.DistributedConjugateGradientSolver.main','org.apache.hadoop.util.ToolRunner.run'
'com.taobao.adfs.distributed.DistributedData.initialize','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getLong'
'com.taobao.adfs.distributed.DistributedData.openElementToTransfer','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.taobao.adfs.distributed.DistributedData.getIsInIncrementRestoreStage','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.distributed.DistributedData.setIsInIncrementRestoreStage','org.apache.hadoop.conf.Configuration.setBoolean'
'com.taobao.adfs.distributed.test.DistributedDataRepositoryBaseOnTableTest.getRepository','org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.distributed.DistributedDataTest.setupAfterClass','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.set'
'com.taobao.adfs.distributed.editlogger.DistributedEditLogger.getDistributedEditLogger','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.getClass'
'com.taobao.adfs.distributed.editlogger.DistributedEditLogger.DistributedEditLogger','org.apache.hadoop.conf.Configuration.<init>'
'com.taobao.adfs.distributed.editlogger.DistributedEditLogger.open','org.apache.hadoop.conf.Configuration.getInt'
'eu.stratosphere.nephele.fs.hdfs.DistributedFileSystem.DistributedFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.getClass'
'eu.stratosphere.nephele.fs.hdfs.DistributedFileSystem.initialize','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.serializeOutput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.main','org.apache.hadoop.util.ToolRunner.run'
'com.taobao.adfs.distributed.metrics.DistributedMetrics.DistributedMetrics','org.apache.hadoop.conf.Configuration.<init>'
'com.taobao.adfs.distributed.metrics.DistributedMetrics.open','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.metrics.MetricsUtil.getContext org.apache.hadoop.metrics.MetricsUtil.createRecord org.apache.hadoop.metrics.MetricsRecord.setTag org.apache.hadoop.metrics.MetricsContext.registerUpdater'
'com.taobao.adfs.distributed.metrics.DistributedMetrics.doUpdates','org.apache.hadoop.metrics.util.MetricsRegistry.getMetricsList org.apache.hadoop.metrics.util.MetricsBase.pushMetric org.apache.hadoop.metrics.MetricsRecord.update'
'com.taobao.adfs.distributed.metrics.DistributedMetrics.DistributedActivtyMBean.DistributedActivtyMBean','org.apache.hadoop.metrics.util.MBeanUtil.registerMBean'
'com.taobao.adfs.distributed.metrics.DistributedMetrics.DistributedActivtyMBean.shutdown','org.apache.hadoop.metrics.util.MBeanUtil.unregisterMBean'
'org.apache.mahout.math.hadoop.DistributedRowMatrix.setConf','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.math.hadoop.DistributedRowMatrix.setOutputTempPathString','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.DistributedRowMatrix.iterateAll','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.DistributedRowMatrix.times','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.math.hadoop.DistributedRowMatrix.transpose','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.mahout.math.hadoop.DistributedRowMatrix.timesSquared','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete'
'com.sematext.hbase.wd.DistributedScanner.close','org.apache.hadoop.hbase.client.ResultScanner.close'
'com.sematext.hbase.wd.DistributedScanner.create','org.apache.hadoop.hbase.client.HTableInterface.getScanner'
'com.sematext.hbase.wd.DistributedScanner.nextInternal','org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.compareTo'
'org.apache.nutch.searcher.DistributedSegmentBean.DistributedSegmentBean','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.ipc.RPC.getProxy'
'com.taobao.adfs.distributed.DistributedShell.getDataClient','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.taobao.adfs.distributed.DistributedShell.shellForClient','org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.distributed.DistributedShell.shellForMonitor','org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.distributed.DistributedShell.invokeMethod','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean'
'com.taobao.adfs.distributed.DistributedShell.stringToObject','org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.permission.FsPermission.<init>'
'org.apache.nutch.tools.DmozParser.RDFProcessor.startElement','org.apache.hadoop.io.MD5Hash.digest'
'org.apache.nutch.tools.DmozParser.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close'
'org.apache.crunch.DoFn.getConfiguration','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getConfiguration'
'org.apache.crunch.DoFn.getCounter','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getCounter org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getCounter'
'org.apache.crunch.DoFn.increment','org.apache.hadoop.mapreduce.Counter.increment'
'org.apache.crunch.DoFn.progress','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.progress'
'org.apache.crunch.DoFn.getTaskAttemptID','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'org.apache.crunch.DoFn.setStatus','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.setStatus'
'org.apache.crunch.DoFn.getStatus','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getStatus'
'org.cloudata.examples.web.DocFreqJob.exec','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.cloudata.examples.web.DocFreqMap.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.collect'
'ivory.core.data.stat.DocScoreTable4BF.initialize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readFloat org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.stat.DocScoreTable4BF.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.collection.DocnoMapping.BuilderUtils.parseDefaultOptions','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'com.digitalpebble.behemoth.DocumentFilter.getFilters','org.apache.hadoop.conf.Configuration.getValByRegex org.apache.hadoop.conf.Configuration.getValByRegex org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'com.digitalpebble.behemoth.DocumentFilter.keep','org.apache.hadoop.io.MapWritable.isEmpty org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.Writable.toString'
'edu.umd.cloud9.collection.DocumentForwardIndexHttpServer.ServerMapper.run','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FSDataOutputStream.writeUTF org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.collection.DocumentForwardIndexHttpServer.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.close'
'org.solbase.lucenehbase.DocumentLoader.loadObject','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'org.solbase.lucenehbase.DocumentLoader.updateObjectStore','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.apache.mahout.vectorizer.DocumentProcessorTest.testTokenizeDocuments','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.mahout.vectorizer.DocumentProcessorTest.testTokenizeDocuments','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.mahout.vectorizer.DocumentProcessor.tokenizeDocuments','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'pl.edu.icm.coansys.importers.io.writers.hbase.DocumentWrapperSequenceFileIncrementalLoadConfigurator.main','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad org.apache.hadoop.mapreduce.Job.getNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.getCacheFiles'
'pl.edu.icm.coansys.importers.io.writers.hbase.DocumentWrapperSequenceFileToHBase.DocumentWrapperToHBasePutMapper.map','org.apache.hadoop.io.BytesWritable.copyBytes org.apache.hadoop.hbase.io.ImmutableBytesWritable.set org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add'
'pl.edu.icm.coansys.importers.io.writers.hbase.DocumentWrapperSequenceFileToHBase.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.addInputPath org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'pl.edu.icm.coansys.importers.io.writers.hbase.DocumentWrapperSequenceFileToHBase.getOptimizedConfiguration','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'pl.edu.icm.coansys.importers.io.writers.hbase.DocumentWrapperSequenceFileToHBase.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.ToolRunner.run'
'com.rapleaf.hank.hadoop.DomainBuilderAbstractOutputFormat.checkOutputSpecs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.rapleaf.hank.hadoop.DomainBuilderAbstractOutputFormat.getTaskAttemptOutputPath','org.apache.hadoop.mapred.JobConf.get'
'com.rapleaf.hank.hadoop.DomainBuilderAbstractOutputFormat.getJobOutputPath','org.apache.hadoop.mapred.JobConf.get'
'com.rapleaf.hank.cascading.DomainBuilderAssembly.KeepPartitions.isRemove','org.apache.hadoop.io.IntWritable.get'
'com.rapleaf.hank.cascading.DomainBuilderAssembly.AddPartitionAndComparableKeyFields.operate','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.<init>'
'com.rapleaf.hank.hadoop.DomainBuilderOutputCommitter.commitJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.rename'
'com.rapleaf.hank.hadoop.DomainBuilderOutputCommitter.cleanupJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'com.rapleaf.hank.hadoop.DomainBuilderProperties.setJobConfProperties','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'com.rapleaf.hank.hadoop.DomainBuilderProperties.getRequiredConfigurationItem','org.apache.hadoop.mapred.JobConf.get'
'com.rapleaf.hank.hadoop.DomainBuilderReducer.reduce','org.apache.hadoop.mapred.OutputCollector<com.rapleaf.hank.hadoop.KeyAndPartitionWritable,com.rapleaf.hank.hadoop.ValueWritable>.collect'
'com.rapleaf.hank.cascading.DomainBuilderTap.sinkConfInit','org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputCommitter org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set'
'com.rapleaf.hank.cascading.DomainBuilderTap.DomainBuilderScheme.sink','org.apache.hadoop.io.IntWritable.<init>'
'org.commoncrawl.mapred.pipelineV3.domainmeta.DomainMetadataTask.getRestrictedParitionPaths','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.pipelineV3.domainmeta.DomainMetadataTask.DomainMetadataTask','org.apache.hadoop.conf.Configuration.<init>'
'org.commoncrawl.mapred.pipelineV3.domainmeta.DomainMetadataTask.getParitionPaths','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.pipelineV3.domainmeta.DomainMetadataTask.getTaskIdentityBasePath','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.pipelineV3.domainmeta.DomainMetadataTask.getTaskOutputBaseDir','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.pipelineV3.domainmeta.DomainMetadataTask.parseArgs','org.apache.hadoop.util.StringUtils.stringifyException'
'elephantdb.DomainSpec.readFromFileSystem','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'elephantdb.DomainSpec.exists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'elephantdb.DomainSpec.writeToFileSystem','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'elephantdb.DomainSpec.write','org.apache.hadoop.io.WritableUtils.writeString'
'elephantdb.DomainSpec.readFields','org.apache.hadoop.io.WritableUtils.readString'
'org.apache.nutch.util.domain.DomainStatistics.run','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.nutch.util.domain.DomainStatistics.DomainStatisticsMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.nutch.util.domain.DomainStatistics.DomainStatisticsReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'org.apache.nutch.util.domain.DomainStatistics.DomainStatisticsCombiner.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'org.apache.nutch.util.domain.DomainStatistics.main','org.apache.hadoop.util.ToolRunner.run'
'elephantdb.store.DomainStore.synchronizeVersions','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileUtil.copy'
'org.apache.nutch.util.domain.DomainSuffixesReader.read','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.util.domain.DomainSuffixes.DomainSuffixes','org.apache.hadoop.util.StringUtils.stringifyException'
'com.mycompany.hiaex.DotProductTest.setUp','org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.setMapper org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.setReducer org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.setMapper org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.setReducer'
'com.mycompany.hiaex.DotProductTest.testMapper1','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.withInput org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.withOutput org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.runTest'
'com.mycompany.hiaex.DotProductTest.testReducer1','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.withOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.runTest'
'org.apache.giraph.aggregators.DoubleMaxAggregator.aggregate','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.giraph.aggregators.DoubleMaxAggregator.createInitialValue','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.aggregators.DoubleOverwriteAggregator.aggregate','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.giraph.aggregators.DoubleOverwriteAggregator.createInitialValue','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.DoubleSumAggregator.aggregate','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.giraph.examples.DoubleSumAggregator.setAggregatedValue','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.giraph.examples.DoubleSumAggregator.getAggregatedValue','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.DoubleSumAggregator.createAggregatedValue','org.apache.hadoop.io.DoubleWritable.<init>'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.DoubleSumReducer.reduce','org.apache.hadoop.io.DoubleWritable.<init>'
'org.lilyproject.repository.impl.primitivevaluetype.DoubleValueType.fromBytes','org.apache.hadoop.hbase.util.Bytes.toDouble'
'org.lilyproject.repository.impl.primitivevaluetype.DoubleValueType.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.pentaho.hadoop.mapreduce.converter.converters.DoubleWritableToDoubleConverter.canConvert','org.apache.hadoop.io.DoubleWritable.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.DoubleWritableToDoubleConverter.convert','org.apache.hadoop.io.DoubleWritable.get'
'org.pentaho.hadoop.mapreduce.converter.converters.DoubleWritableToLongConverterTest.convert','org.apache.hadoop.io.DoubleWritable.<init>'
'hipi.examples.downloader.Downloader.DownloaderMapper.map','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.Text.<init>'
'hipi.examples.downloader.Downloader.DownloaderReducer.reduce','org.apache.hadoop.io.BooleanWritable.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.Text.<init>'
'hipi.examples.downloader.Downloader.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'hipi.examples.downloader.Downloader.createDir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'hipi.examples.downloader.Downloader.main','org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.hbase.Driver.main','org.apache.hadoop.util.ProgramDriver.<init> org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.driver'
'org.apache.mahout.utils.vectors.lucene.Driver.getSeqFileWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter'
'mapreduce.Driver.main','org.apache.hadoop.util.ProgramDriver.<init> org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.driver'
'neuralnet.mapred.Driver.Driver','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getClassLoader'
'neuralnet.mapred.Driver.writeShortRunParams','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'neuralnet.mapred.Driver.shareShortRunParams','org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'neuralnet.mapred.Driver.runHDFS','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.waitForCompletion'
'neuralnet.mapred.Driver.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.ToolRunner.run'
'com.senseidb.indexing.hadoop.map.DummyMapInputConverter.getJsonInput','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.common.DummyRecordWriter.build','org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'.DumpColumnNames.ColumnFamilyMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'.DumpColumnNames.ColumnFamilyMapper.setup','org.apache.hadoop.conf.Configuration.get'
'.DumpColumnNames.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'.DumpColumnNames.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.service.crawler.util.DumpCrawlLog.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'com.twitter.corpus.download.DumpHtmlStatusCrawl.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'com.datasalt.utils.commons.io.DumpSequenceFileAsText.execute','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'com.datasalt.utils.commons.io.DumpTextFileAsSequenceFile.dump','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.commoncrawl.server.DynamicClassLoader.loaderFromProjectRoot','org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.util.EC2MetadataTransferUtil.EC2MetadataTransferUtil','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.util.EC2MetadataTransferUtil.downloadComplete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.commoncrawl.util.EC2MetadataTransferUtil.downloadFailed','org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.util.EC2MetadataTransferUtil.downloadStarting','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.create'
'org.commoncrawl.service.parser.ec2.EC2ParserNode.EC2ParserNode','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get'
'org.commoncrawl.service.parser.ec2.EC2ParserNode.buildCrawlLogPath','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.parser.ec2.EC2ParserNode.buildCrawlLogCheckpointPath','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.parser.ec2.EC2ParserNode.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.commoncrawl.service.parser.ec2.EC2ParserNode.doCheckpoint','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'org.commoncrawl.service.parser.ec2.EC2ParserNode.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'edu.umd.hooka.alignment.EFMarginalCounter.MarginalMapper.close','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.hooka.alignment.IndexedFloatArray>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.hooka.alignment.IndexedFloatArray>.collect'
'edu.umd.hooka.alignment.EFMarginalCounter.MarginalReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.hooka.alignment.IndexedFloatArray>.collect'
'edu.umd.hooka.alignment.EFMarginalCounter.computeMarginals','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.hooka.alignment.EFMarginalCounter.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'edu.umd.hooka.alignment.EFMarginals.EFMarginals','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.oozie.util.ELConstantsFunctions.toConfigurationStr','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.ELService.extractConstants','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.oozie.service.ELService.extractFunctions','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.mahout.math.hadoop.decomposer.EigenVerificationJob.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.decomposer.EigenVerificationJob.saveCleanEigens','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'org.apache.mahout.math.hadoop.decomposer.EigenVerificationJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.hadoop.decomposer.EigenVerificationJob.runJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.spectral.eigencuts.EigencutsDriver.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.spectral.eigencuts.EigencutsDriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.spectral.eigencuts.EigencutsDriver.performEigenDecomposition','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.spectral.eigencuts.EigencutsSensitivityJob.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.spectral.eigencuts.EigencutsSensitivityReducer.reduce','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'backtype.hadoop.ElasticScheme.sourceInit','org.apache.hadoop.mapred.JobConf.setInputFormat'
'backtype.hadoop.ElasticScheme.sinkInit','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat'
'backtype.hadoop.ElasticScheme.sink','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector.collect'
'backtype.hadoop.ElasticSearchInputFormat.setConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'backtype.hadoop.ElasticSearchInputFormat.ElasticSearchRecordReader.initialize','org.apache.hadoop.mapred.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'backtype.hadoop.ElasticSearchInputFormat.ElasticSearchRecordReader.next','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'backtype.hadoop.ElasticSearchInputFormat.ElasticSearchRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'backtype.hadoop.ElasticSearchInputFormat.ElasticSearchRecordReader.createValue','org.apache.hadoop.io.Text.<init>'
'com.infochimps.elasticsearch.ElasticSearchInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.infochimps.elasticsearch.ElasticSearchInputFormat.setConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.infochimps.elasticsearch.ElasticSearchInputFormat.ElasticSearchRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.infochimps.elasticsearch.ElasticSearchInputFormat.ElasticSearchRecordReader.nextKeyValue','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.infochimps.elasticsearch.ElasticSearchOutputFormat.ElasticSearchRecordWriter.ElasticSearchRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.infochimps.elasticsearch.ElasticSearchOutputFormat.ElasticSearchRecordWriter.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get'
'com.infochimps.elasticsearch.ElasticSearchOutputFormat.ElasticSearchRecordWriter.buildContent','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.BooleanWritable.get org.apache.hadoop.io.MapWritable.entrySet org.apache.hadoop.io.ArrayWritable.get'
'backtype.hadoop.ElasticSearchSplit.readFields','org.apache.hadoop.io.Text.readString'
'backtype.hadoop.ElasticSearchSplit.write','org.apache.hadoop.io.Text.writeString'
'backtype.hadoop.ElasticTap.getPath','org.apache.hadoop.fs.Path.<init>'
'com.infochimps.elasticsearch.ElasticTest.IndexMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.NullWritable.get'
'com.infochimps.elasticsearch.ElasticTest.IndexMapper.setup','org.apache.hadoop.conf.Configuration.get'
'com.infochimps.elasticsearch.ElasticTest.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.infochimps.elasticsearch.ElasticTest.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'backtype.hadoop.ElasticUtil.toWritable','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.ArrayWritable.<init> org.apache.hadoop.io.NullWritable.get'
'backtype.hadoop.ElasticUtil.uploadLocalFile','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'backtype.hadoop.ElasticUtil.uploadLocalFileIfChanged','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'backtype.hadoop.ElasticUtil.fetchFileFromCache','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.ElasticUtil.fetchArchiveFromCache','org.apache.hadoop.filecache.DistributedCache.getLocalCacheArchives org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.ElasticUtil.shipFileIfNotShipped','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'backtype.hadoop.ElasticUtil.shipArchiveIfNotShipped','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheArchive'
'org.apache.nutch.indexer.elasticsearch.ElasticsearchWriter.open','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'elephantdb.cascading.ElephantDBTap.sourceConfInit','org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setInt'
'elephantdb.cascading.ElephantDBTap.sinkConfInit','org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setSpeculativeExecution'
'elephantdb.cascading.ElephantDBTap.getPath','org.apache.hadoop.fs.Path.<init>'
'elephantdb.cascading.ElephantDBTap.commitResource','org.apache.hadoop.fs.Path.<init>'
'elephantdb.hadoop.ElephantInputFormat.ElephantRecordReader.next','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.mapred.Reporter.progress org.apache.hadoop.mapred.Reporter.progress'
'elephantdb.hadoop.ElephantInputFormat.ElephantRecordReader.createKey','org.apache.hadoop.io.BytesWritable.<init>'
'elephantdb.hadoop.ElephantInputFormat.ElephantRecordReader.createValue','org.apache.hadoop.io.BytesWritable.<init>'
'elephantdb.hadoop.ElephantInputFormat.ElephantInputSplit.getLength','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getContentSummary'
'elephantdb.hadoop.ElephantInputFormat.ElephantInputSplit.write','org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.mapred.JobConf.write'
'elephantdb.hadoop.ElephantInputFormat.ElephantInputSplit.readFields','org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.readFields'
'elephantdb.hadoop.ElephantInputFormat.getSplits','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'elephantdb.hadoop.ElephantOutputFormat.ElephantRecordWriter.write','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'elephantdb.hadoop.ElephantOutputFormat.ElephantRecordWriter.close','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'elephantdb.hadoop.ElephantOutputFormat.ElephantRecordWriter.progress','org.apache.hadoop.util.Progressable.progress'
'elephantdb.hadoop.ElephantOutputFormat.checkOutputSpecs','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.InvalidJobConfException.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.InvalidJobConfException.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.InvalidJobConfException.<init>'
'elephantdb.cascading.ElephantScheme.sourceConfInit','org.apache.hadoop.mapred.JobConf.setInputFormat'
'elephantdb.cascading.ElephantScheme.sinkConfInit','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat'
'elephantdb.cascading.ElephantScheme.sink','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.BytesWritable.<init>'
'org.apache.mahout.cf.taste.example.email.EmailUtility.loadDictionaries','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'org.apache.ivory.cluster.util.EmbeddedCluster.newCluster','org.apache.hadoop.security.UserGroupInformation.createRemoteUser org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.ivory.cluster.util.EmbeddedCluster.createClusterAsUser','org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.ivory.cluster.util.EmbeddedCluster.buildClusterObject','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.ivory.cluster.util.EmbeddedCluster.shutdown','org.apache.hadoop.mapred.MiniMRCluster.shutdown org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'com.odiago.flumebase.flume.EmbeddedFlumeConfig.isStandalone','org.apache.hadoop.conf.Configuration.getBoolean'
'com.odiago.flumebase.flume.EmbeddedFlumeConfig.start','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.odiago.flumebase.flume.EmbeddedFlumeConfig.connectToMaster','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'com.jointhegrid.hive_test.EmbeddedHiveTest.testUseEmbedded','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'com.asakusafw.modelgen.emitter.EmitterTestRoot.restore','org.apache.hadoop.io.Writable.write org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.Writable.hashCode'
'org.softlang.company.Employee.setName','org.apache.hadoop.io.Text.<init>'
'org.softlang.company.Employee.setAddress','org.apache.hadoop.io.Text.<init>'
'org.softlang.company.Employee.setSalary','org.apache.hadoop.io.DoubleWritable.<init>'
'org.softlang.company.Employee.setManager','org.apache.hadoop.io.BooleanWritable.<init>'
'org.softlang.company.Employee.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.readFields'
'org.softlang.company.Employee.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.DoubleWritable.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.BooleanWritable.write'
'com.asakusafw.runtime.stage.output.EmptyFileOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.RecordWriter<java.lang.Object,java.lang.Object>.<init>'
'edu.umd.cloud9.example.bfs.EncodeBFSGraph.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.example.bfs.EncodeBFSGraph.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.bfs.EncodeBFSGraph.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest.runTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.nutch.util.EncodingDetector.EncodingDetector','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.util.EncodingDetector.main','org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.util.EncodingDetector.EncodingDetector','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.util.EncodingDetector.main','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.vectorizer.EncodingMapper.setup','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.vectorizer.EncodingMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.lilyproject.repository.impl.EncodingUtil.generateRecordTypeRowKey','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.repository.impl.EncodingUtil.generateFieldTypeRowKey','org.apache.hadoop.hbase.util.Bytes.toBytes'
'coprocessor.EndpointCombinedExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.split org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getRegionsInfo org.apache.hadoop.hbase.client.HTable.coprocessorExec org.apache.hadoop.hbase.util.Bytes.toString'
'coprocessor.EndpointCombinedExample.call','org.apache.hadoop.hbase.util.Pair.<init>'
'coprocessor.EndpointProxyExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.split org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getRegionsInfo org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.coprocessorProxy'
'org.visitante.basic.EngagementEventGenerator.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.visitante.basic.EngagementEventGenerator.SessionReducer.setup','org.apache.hadoop.mapreduce.Reducer.Context.getConfiguration org.apache.hadoop.mapreduce.Reducer.Context.getConfiguration'
'org.visitante.basic.EngagementEventGenerator.SessionReducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.Reducer.Context.write'
'org.visitante.basic.EngagementEventGenerator.main','org.apache.hadoop.util.ToolRunner.run'
'meetup.beeno.EntityIndexer.getUpdateForValue','org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.getTimeStamp org.apache.hadoop.hbase.client.Put.setTimeStamp org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.add'
'meetup.beeno.EntityIndexer.getValue','org.apache.hadoop.hbase.KeyValue.matchingColumn org.apache.hadoop.hbase.KeyValue.getValue'
'meetup.beeno.EntityIndexer.DefaultKeyFactory.createKey','org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add'
'meetup.beeno.EntityIndexer.ModKeyFactory.createKey','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add'
'org.apache.ivory.messaging.EntityInstanceMessage.getFeedPaths','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FileSystem.delete'
'org.apache.ivory.resource.EntityManagerJerseyTest.testProcessDeleteAndSchedule','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.resource.EntityManagerJerseyTest.createTestData','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.setOwner org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FsShell.<init> org.apache.hadoop.fs.FsShell.run'
'meetup.beeno.EntityService.get','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty'
'meetup.beeno.EntityService.createFromRow','org.apache.hadoop.hbase.client.Result.isEmpty'
'meetup.beeno.EntityService.populate','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getColumn org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue'
'meetup.beeno.EntityService.delete','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'meetup.beeno.EntityService.deleteProperty','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete'
'meetup.beeno.EntityService.deleteMapProperty','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete'
'meetup.beeno.EntityService.processUpdates','org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.getTableName org.apache.hadoop.hbase.util.Bytes.toString'
'meetup.beeno.EntityService.getUpdateForEntity','org.apache.hadoop.hbase.client.Put.<init>'
'meetup.beeno.EntityService.setUpdateField','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.apache.ivory.entity.EntityUtil.getStagingPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.entity.EntityUtil.getLogPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.stats.entropy.Entropy.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.stats.entropy.Entropy.prepareArguments','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.stats.entropy.Entropy.groupAndCount','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'org.apache.mahout.math.stats.entropy.Entropy.calculateEntropy','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'backtype.hadoop.EnumDeserializer.deserialize','org.apache.hadoop.io.WritableUtils.readVInt'
'backtype.hadoop.EnumSerializer.serialize','org.apache.hadoop.io.WritableUtils.writeVInt'
'com.urbanairship.datacube.ErrorHandlingTest.test','org.apache.hadoop.hbase.client.HTablePool.<init>'
'org.apache.accumulo.server.test.TextMemoryUsageTest.addEntry','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.InMemoryMapMemoryUsageTest.init','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.InMemoryMapMemoryUsageTest.addEntry','org.apache.hadoop.io.Text.set'
'org.apache.accumulo.server.test.MutationMemoryUsageTest.init','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.MutationMemoryUsageTest.addEntry','org.apache.hadoop.io.Text.set'
'org.apache.mahout.ga.watchmaker.EvalMapperTest.testMap','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.get'
'org.lwes.hadoop.hive.EventSerDe.initialize','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo'
'org.lwes.hadoop.hive.EventSerDe.deserialize','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.SerDeException.<init>'
'org.lwes.hadoop.hive.EventSerDe.deserialize_column','org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.set org.apache.hadoop.hive.serde2.io.ShortWritable.<init> org.apache.hadoop.hive.serde2.io.ShortWritable.set org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.hive.serde2.io.DoubleWritable.<init> org.apache.hadoop.hive.serde2.io.DoubleWritable.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory'
'org.lwes.hadoop.hive.EventSerDe.serialize','org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.SerDeException.<init>'
'org.lwes.hadoop.hive.EventSerDe.serialize_column','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveWritableObject org.apache.hadoop.io.Text.toString org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory'
'com.taobao.adfs.distributed.example.ExampleData.ExampleData','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hama.examples.ExampleDriver.main','org.apache.hadoop.util.ProgramDriver.<init> org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.driver'
'org.commoncrawl.examples.ExampleMetadataStats.ExampleMetadataStatsMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.examples.ExampleMetadataStats.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'org.commoncrawl.examples.ExampleMetadataStats.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.isi.mavuno.app.util.ExamplesToSequenceFile.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.set'
'edu.isi.mavuno.app.util.ExamplesToSequenceFile.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.isi.mavuno.app.util.ExamplesToSequenceFile.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.pig.impl.streaming.ExecutableManager.addJobConfToEnvironment','org.apache.hadoop.conf.Configuration.iterator org.apache.hadoop.conf.Configuration.get'
'org.springframework.data.hadoop.mapreduce.ExecutionUtils.isLegacyJar','org.apache.hadoop.io.IOUtils.closeStream'
'org.springframework.data.hadoop.mapreduce.ExecutionUtils.detectBaseDir','org.apache.hadoop.conf.Configuration.get'
'org.springframework.data.hadoop.mapreduce.ExecutionUtils.unjar','org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'org.springframework.data.hadoop.mapreduce.ExecutionUtils.mainClass','org.apache.hadoop.io.IOUtils.closeStream'
'org.springframework.data.hadoop.mapreduce.ExecutionUtils.preventHadoopLeaks','org.apache.hadoop.mapred.Counters.<init>'
'org.springframework.data.hadoop.mapreduce.ExecutionUtils.shutdownFileSystem','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.close'
'org.springframework.data.hadoop.mapreduce.ExecutionUtils.earlyLeaseDaemonInit','org.apache.hadoop.conf.Configuration.getClassLoader org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.delete'
'com.inadco.hbl.math.pig.ExpAvgGet.exec','org.apache.hadoop.io.DataInputBuffer.reset'
'com.cloudera.sqoop.testutil.ExplicitSetMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance'
'co.nubetech.hiho.job.ExportDelimitedToDB.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'co.nubetech.hiho.job.ExportDelimitedToDB.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.bulkloader.collector.ExportFileSend.sendExportFile','org.apache.hadoop.conf.Configuration.<init>'
'com.asakusafw.bulkloader.collector.ExportFileSend.send','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.close'
'com.asakusafw.bulkloader.collector.ExportFileSend.isSystemFile','org.apache.hadoop.fs.Path.getName'
'org.apache.accumulo.server.test.randomwalk.shard.ExportIndex.visit','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileUtil.copy org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.sqoop.mapreduce.ExportJobBase.getFileType','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.sqoop.mapreduce.ExportJobBase.fromMagicNumber','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.sqoop.mapreduce.ExportJobBase.getInputPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.makeQualified'
'org.apache.sqoop.mapreduce.ExportJobBase.configureInputFormat','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath'
'org.apache.sqoop.mapreduce.ExportJobBase.configureMapper','org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass'
'org.apache.sqoop.mapreduce.ExportJobBase.configureNumTasks','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.mapreduce.ExportJobBase.runJob','org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counters.getGroup'
'org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob','org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.sqoop.mapreduce.ExportJobBase.runExport','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.cloudera.sqoop.testutil.ExportJobTestCase.runExport','org.apache.hadoop.util.StringUtils.stringifyException'
'co.nubetech.hiho.job.sf.ExportSalesForceJob.populateConfiguration','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.sf.ExportSalesForceJob.checkMandatoryConfs','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.job.sf.ExportSalesForceJob.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'co.nubetech.hiho.job.sf.ExportSalesForceJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'.ExportStressTest.createFile','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'.ExportStressTest.createData','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'.ExportStressTest.run','org.apache.hadoop.util.StringUtils.stringifyException'
'.ExportStressTest.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.master.tableOps.WriteExportFiles.isReady','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.tableOps.WriteExportFiles.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.master.tableOps.WriteExportFiles.exportTable','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.accumulo.server.master.tableOps.WriteExportFiles.createDistcpFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.isAbsolute org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'org.apache.accumulo.server.master.tableOps.WriteExportFiles.exportMetadata','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.tableOps.ExportTable.isReady','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.master.tableOps.ExportTable.undo','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'test.modelgen.table.model.ExportTempImportTarget11.setDuplicateFlg','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ExportTempImportTarget11.setTextdata1','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ExportTempTable.setTableName','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ExportTempTable.setExportTempName','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ExportTempTable.setDuplicateFlgName','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ExportTempTable.setTempTableStatus','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ExportTempTest02.setTextdata1','org.apache.hadoop.io.Text.modify'
'co.nubetech.hiho.job.ExportToFTPServer.populateConfiguration','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.ExportToFTPServer.checkMandatoryConfs','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.job.ExportToFTPServer.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'co.nubetech.hiho.job.ExportToFTPServer.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.hiho.job.ExportToMySQLDB.populateConfiguration','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.ExportToMySQLDB.checkMandatoryConfs','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.job.ExportToMySQLDB.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'co.nubetech.hiho.job.ExportToMySQLDB.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.hiho.job.ExportToOracleDb.populateConfiguration','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.ExportToOracleDb.checkMandatoryConfs','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.job.ExportToOracleDb.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init>'
'co.nubetech.hiho.job.ExportToOracleDb.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.hiho.job.ExportToOracleDb.getAlterTableDML','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileStatus.getPath'
'co.nubetech.hiho.job.ExportToOracleDb.runQuery','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'elephantdb.hadoop.Exporter.CompoundKey.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt'
'elephantdb.hadoop.Exporter.CompoundKey.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'elephantdb.hadoop.Exporter.ExporterMapper.map','org.apache.hadoop.mapred.OutputCollector<elephantdb.hadoop.Exporter.CompoundKey,elephantdb.hadoop.ElephantRecordWritable>.collect'
'elephantdb.hadoop.Exporter.ExporterReducer.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,elephantdb.hadoop.ElephantRecordWritable>.collect'
'elephantdb.hadoop.Exporter.ElephantPrimarySort.compare','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'elephantdb.hadoop.Exporter.ElephantSecondarySort.compare','org.apache.hadoop.io.WritableComparator.compareBytes'
'elephantdb.hadoop.Exporter.export','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.SequenceFileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setOutputValueGroupingComparator org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isSuccessful'
'com.lightboxtechnologies.spectrum.ExtentsExtractor.ExtentsExtractorMapper.map','org.apache.hadoop.io.LongWritable.set'
'com.lightboxtechnologies.spectrum.ExtentsExtractor.run','org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.tripadvisor.hadoop.ExternalHDFSChecksumGenerator.init','org.apache.hadoop.fs.FileSystem.get'
'com.tripadvisor.hadoop.ExternalHDFSChecksumGenerator.getLocalFilesystemHDFSStyleChecksum','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.util.DataChecksum.newDataChecksum org.apache.hadoop.util.DataChecksum.reset org.apache.hadoop.util.DataChecksum.update org.apache.hadoop.util.DataChecksum.writeValue org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.write org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.<init>'
'testjar.ExternalMapperReducer.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<testjar.ExternalWritable,org.apache.hadoop.io.IntWritable>.collect'
'testjar.ExternalMapperReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.IntWritable>.collect'
'com.lightboxtechnologies.spectrum.ExtractData.run','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.getName org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.<init> org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.setConf org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.doBulkLoad org.apache.hadoop.fs.FileSystem.delete'
'com.lightboxtechnologies.spectrum.ExtractData.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'ivory.ltr.ExtractFeatures.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'gov.llnl.ontology.mapreduce.stats.ExtractKeysMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.ExtractKeysMR.setupReducer','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'com.manning.hip.ch10.ExtractMovieUDF.evaluate','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.loadTypes','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.loadPatterns','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.map','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.loadDocumentText','org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.io.Text.append org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.io.Text.toString'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.getSpan','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.getChunkSpan','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.getOffset','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.getLength','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'edu.isi.mavuno.app.ie.ExtractRelations.MyMapper.getMatchingInstances','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'edu.isi.mavuno.app.ie.ExtractRelations.MyReducer.reduce','org.apache.hadoop.io.Text.toString'
'edu.isi.mavuno.app.ie.ExtractRelations.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.isi.mavuno.app.ie.ExtractRelations.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ivory.ptc.driver.ExtractSortedPseudoTestCollection.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.ptc.driver.ExtractSortedPseudoTestCollection.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'ivory.ptc.driver.ExtractSortedPseudoTestCollection.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'cc.ExtractVisibleText.main','org.apache.hadoop.util.ToolRunner.run'
'cc.ExtractVisibleText.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMaxMapTaskFailuresPercent org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'cc.ExtractVisibleText.ExtractVisibleTextMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter'
'edu.jhu.thrax.hadoop.jobs.ExtractionJob.getJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setMaxInputSplitSize org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput'
'extramuros.java.jobs.utils.ExtramurosJob.ExtramurosJob','org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.utils.ExtramurosJob.readFirstRecord','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.utils.ExtramurosJob.getAllChildrenFiles','org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileStatus.getPath'
'extramuros.java.jobs.utils.ExtramurosJob.composeInputPathString','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.utils.ExtramurosJob.cleanOutput','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'fr.insarennes.fafdti.cli.FAFBuildMode.main','org.apache.hadoop.fs.Path.<init>'
'com.inadco.hbl.client.impl.functions.FCustomFunc.saveState','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Writable.write org.apache.hadoop.io.DataOutputBuffer.close org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'com.inadco.hbl.client.impl.functions.FCustomFunc.readState','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.Writable.readFields'
'fr.insarennes.fafdti.builder.gram.FGram.FGram','org.apache.hadoop.io.Text.<init>'
'fr.insarennes.fafdti.builder.gram.FGram.cloneGram','org.apache.hadoop.io.Text.<init>'
'fr.insarennes.fafdti.builder.gram.FGram.query','org.apache.hadoop.io.Text.toString'
'fr.insarennes.fafdti.builder.gram.FGram.readFields','org.apache.hadoop.io.Text.<init>'
'fr.insarennes.fafdti.builder.gram.FGram.write','org.apache.hadoop.io.Text.toString'
'fr.insarennes.fafdti.builder.gram.FGram.compareTo','org.apache.hadoop.io.Text.compareTo'
'fr.insarennes.fafdti.builder.gram.FGram.fromString','org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.score.FMeasureScorer.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.fpm.pfpgrowth.FPGrowthRetailDataTest2.testSpecificCaseFromRetailDataMinSup500','org.apache.hadoop.mapred.OutputCollector<java.lang.String,java.util.List<org.apache.mahout.common.Pair<java.util.List<java.lang.String>,java.lang.Long>>>.<init>'
'org.apache.mahout.fpm.pfpgrowth.FPGrowthRetailDataTest.testSpecificCaseFromRetailDataMinSup500','org.apache.hadoop.mapred.OutputCollector<java.lang.String,java.util.List<org.apache.mahout.common.Pair<java.util.List<java.lang.String>,java.lang.Long>>>.<init>'
'org.apache.mahout.fpm.pfpgrowth.FPGrowthTest2.testMaxHeapFPGrowth','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.fpm.pfpgrowth.FPGrowthTest2.testMaxHeapFPGrowthData1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.fpm.pfpgrowth.FPGrowthTest2.testMaxHeapFPGrowthData2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.fpm.pfpgrowth.FPGrowthTest2.testNoNullPointerExceptionWhenReturnableFeaturesIsNull','org.apache.hadoop.mapred.OutputCollector<java.lang.String,java.util.List<org.apache.mahout.common.Pair<java.util.List<java.lang.String>,java.lang.Long>>>.<init>'
'com.cloudera.recordbreaker.analyzer.FSCrawler.addSingleFile','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir'
'com.cloudera.recordbreaker.analyzer.FSCrawler.recursiveCrawlBuildList','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.cloudera.recordbreaker.analyzer.FSCrawler.getStartNonblockingCrawl','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified'
'com.cloudera.recordbreaker.analyzer.FSCrawler.run','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'com.cloudera.hoop.fs.FSCreate.FSCreate','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.hoop.fs.FSCreate.execute','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.create'
'org.sleuthkit.hadoop.FSEntryTikaTextExtractor.runTask','org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.cloudera.hoop.fs.FSHomeDir.execute','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.toString'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.FSInfo.putDFSClient','org.apache.hadoop.hdfs.DFSClient.close'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.FSInfo.getDFSClient','org.apache.hadoop.hdfs.DFSClient.<init>'
'com.cloudera.hoop.fs.FSOpen.FSOpen','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.hoop.fs.FSOpen.execute','org.apache.hadoop.fs.FileSystem.open'
'edu.umd.cloud9.io.FSProperty.writeInt','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.io.FSProperty.writeLong','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.io.FSProperty.writeFloat','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeFloat org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.io.FSProperty.writeString','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeUTF org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.io.FSProperty.readInt','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close'
'edu.umd.cloud9.io.FSProperty.readLong','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.close'
'edu.umd.cloud9.io.FSProperty.readFloat','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readFloat org.apache.hadoop.fs.FSDataInputStream.close'
'edu.umd.cloud9.io.FSProperty.readString','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.close'
'edu.umd.cloud9.io.FSProperty.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'com.cloudera.hoop.fs.FSRename.FSRename','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.hoop.fs.FSRename.execute','org.apache.hadoop.fs.FileSystem.rename'
'com.cloudera.hoop.fs.FSSetOwner.FSSetOwner','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.hoop.fs.FSSetOwner.execute','org.apache.hadoop.fs.FileSystem.setOwner'
'com.cloudera.hoop.fs.FSSetReplication.FSSetReplication','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.hoop.fs.FSSetReplication.execute','org.apache.hadoop.fs.FileSystem.setReplication'
'com.cloudera.hoop.fs.FSSetTimes.FSSetTimes','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.hoop.fs.FSSetTimes.execute','org.apache.hadoop.fs.FileSystem.setTimes'
'com.cloudera.hoop.fs.FSUtils.getPermission','org.apache.hadoop.fs.permission.FsPermission.getDefault org.apache.hadoop.fs.permission.FsPermission.valueOf'
'com.cloudera.hoop.fs.FSUtils.convertPathToHoop','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'com.cloudera.hoop.fs.FSUtils.permissionToString','org.apache.hadoop.fs.permission.FsPermission.getUserAction org.apache.hadoop.fs.permission.FsPermission.getGroupAction org.apache.hadoop.fs.permission.FsPermission.getOtherAction'
'com.cloudera.hoop.fs.FSUtils.fileStatusToJSON','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileStatus.getAccessTime org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getBlockSize org.apache.hadoop.fs.FileStatus.getReplication'
'fr.insarennes.fafdti.FSUtils.FSUtils','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'fr.insarennes.fafdti.FSUtils.getSize','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'fr.insarennes.fafdti.FSUtils.getPartNonEmptyPath','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName'
'fr.insarennes.fafdti.FSUtils.readNonEmptyPartFirstLine','org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString'
'fr.insarennes.fafdti.FSUtils.getPartNonEmpty','org.apache.hadoop.fs.FileSystem.open'
'fr.insarennes.fafdti.FSUtils.deleteDir','org.apache.hadoop.fs.FileSystem.delete'
'com.linkedin.mr_kluj.FSUtils.spiderPath','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath'
'edu.jhu.thrax.hadoop.paraphrasing.FeatureCollectionReducer.setup','org.apache.hadoop.io.MapWritable.<init>'
'edu.jhu.thrax.hadoop.paraphrasing.FeatureCollectionReducer.reduce','org.apache.hadoop.io.MapWritable.clear org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.equals org.apache.hadoop.io.MapWritable.put'
'org.apache.mahout.classifier.bayes.mapreduce.common.FeaturePartitioner.getPartition','org.apache.hadoop.io.WritableComparator.hashBytes'
'edu.jhu.thrax.hadoop.tools.FeatureTool.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.submit'
'edu.jhu.thrax.hadoop.tools.FeatureTool.main','org.apache.hadoop.util.ToolRunner.run'
'pl.edu.icm.coansys.disambiguation.author.jobs.FeaturesExtractionMapper_Toy.map','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'org.apache.ivory.cleanup.FeedCleanupHandler.getLogPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.retention.FeedEvictor.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.util.ToolRunner.run'
'org.apache.ivory.retention.FeedEvictor.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.retention.FeedEvictor.logInstancePaths','org.apache.hadoop.fs.FileSystem.create'
'org.apache.ivory.retention.FeedEvictor.discoverInstanceToDelete','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.retention.FeedEvictor.findFilesForFeed','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus'
'org.apache.ivory.retention.FeedEvictor.getDate','org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.retention.FeedEvictor.deleteInstance','org.apache.hadoop.fs.FileSystem.delete'
'org.apache.ivory.retention.FeedEvictor.debug','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes'
'org.apache.nutch.parse.feed.FeedParser.getParse','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.parse.feed.FeedParser.setConf','org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.parse.feed.FeedParser.getParse','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.parse.feed.FeedParser.setConf','org.apache.hadoop.conf.Configuration.get'
'org.apache.ivory.messaging.FeedProducerTest.setup','org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.messaging.FeedProducerTest.tearDown','org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'org.apache.ivory.messaging.FeedProducerTest.testLogFile','org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes'
'org.apache.ivory.messaging.FeedProducerTest.testEmptyLogFile','org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes'
'org.apache.ivory.replication.FeedReplicator.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.ivory.replication.FeedReplicator.run','org.apache.hadoop.tools.DistCp.execute org.apache.hadoop.tools.DistCpOptions.getTargetPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.tools.DistCpOptions.getSourcePaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.replication.FeedReplicator.getDistCpOptions','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.tools.DistCpOptions.<init> org.apache.hadoop.tools.DistCpOptions.setSyncFolder org.apache.hadoop.tools.DistCpOptions.setBlocking org.apache.hadoop.tools.DistCpOptions.setMaxMaps'
'org.apache.ivory.replication.FeedReplicator.getPaths','org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.entity.parser.FeedUpdateTest.init','org.apache.hadoop.hdfs.MiniDFSCluster.<init>'
'com.nearinfinity.hbase.dsl.FetchRow.row','org.apache.hadoop.hbase.client.Result.getRow'
'com.nearinfinity.hbase.dsl.FetchRow.family','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addFamily'
'com.nearinfinity.hbase.dsl.FetchRow.col','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn'
'com.nearinfinity.hbase.dsl.FetchRow.fetch','org.apache.hadoop.hbase.client.Get.getRow org.apache.hadoop.hbase.util.Bytes.toString'
'com.nearinfinity.hbase.dsl.FetchRow.newGet','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.getFamilyMap org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.Get.addColumn'
'com.nearinfinity.hbase.dsl.FetchTest.fetch','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.nearinfinity.hbase.dsl.FetchTest.forEachColumn','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.apache.nutch.searcher.FetchedSegments.SegmentUpdater.run','org.apache.hadoop.fs.listStatus org.apache.hadoop.fs.exists org.apache.hadoop.mapred.Merger.Segment.close org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.Merger.Segment.<init>'
'org.apache.nutch.searcher.FetchedSegments.Segment.getReaders','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getReaders'
'org.apache.nutch.searcher.FetchedSegments.Segment.getEntry','org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getEntry'
'org.apache.nutch.searcher.FetchedSegments.FetchedSegments','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.Merger.Segment.<init>'
'org.apache.nutch.searcher.FetchedSegments.getContent','org.apache.hadoop.mapred.Merger.Segment.getContent'
'org.apache.nutch.searcher.FetchedSegments.getParseData','org.apache.hadoop.mapred.Merger.Segment.getParseData'
'org.apache.nutch.searcher.FetchedSegments.getFetchDate','org.apache.hadoop.mapred.Merger.Segment.getCrawlDatum'
'org.apache.nutch.searcher.FetchedSegments.getParseText','org.apache.hadoop.mapred.Merger.Segment.getParseText'
'org.apache.nutch.searcher.FetchedSegments.getSummary','org.apache.hadoop.mapred.Merger.Segment.getParseText'
'org.apache.nutch.searcher.FetchedSegments.getUrl','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.fetcher.Fetcher.InputFormat.getSplits','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init>'
'org.apache.nutch.fetcher.Fetcher.FetchItem.create','org.apache.hadoop.io.Text.toString'
'org.apache.nutch.fetcher.Fetcher.FetchItemQueues.FetchItemQueues','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.fetcher.Fetcher.QueueFeeder.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.next org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.next'
'org.apache.nutch.fetcher.Fetcher.FetcherThread.FetcherThread','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.fetcher.Fetcher.FetcherThread.run','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.nutch.fetcher.Fetcher.FetcherThread.handleRedirect','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.fetcher.Fetcher.FetcherThread.output','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.io.Text.equals org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.nutch.fetcher.Fetcher.reportStatus','org.apache.hadoop.mapred.Reporter.setStatus'
'org.apache.nutch.fetcher.Fetcher.configure','org.apache.hadoop.mapred.JobConf.get'
'org.apache.nutch.fetcher.Fetcher.isParsing','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.fetcher.Fetcher.isStoringContent','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.fetcher.Fetcher.run','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.fetcher.Fetcher.fetch','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.fetcher.Fetcher.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.fetcher.FetcherOutputFormat.checkOutputSpecs','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.JobConf.getNumReduceTasks org.apache.hadoop.mapred.InvalidJobConfException.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.nutch.fetcher.FetcherOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileOutputFormat.getOutputCompressionType org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.<init>'
'org.apache.nutch.fetcher.FetcherOutputFormat.write','org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse>.write'
'org.apache.nutch.fetcher.FetcherOutputFormat.close','org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse>.close'
'org.apache.nutch.fetcher.Fetcher.InputFormat.getSplits','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init>'
'org.apache.nutch.fetcher.Fetcher.FetchItem.create','org.apache.hadoop.io.Text.toString'
'org.apache.nutch.fetcher.Fetcher.FetchItemQueues.FetchItemQueues','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.fetcher.Fetcher.QueueFeeder.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.next org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.next'
'org.apache.nutch.fetcher.Fetcher.FetcherThread.FetcherThread','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.fetcher.Fetcher.FetcherThread.run','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.fetcher.Fetcher.FetcherThread.handleRedirect','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.fetcher.Fetcher.FetcherThread.output','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.nutch.fetcher.Fetcher.reportStatus','org.apache.hadoop.mapred.Reporter.setStatus'
'org.apache.nutch.fetcher.Fetcher.configure','org.apache.hadoop.mapred.JobConf.get'
'org.apache.nutch.fetcher.Fetcher.isParsing','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.fetcher.Fetcher.isStoringContent','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.fetcher.Fetcher.run','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.fetcher.Fetcher.fetch','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.fetcher.Fetcher.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.indexer.field.FieldFilters.FieldFilters','org.apache.hadoop.conf.Configuration.get'
'com.smartitengineering.cms.spi.impl.content.FieldsObjectConverter.rowsToObject','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'com.smartitengineering.cms.spi.impl.content.FieldsObjectConverter.putField','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.apache.nutch.protocol.file.File.getProtocolOutput','org.apache.hadoop.io.Text.toString'
'org.apache.nutch.protocol.file.File.main','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.protocol.file.File.setConf','org.apache.hadoop.conf.Configuration.getInt'
'com.manning.hip.ch5.FileCat.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes'
'backtype.hadoop.FileCopyInputFormat.FileCopySplit.write','org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString'
'backtype.hadoop.FileCopyInputFormat.FileCopySplit.readFields','org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString'
'backtype.hadoop.FileCopyInputFormat.FileCopyRecordReader.next','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'backtype.hadoop.FileCopyInputFormat.FileCopyRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'backtype.hadoop.FileCopyInputFormat.FileCopyRecordReader.createValue','org.apache.hadoop.io.Text.<init>'
'backtype.hadoop.FileCopyInputFormat.getSplits','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.fs.FileSystem.getContentSummary org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'.FileDecompressor.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.io.compress.CompressionCodecFactory.removeSuffix org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'.FileDecompressorTest.decompressesGzippedFile','org.apache.hadoop.fs.FileUtil.fullyDelete'
'.FileDecompressor.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.io.compress.CompressionCodecFactory.removeSuffix org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'com.asakusafw.testdriver.file.FileExporterRetriever.truncate','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.testdriver.file.FileExporterRetriever.createOutput','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.asakusafw.testdriver.file.FileExporterRetriever.createSource','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init>'
'com.asakusafw.testdriver.file.FileExporterRetriever.getOpposite','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.conf.Configurable.setConf'
'com.asakusafw.testdriver.file.FileExporterRetrieverTest.setUp','org.apache.hadoop.fs.FileSystem.get'
'com.asakusafw.testdriver.file.FileExporterRetrieverTest.tearDown','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.testdriver.file.FileExporterRetrieverTest.simple','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.asakusafw.testdriver.file.FileExporterRetrieverTest.sequenceFile','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.asakusafw.testdriver.file.FileExporterRetrieverTest.putTextRaw','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'com.asakusafw.testdriver.file.FileExporterRetrieverTest.putTextSequenceFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.FileIDHandler.get','org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.testdriver.file.FileImporterPreparatorTest.setUp','org.apache.hadoop.fs.FileSystem.get'
'com.asakusafw.testdriver.file.FileImporterPreparatorTest.tearDown','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.testdriver.file.FileImporterPreparatorTest.simple','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.testdriver.file.FileImporterPreparatorTest.sequenceFile','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'com.asakusafw.testdriver.file.FileImporterPreparatorTest.loadResult','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.ga.watchmaker.cd.FileInfosDatasetTest.testRanges','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified'
'com.asakusafw.testdriver.file.FileInputFormatDriver.FileInputFormatDriver','org.apache.hadoop.mapreduce.lib.input.FileInputFormat<?,com.asakusafw.testdriver.file.V>.getClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat<?,com.asakusafw.testdriver.file.V>.getClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat<?,com.asakusafw.testdriver.file.V>.getSplits'
'com.asakusafw.testdriver.file.FileInputFormatDriver.getNext','org.apache.hadoop.mapreduce.RecordReader<?,com.asakusafw.testdriver.file.V>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<?,com.asakusafw.testdriver.file.V>.getCurrentValue'
'com.asakusafw.testdriver.file.FileInputFormatDriver.disposeCurrent','org.apache.hadoop.mapreduce.RecordReader<?,com.asakusafw.testdriver.file.V>.close'
'com.asakusafw.testdriver.file.FileInputFormatDriver.prepareNext','org.apache.hadoop.mapreduce.lib.input.FileInputFormat<?,com.asakusafw.testdriver.file.V>.createRecordReader org.apache.hadoop.mapreduce.RecordReader<?,com.asakusafw.testdriver.file.V>.initialize'
'com.asakusafw.testdriver.file.FileInputFormatDriver.close','org.apache.hadoop.mapreduce.RecordReader<?,com.asakusafw.testdriver.file.V>.close'
'.FileInputFormatTest.directoryWithSubdirectory','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.getInputFormat'
'.FileInputFormatTest.directoryWithSubdirectoryUsingGlob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.getInputFormat'
'.FileInputFormatTest.inputPathProperty','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.get'
'com.asakusafw.windgate.hadoopfs.ssh.FileList.createFileStatus','org.apache.hadoop.fs.FileStatus.<init>'
'com.asakusafw.windgate.hadoopfs.ssh.FileList.Reader.restoreExtra','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.fs.FileStatus.readFields'
'com.asakusafw.windgate.hadoopfs.ssh.FileList.Writer.openNext','org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.windgate.hadoopfs.ssh.FileList.Writer.createEntryFromStatus','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.fs.FileStatus.write org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.apache.accumulo.server.tabletserver.FileManager.reserveReaders','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.asakusafw.bulkloader.common.FileNameUtil.createPaths','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.<init>'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.FileOutputCommitterContainer','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.abortTask','org.apache.hadoop.mapred.HCatMapRedUtil.createTaskAttemptContext'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.commitTask','org.apache.hadoop.mapred.HCatMapRedUtil.createTaskAttemptContext'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.needsTaskCommit','org.apache.hadoop.mapred.HCatMapRedUtil.createTaskAttemptContext'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.setupJob','org.apache.hadoop.mapred.HCatMapRedUtil.createJobContext'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.setupTask','org.apache.hadoop.mapred.HCatMapRedUtil.createTaskAttemptContext'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.abortJob','org.apache.hadoop.mapred.HCatMapRedUtil.createJobContext org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getOutputCommitter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.getOutputDirMarking','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getOutputCommitter org.apache.hadoop.mapred.HCatMapRedUtil.createJobContext org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.create'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.getPartitionRootLocation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.toString'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.constructPartition','org.apache.hadoop.hive.metastore.api.Partition.<init> org.apache.hadoop.hive.ql.metadata.Table.getDbName org.apache.hadoop.hive.metastore.api.Partition.setDbName org.apache.hadoop.hive.ql.metadata.Table.getTableName org.apache.hadoop.hive.metastore.api.Partition.setTableName org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init> org.apache.hadoop.hive.metastore.api.Partition.setSd org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.hive.metastore.api.Partition.setValues org.apache.hadoop.hive.metastore.api.Partition.setParameters org.apache.hadoop.hive.ql.metadata.Table.getProperty org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.hive.metastore.api.FieldSchema.getName org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.fs.Path.toString'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.applyGroupAndPerms','org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.setOwner org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.setOwner'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.getFinalDynamicPartitionDestination','org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.hive.metastore.api.FieldSchema.getName org.apache.hadoop.fs.Path.toString'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.constructPartialPartPath','org.apache.hadoop.hive.common.FileUtils.escapePathName org.apache.hadoop.hive.common.FileUtils.escapePathName org.apache.hadoop.fs.Path.<init>'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.updateTableSchema','org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.ql.metadata.Table.getDbName org.apache.hadoop.hive.ql.metadata.Table.getTableName org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.moveTaskOutputs','org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.getFinalPath','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.discoverPartitions','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.hive.metastore.Warehouse.makeSpecFromName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapred.HCatMapRedUtil.createTaskAttemptContext org.apache.hadoop.mapred.HCatMapRedUtil.createJobContext org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.registerPartitions','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.hive.ql.metadata.Table.<init> org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.ql.metadata.Table.getParameters org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.Path.toString org.apache.hadoop.hive.ql.metadata.Table.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.hive.ql.metadata.Table.getTableName org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.hive.ql.metadata.Table.getTableName org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.hive.metastore.api.Partition.getValues org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartition'
'org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.cancelDelegationTokens','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTokenStrForm org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.hive.metastore.HiveMetaStoreClient.cancelDelegationToken'
'org.apache.hcatalog.mapreduce.FileOutputFormatContainer.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.hcatalog.mapreduce.FileOutputFormatContainer.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.io.NullWritable.getName org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.hive.serde2.SerDe.getSerializedClass org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.FileOutputFormat.getUniqueName'
'org.apache.hcatalog.mapreduce.FileOutputFormatContainer.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.hive.ql.metadata.Table.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.hcatalog.mapreduce.FileOutputFormatContainer.getOutputCommitter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getOutputCommitter'
'org.apache.hcatalog.mapreduce.FileOutputFormatContainer.handleDuplicatePublish','org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionNames org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.hive.ql.metadata.Table.getDbName org.apache.hadoop.hive.ql.metadata.Table.getTableName'
'org.apache.hcatalog.mapreduce.FileOutputFormatContainer.getPartitionValueList','org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.hive.ql.metadata.Table.getTableName org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.hive.metastore.api.FieldSchema.getName org.apache.hadoop.hive.metastore.api.FieldSchema.getName org.apache.hadoop.hive.ql.metadata.Table.getTableName'
'org.apache.hcatalog.mapreduce.FileOutputFormatContainer.setWorkOutputPath','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getWorkPath'
'org.freeeed.main.FileProcessor.emitAsMap','org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.freeeed.main.FileProcessor.createMapWritable','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.MapWritable.put'
'com.taobao.adfs.file.FileRepository.FileRepository','org.apache.hadoop.conf.Configuration.getInt'
'org.archive.util.binsearch.FileSearchTool.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.archive.util.binsearch.FileSearchTool.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.twitter.twadoop.mapreduce.input.FileSetInputSplit.FileSetInputSplit','org.apache.hadoop.conf.Configuration.<init>'
'com.twitter.twadoop.mapreduce.input.FileSetInputSplit.getLocations','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.getFileBlockLocations org.apache.hadoop.fs.BlockLocation.getHosts'
'com.twitter.twadoop.mapreduce.input.FileSetInputSplit.write','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.write'
'com.twitter.twadoop.mapreduce.input.FileSetInputSplit.readFields','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.readFields'
'com.twitter.twadoop.mapreduce.input.FileSetRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.crunch.io.impl.FileSourceImpl.configureSource','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.crunch.io.impl.FileSourceImpl.equals','org.apache.hadoop.fs.Path.equals'
'org.gora.query.impl.FileSplitPartitionQuery.FileSplitPartitionQuery','org.apache.hadoop.mapreduce.lib.input.FileSplit.getLocations'
'org.gora.query.impl.FileSplitPartitionQuery.getLength','org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'org.gora.query.impl.FileSplitPartitionQuery.getStart','org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart'
'org.gora.query.impl.FileSplitPartitionQuery.write','org.apache.hadoop.mapreduce.lib.input.FileSplit.write'
'org.gora.query.impl.FileSplitPartitionQuery.readFields','org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.readFields'
'org.gora.query.impl.FileSplitPartitionQuery.equals','org.apache.hadoop.mapreduce.lib.input.FileSplit.equals'
'com.datasalt.pangool.tuplemr.mapred.lib.input.FileSplit.write','org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.writeString'
'com.datasalt.pangool.tuplemr.mapred.lib.input.FileSplit.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.fs.Path.<init>'
'com.splunk.shuttl.archiver.filesystem.FileStatusBackedList.get','org.apache.hadoop.fs.FileStatus.getPath'
'com.splunk.shuttl.archiver.filesystem.FileStatusBackedListTest.get_correctIndex_correctItem','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init>'
'co.nubetech.hiho.mapreduce.lib.input.FileStreamRecordReader.close','org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.mapreduce.lib.input.FileStreamRecordReader.getCurrentKey','org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.mapreduce.lib.input.FileStreamRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open'
'com.cloudera.recordbreaker.analyzer.FileSummary.getPath','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.recordbreaker.analyzer.FileSummaryData.FileSummaryData','org.apache.hadoop.fs.permission.FsPermission.valueOf'
'com.ning.metrics.serialization.hadoop.FileSystemAccess.close','org.apache.hadoop.fs.FileSystem.close'
'com.ning.metrics.serialization.hadoop.FileSystemAccess.getFileSystemSafe','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.ning.metrics.action.binder.modules.FileSystemAccessProvider.FileSystemAccessProvider','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setStrings'
'.FileSystemCat.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.FileSystemDirectory','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.create','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.getPath'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.listAll','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.fileExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.fileLength','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.deleteFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.renameFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.createOutput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.openInput','org.apache.hadoop.fs.Path.<init>'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.toString','org.apache.hadoop.fs.Path.<init>'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.FileSystemIndexInput.Descriptor.Descriptor','org.apache.hadoop.fs.FileSystem.open'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.FileSystemIndexInput.FileSystemIndexInput','org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.FileSystemIndexOutput.FileSystemIndexOutput','org.apache.hadoop.fs.FileSystem.create'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.FileSystemIndexOutput.flushBuffer','org.apache.hadoop.fs.FSDataOutputStream.write'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.FileSystemIndexOutput.close','org.apache.hadoop.fs.FSDataOutputStream.close'
'com.senseidb.indexing.hadoop.reduce.FileSystemDirectory.FileSystemIndexOutput.length','org.apache.hadoop.fs.FSDataOutputStream.getPos'
'org.springframework.data.hadoop.fs.FileSystemFactoryBean.afterPropertiesSet','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getDefaultUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'org.springframework.data.hadoop.fs.FileSystemFactoryBean.destroy','org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.closeAll'
'org.springframework.data.hadoop.fs.FileSystemFactoryBean.getObjectType','org.apache.hadoop.fs.FileSystem.getClass'
'.FileSystemGlobTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'.FileSystemGlobTest.tearDown','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'.FileSystemGlobTest.glob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths'
'.FileSystemGlobTest.paths','org.apache.hadoop.fs.Path.<init>'
'com.splunk.shuttl.testutil.FileSystemUtils.getLocalFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'com.splunk.shuttl.testutil.FileSystemUtils.getRemoteFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.crunch.io.impl.FileTargetImpl.configureForMapReduce','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.output.CrunchMultipleOutputs.addNamedOutput'
'org.apache.crunch.io.impl.FileTargetImpl.equals','org.apache.hadoop.fs.Path.equals'
'org.apache.accumulo.core.file.FileUtil.createTmpDir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile'
'org.apache.accumulo.core.file.FileUtil.estimatePercentageLTE','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.file.FileUtil.findMidPoint','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.file.FileUtil.cleanupIndexOp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.accumulo.core.file.FileUtil.estimateSizes','org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.core.file.FileUtil.getFileSystem','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'org.apache.giraph.utils.FileUtils.deletePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.godhuli.rhipe.FileUtils.FileUtils','org.apache.hadoop.fs.FsShell.<init>'
'org.godhuli.rhipe.FileUtils.copyFromLocalFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'org.godhuli.rhipe.FileUtils.copyToLocal','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileUtil.createLocalTempFile org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileUtil.copy org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.godhuli.rhipe.FileUtils.makeFolderToDelete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.deleteOnExit'
'org.godhuli.rhipe.FileUtils.copyMain','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName'
'org.godhuli.rhipe.FileUtils.ls__','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getPath'
'org.godhuli.rhipe.FileUtils.delete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.isDirectory org.apache.hadoop.fs.Trash.<init> org.apache.hadoop.fs.Trash.moveToTrash org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists'
'org.godhuli.rhipe.FileUtils.DelayedExceptionThrowing.globAndProcess','org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths'
'org.godhuli.rhipe.FileUtils.mapredopts','org.apache.hadoop.conf.Configuration.iterator'
'org.godhuli.rhipe.FileUtils.sequence2binary','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.WritableUtils.writeVInt'
'org.godhuli.rhipe.FileUtils.hdfsrename','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.godhuli.rhipe.FileUtils.getKeys','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.io.WritableUtils.writeVInt'
'org.godhuli.rhipe.FileUtils.getstatus','org.apache.hadoop.mapred.JobID.forName'
'org.godhuli.rhipe.FileUtils.killjob','org.apache.hadoop.mapred.JobID.forName'
'org.godhuli.rhipe.FileUtils.joinjob','org.apache.hadoop.mapred.JobID.forName'
'org.godhuli.rhipe.FileUtils.main','org.apache.hadoop.conf.Configuration.<init>'
'com.cloudera.recordbreaker.fisheye.FilesPage.FileListing.FileListing','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init>'
'filters.FilterListExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.BinaryComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.BinaryComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.filter.RegexStringComparator.<init> org.apache.hadoop.hbase.filter.QualifierFilter.<init> org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'org.apache.accumulo.core.iterators.user.FilterTest.test1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.test1neg','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.testDeepCopy','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.test2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.test2a','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.test3','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.test4','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.test5','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.testTimestampFilter','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.FilterTest.testDeletes','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'cc.FilterTextHtml.main','org.apache.hadoop.util.ToolRunner.run'
'cc.FilterTextHtml.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMaxMapTaskFailuresPercent org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'cc.FilterTextHtml.FilterTextHtmlMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.mapred.Reporter.getCounter'
'org.apache.accumulo.core.bloomfilter.Filter.Filter','org.apache.hadoop.util.bloom.HashFunction.<init>'
'org.apache.accumulo.core.bloomfilter.Filter.readFields','org.apache.hadoop.util.bloom.HashFunction.<init>'
'org.apache.ivory.replication.FilteredCopyListing.FilteredCopyListing','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.ivory.replication.FilteredCopyListing.shouldCopy','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.replication.FilteredCopyListingTest.mkdirs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.io.IOUtils.cleanup'
'org.apache.ivory.replication.FilteredCopyListingTest.rmdirs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.IOUtils.cleanup'
'org.apache.ivory.replication.FilteredCopyListingTest.touchFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.cleanup'
'org.apache.ivory.replication.FilteredCopyListingTest.recordInExpectedValues','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.tools.util.DistCpUtils.getRelativePath'
'org.apache.ivory.replication.FilteredCopyListingTest.testRunNoPattern','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.tools.DistCpOptions.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.ivory.replication.FilteredCopyListingTest.testRunStarPattern','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.tools.DistCpOptions.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.ivory.replication.FilteredCopyListingTest.testRunQuestionPattern','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.tools.DistCpOptions.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.ivory.replication.FilteredCopyListingTest.testRunRangePattern','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.tools.DistCpOptions.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.ivory.replication.FilteredCopyListingTest.testRunSpecificPattern','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.tools.DistCpOptions.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.ivory.replication.FilteredCopyListingTest.testRunListPattern','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.tools.DistCpOptions.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.ivory.replication.FilteredCopyListingTest.verifyContents','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Text.toString'
'com.inadco.hbl.client.impl.scanner.FilteringScanSpecScanner.FilteringScanSpecScanner','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.HTableInterface.close'
'com.inadco.hbl.client.impl.scanner.FilteringScanSpecScanner.fetchNextRawResult','org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getColumnLatest org.apache.hadoop.hbase.KeyValue.getBuffer org.apache.hadoop.hbase.KeyValue.getValueOffset org.apache.hadoop.hbase.KeyValue.getValueLength'
'de.tuberlin.dima.aim.exercises.one.FilteringWordCount.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.nearinfinity.hadoop.patent.FindCitingPatents.MapClass.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'com.nearinfinity.hadoop.patent.FindCitingPatents.Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.nearinfinity.hadoop.patent.FindCitingPatents.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.nearinfinity.hadoop.patent.FindCitingPatents.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.pagerank.FindMaxPageRankNodes.MyMapper.cleanup','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.pagerank.FindMaxPageRankNodes.MyReducer.reduce','org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.example.pagerank.FindMaxPageRankNodes.MyReducer.cleanup','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.pagerank.FindMaxPageRankNodes.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.pagerank.FindMaxPageRankNodes.run','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.pagerank.FindMaxPageRankNodes.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.bfs.FindNodeAtDistance.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.bfs.FindNodeAtDistance.main','org.apache.hadoop.util.ToolRunner.run'
'ivory.lsh.bitext.FindParallelSentencePairs.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.lsh.bitext.FindParallelSentencePairs.MyMapper.loadPairs','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.lsh.bitext.FindParallelSentencePairs.MyMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfInts,ivory.lsh.data.WikiDocInfo>.collect'
'ivory.lsh.bitext.FindParallelSentencePairs.MyReducer.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getFloat'
'ivory.lsh.bitext.FindParallelSentencePairs.MyReducer.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'ivory.lsh.bitext.FindParallelSentencePairs.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapred.FileInputFormat.addInputPaths org.apache.hadoop.mapred.FileInputFormat.addInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'ivory.lsh.bitext.FindParallelSentencePairs.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.bfs.FindReachableNodes.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.bfs.FindReachableNodes.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.metanalysis.FindTablet.findContainingTablets','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.FirstEntryInRowIterator.consume','org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.core.iterators.FirstEntryInRowTest.nk','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.cloudata.examples.first.FirstMapReduce.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMaxReduceAttempts org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.cloudata.examples.first.FirstMapReduce.FirstMapReduceMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.cloudata.examples.first.FirstMapReduce.FirstMapReduceReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.livingsocial.hive.udtf.FirstN.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector'
'org.apache.jena.tdbloader4.FirstReducer.setup','org.apache.hadoop.io.Text.<init>'
'org.apache.jena.tdbloader4.FirstReducer.reduce','org.apache.hadoop.io.Text.toString'
'org.apache.jena.tdbloader4.FirstReducer.cleanup','org.apache.hadoop.io.LongWritable.<init>'
'com.cloudera.recordbreaker.fisheye.FishEye.getTopDir','org.apache.hadoop.fs.Path.toString'
'st.happy_camper.hbase.coprocessors.fizzbuzz.FizzBuzzRegionObserver.start','org.apache.hadoop.hbase.CoprocessorEnvironment.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.<init>'
'st.happy_camper.hbase.coprocessors.fizzbuzz.FizzBuzzRegionObserver.postPut','org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.getEnvironment org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.getEnvironment org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.client.Put.get org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTableInterface.close'
'st.happy_camper.hbase.coprocessors.fizzbuzz.FizzBuzzRegionObserver.newPut','org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.KeyValue.getRowLength org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.KeyValue.getFamilyLength org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.KeyValue.getQualifierLength org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.client.Put.add'
'st.happy_camper.hbase.coprocessors.fizzbuzz.FizzBuzzRegionObserverTest.setUpBeforeClass','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster'
'st.happy_camper.hbase.coprocessors.fizzbuzz.FizzBuzzRegionObserverTest.tearDownAfterClass','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'st.happy_camper.hbase.coprocessors.fizzbuzz.FizzBuzzRegionObserverTest.setUp','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin'
'st.happy_camper.hbase.coprocessors.fizzbuzz.FizzBuzzRegionObserverTest.tearDown','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.HBaseTestingUtility.deleteTable'
'st.happy_camper.hbase.coprocessors.fizzbuzz.FizzBuzzRegionObserverTest.testPostPut','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addCoprocessor org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'org.commoncrawl.util.FlexBuffer.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.util.FlexBuffer.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.util.shared.FlexBuffer.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.util.shared.FlexBuffer.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.hackreduce.mappers.FlightMapper.configureJob','org.apache.hadoop.mapreduce.Job.setInputFormatClass'
'org.lilyproject.hbaseindex.FloatIndexFieldDefinition.toBytes','org.apache.hadoop.hbase.util.Bytes.putFloat'
'hadoop.FloatIntervalInputFormat.getSplits','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.util.LineReader.close'
'hadoop.FloatIntervalInputFormat.IntervalRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'hadoop.FloatIntervalInputFormat.IntervalRecordReader.nextKeyValue','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'hadoop.FloatIntervalInputFormat.IntervalRecordReader.getCurrentValue','org.apache.hadoop.io.Text.toString'
'hadoop.FloatIntervalInputFormat.IntervalRecordReader.close','org.apache.hadoop.util.LineReader.close'
'hadoop.FloatIntervalOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'org.apache.giraph.aggregators.FloatMaxAggregator.aggregate','org.apache.hadoop.io.FloatWritable.get'
'org.apache.giraph.aggregators.FloatMaxAggregator.createInitialValue','org.apache.hadoop.io.FloatWritable.<init>'
'org.goldenorb.types.message.FloatMessage.FloatMessage','org.apache.hadoop.io.FloatWritable.<init>'
'org.goldenorb.types.message.FloatMessage.get','org.apache.hadoop.io.FloatWritable.get'
'org.goldenorb.types.message.FloatMessage.set','org.apache.hadoop.io.FloatWritable.set'
'org.apache.giraph.aggregators.FloatProductAggregator.aggregate','org.apache.hadoop.io.FloatWritable.get'
'org.apache.giraph.aggregators.FloatProductAggregator.createInitialValue','org.apache.hadoop.io.FloatWritable.<init>'
'co.nubetech.apache.hadoop.FloatSplitter.split','org.apache.hadoop.conf.Configuration.getInt'
'cascading.flow.hadoop.FlowMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getRaw org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'cascading.flow.hadoop.FlowReducer.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getRaw org.apache.hadoop.mapred.JobConf.get'
'com.cloudera.flume.agent.FlumeNode.tryKerberosLogin','org.apache.hadoop.util.VersionInfo.getVersion org.apache.hadoop.security.UserGroupInformation.getUserName'
'test.modelgen.model.Foo.setDetailGroupId','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setDetailType','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setDetailSenderId','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setDetailReceiverId','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setDetailTestType','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setDetailStatus','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setDeleteFlg','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setPurchaseNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setPurchaseType','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setTradeType','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setTradeNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setStoreCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setBuyerCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setSalesTypeCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setSellerCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setTenantCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setShipmentStoreCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setShipmentSalesTypeCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setDeductionCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setAccountCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setOwnershipFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setCutoffFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setPayoutFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.model.Foo.setDisposeNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDetailGroupId','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDetailType','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDetailSenderId','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDetailReceiverId','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDetailTestType','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDetailStatus','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDeleteFlg','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setPurchaseNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setPurchaseType','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setTradeType','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setTradeNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setStoreCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setBuyerCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setSalesTypeCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setSellerCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setTenantCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setShipmentStoreCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setShipmentSalesTypeCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDeductionCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setAccountCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setOwnershipFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setCutoffFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setPayoutFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.dummy.model.Foo.setDisposeNo','org.apache.hadoop.io.Text.modify'
'org.apache.jena.tdbloader4.FourthDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.jena.tdbloader4.FourthDriver.main','org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.rpc.base.internal.Frame.Decoder.getNextFrame','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.FreeGenerator.FG.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.tools.FreeGenerator.FG.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.tools.Generator.SelectorEntry>.collect'
'org.apache.nutch.tools.FreeGenerator.FG.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.collect'
'org.apache.nutch.tools.FreeGenerator.run','org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.getNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.FreeGenerator.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.df.tools.Frequencies.runTool','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.tools.Frequencies.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ScoreFriends.FriendScoreDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.crunch.io.From.formattedFile','org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.From.avroFile','org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.From.hbaseTable','org.apache.hadoop.hbase.client.Scan.<init>'
'org.apache.crunch.io.From.sequenceFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.From.textFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.example.email.FromEmailToDictionaryMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.oozie.action.hadoop.FsActionExecutor.getPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.FsActionExecutor.validatePath','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.hadoop.FsActionExecutor.resolveToFullPath','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.isAbsolute org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.hadoop.FsActionExecutor.validateSameNN','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.hadoop.FsActionExecutor.doOperations','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.getParent'
'org.apache.oozie.action.hadoop.FsActionExecutor.getFileSystemFor','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.hadoop.FsActionExecutor.mkdir','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.oozie.action.hadoop.FsActionExecutor.delete','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.hadoop.FsActionExecutor.move','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.rename'
'org.apache.oozie.action.hadoop.FsActionExecutor.chmod','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.setPermission'
'org.apache.oozie.action.hadoop.FsActionExecutor.touchz','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.oozie.action.hadoop.FsActionExecutor.createShortPermission','org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf'
'org.apache.oozie.action.hadoop.FsActionExecutor.end','org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.hadoop.FsActionExecutor.getRecoveryPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.FsELFunctions.getFileStatus','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_exists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_isDir','org.apache.hadoop.fs.FileStatus.isDir'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_fileSize','org.apache.hadoop.fs.FileStatus.getLen'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_dirSize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_blockSize','org.apache.hadoop.fs.FileStatus.getBlockSize'
'org.apache.oozie.action.hadoop.FsELFunctions.getFileStatus','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_exists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_isDir','org.apache.hadoop.fs.FileStatus.isDir'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_fileSize','org.apache.hadoop.fs.FileStatus.getLen'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_dirSize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen'
'org.apache.oozie.action.hadoop.FsELFunctions.fs_blockSize','org.apache.hadoop.fs.FileStatus.getBlockSize'
'com.lightboxtechnologies.spectrum.FsEntryHBaseCommon.createColSpec','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.lightboxtechnologies.spectrum.FsEntryHBaseCommon.colName','org.apache.hadoop.hbase.util.Bytes.toString'
'com.lightboxtechnologies.spectrum.FsEntryHBaseCommon.unmarshall','org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toString'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.FsEntryHBaseInputFormat','org.apache.hadoop.hbase.mapreduce.TableInputFormat.<init>'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.setupJob','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.convertScanToString','org.apache.hadoop.hbase.client.Scan.write org.apache.hadoop.hbase.util.Base64.encodeBytes'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.getConf','org.apache.hadoop.hbase.mapreduce.TableInputFormat.getConf'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.setConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf org.apache.hadoop.conf.Configuration.get'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.getSplits','org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.createRecordReader','org.apache.hadoop.hbase.mapreduce.TableInputFormat.createRecordReader'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.FsEntryRecordReader.close','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.close'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.FsEntryRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.getCurrentValue org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.FsEntryRecordReader.getCurrentValue','org.apache.hadoop.hbase.client.Result.getFamilyMap'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.FsEntryRecordReader.getProgress','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.getProgress'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.FsEntryRecordReader.initialize','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.initialize org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.FileSystem.get'
'com.lightboxtechnologies.spectrum.FsEntryHBaseInputFormat.FsEntryRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.getCurrentValue org.apache.hadoop.hbase.client.Result.getRow'
'org.gora.avro.mapreduce.FsInput.FsInput','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem'
'org.gora.avro.mapreduce.FsInput.read','org.apache.hadoop.fs.FSDataInputStream.read'
'org.gora.avro.mapreduce.FsInput.seek','org.apache.hadoop.fs.FSDataInputStream.seek'
'org.gora.avro.mapreduce.FsInput.tell','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.gora.avro.mapreduce.FsInput.close','org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.avro.mapred.FsInput.FsInput','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.avro.mapred.FsInput.read','org.apache.hadoop.fs.FSDataInputStream.read'
'org.apache.avro.mapred.FsInput.seek','org.apache.hadoop.fs.FSDataInputStream.seek'
'org.apache.avro.mapred.FsInput.tell','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.avro.mapred.FsInput.close','org.apache.hadoop.fs.FSDataInputStream.close'
'org.springframework.data.hadoop.fs.FsShell.FsShell','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Trash.<init>'
'org.springframework.data.hadoop.fs.FsShell.close','org.apache.hadoop.fs.FileSystem.close'
'org.springframework.data.hadoop.fs.FsShell.toString','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.ContentSummary.toString org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getReplication org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getPath'
'org.springframework.data.hadoop.fs.FsShell.cat','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths'
'org.springframework.data.hadoop.fs.FsShell.copyFromLocal','org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'org.springframework.data.hadoop.fs.FsShell.copyToLocal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.setVerifyChecksum org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileUtil.createLocalTempFile org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileUtil.copy org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile org.apache.hadoop.fs.ChecksumFileSystem.getRawFileSystem org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.springframework.data.hadoop.fs.FsShell.count','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.getContentSummary'
'org.springframework.data.hadoop.fs.FsShell.cp','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileUtil.copy'
'org.springframework.data.hadoop.fs.FsShell.du','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.getContentSummary org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.getContentSummary org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath'
'org.springframework.data.hadoop.fs.FsShell.expunge','org.apache.hadoop.fs.Trash.expunge org.apache.hadoop.fs.Trash.checkpoint'
'org.springframework.data.hadoop.fs.FsShell.getmerge','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileUtil.copyMerge org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileUtil.copyMerge'
'org.springframework.data.hadoop.fs.FsShell.ls','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir'
'org.springframework.data.hadoop.fs.FsShell.mkdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.mkdirs'
'org.springframework.data.hadoop.fs.FsShell.moveFromLocal','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.moveFromLocalFile'
'org.springframework.data.hadoop.fs.FsShell.mv','org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.isDir'
'org.springframework.data.hadoop.fs.FsShell.rm','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Trash.<init> org.apache.hadoop.fs.Trash.moveToTrash org.apache.hadoop.fs.FileSystem.delete'
'org.springframework.data.hadoop.fs.FsShell.setrep','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.getFileBlockLocations org.apache.hadoop.fs.BlockLocation.getHosts org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.setReplication org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.springframework.data.hadoop.fs.FsShell.test','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.springframework.data.hadoop.fs.FsShell.text','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readShort org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.readByte org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'org.springframework.data.hadoop.fs.FsShell.touchz','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.closeStream'
'org.springframework.data.hadoop.fs.FsShell.parseVarargs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.springframework.data.hadoop.fs.FsShell.getFS','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.hive.pdk.FunctionExtractor.main','org.apache.hadoop.hive.ql.exec.Description.name'
'.Hog.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text>.collect'
'.Hog.Reduce.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'mia.clustering.ch09.FuzzyKMeansClustering.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver.buildClusters','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver.clusterData','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansUtil.configureWithClusterInfo','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.io.Writable.getClass'
'com.digitalpebble.behemoth.gate.GATEAnnotationFilters.getFilters','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getStrings'
'com.digitalpebble.behemoth.gate.GATEMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,com.digitalpebble.behemoth.BehemothDocument>.collect'
'com.digitalpebble.behemoth.gate.GATEProcessorTest.setUp','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.digitalpebble.behemoth.gate.GATEXMLMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.apache.accumulo.server.test.GCLotsOfCandidatesTest.generateCandidates','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.bizo.hadoop.mapred.GDataRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'com.bizo.hadoop.mapred.GDataRecordReader.createValue','org.apache.hadoop.io.MapWritable.<init>'
'com.bizo.hadoop.mapred.GDataRecordReader.next','org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.MapWritable.clear org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put'
'com.bizo.hive.gdata.GDataStorageHandler.configureTableJobProperties','org.apache.hadoop.hive.ql.plan.TableDesc.getProperties'
'org.apache.pig.impl.builtin.GFCross.exec','org.apache.hadoop.conf.Configuration.get'
'hadoopGIS.GIS.toText','org.apache.hadoop.io.Text.<init>'
'hadoopGIS.GIS.getBytes','org.apache.hadoop.io.Text.getBytes'
'hadoopGIS.GIS.getLength','org.apache.hadoop.io.Text.getLength'
'hadoopGIS.GIS.update','org.apache.hadoop.io.Text.toString'
'hadoopGIS.GIS.write','org.apache.hadoop.io.Text.write'
'hadoopGIS.GIS.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'hadoopGIS.GISRecordReader.GISRecordReader','org.apache.hadoop.mapred.LineRecordReader.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'hadoopGIS.GISRecordReader.getProgress','org.apache.hadoop.mapred.LineRecordReader.getProgress'
'hadoopGIS.GISRecordReader.close','org.apache.hadoop.mapred.LineRecordReader.close'
'hadoopGIS.GISRecordReader.getPos','org.apache.hadoop.mapred.LineRecordReader.getPos'
'hadoopGIS.GISRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'hadoopGIS.GISRecordReader.next','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.LineRecordReader.next org.apache.hadoop.io.LongWritable.set'
'semvec.mahout.matrix.GLMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'semvec.mahout.matrix.GLMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.archive.hadoop.mapreduce.GZIPMembersLineInputFormat.getSplits','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.getSplits'
'org.archive.hadoop.mapreduce.GZIPMembersLineRecordReader.getProgress','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress'
'org.archive.hadoop.mapreduce.GZIPMembersLineRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.<init> org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.archive.hadoop.mapreduce.GZIPMembersLineRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.archive.server.GZRangeClientTool.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.flume.reporter.ganglia.GangliaSink.open','org.apache.hadoop.metrics.spi.Util.parse'
'edu.duke.starfish.profile.utils.GatherResults.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.toString'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.GenBlogPlatformUrlsStep.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.GenBlogPlatformUrlsStep.emitItem','org.apache.hadoop.io.BooleanWritable.set org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.apache.hadoop.io.BooleanWritable>.collect'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.GenBlogPlatformUrlsStep.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.GenBlogPlatformUrlsStep.runStep','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobClient.runJob'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.GenHomepageUrlsStep.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.GenHomepageUrlsStep.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.GenHomepageUrlsStep.runStep','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobClient.runJob'
'voldemort.store.readonly.benchmark.GenerateData.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'voldemort.store.readonly.benchmark.GenerateData.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobClient.runJob'
'voldemort.store.readonly.benchmark.GenerateData.GenerateDataMapper.map','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect'
'voldemort.store.readonly.benchmark.GenerateData.GenerateDataMapper.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.io.BytesWritable.<init>'
'edu.umd.cloud9.io.benchmark.GenerateRandomPairsOfInts.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'.GenerateRowNumberPartitioner.getPartition','org.apache.hadoop.io.Text.toString'
'.GenerateRowNumberReducer.configure','org.apache.hadoop.mapred.lib.MultipleOutputs.<init>'
'.GenerateRowNumberReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector.collect'
'.GenerateRowNumberReducer.close','org.apache.hadoop.mapred.lib.MultipleOutputs.close'
'org.apache.giraph.examples.GeneratedVertexReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.nutch.crawl.Generator.SelectorEntry.SelectorEntry','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.nutch.crawl.Generator.SelectorEntry.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.IntWritable.readFields'
'org.apache.nutch.crawl.Generator.SelectorEntry.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.IntWritable.write'
'org.apache.nutch.crawl.Generator.SelectorEntry.toString','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.toString'
'org.apache.nutch.crawl.Generator.Selector.configure','org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getNumReduceTasks org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.Partitioner<org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable>.configure org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapred.JobConf.getFloat org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.nutch.crawl.Generator.Selector.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator.SelectorEntry>.collect'
'org.apache.nutch.crawl.Generator.Selector.getPartition','org.apache.hadoop.mapred.Partitioner<org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable>.getPartition'
'org.apache.nutch.crawl.Generator.Selector.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator.SelectorEntry>.collect'
'org.apache.nutch.crawl.Generator.SelectorInverseMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.Generator.SelectorEntry>.collect'
'org.apache.nutch.crawl.Generator.PartitionReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.CrawlDatum>.collect'
'org.apache.nutch.crawl.Generator.HashComparator.compare','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.nutch.crawl.Generator.CrawlDbUpdater.configure','org.apache.hadoop.mapred.JobConf.getLong'
'org.apache.nutch.crawl.Generator.CrawlDbUpdater.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.CrawlDatum>.collect'
'org.apache.nutch.crawl.Generator.CrawlDbUpdater.reduce','org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.CrawlDatum>.collect'
'org.apache.nutch.crawl.Generator.generate','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.getNumMapTasks org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.Generator.partitionSegment','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.crawl.Generator.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.crawl.Generator.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.expreval.expr.compare.GenericCompare.validateArgsForCompareFilter','org.apache.hadoop.hbase.hbql.impl.InvalidServerFilterException.<init>'
'org.apache.expreval.expr.compare.GenericCompare.validateType','org.apache.hadoop.hbase.hbql.client.HBqlException.printStackTrace'
'org.apache.expreval.expr.compare.GenericCompare.GenericComparable.setValueInBytes','org.apache.hadoop.hbase.hbql.io.IO.getSerialization org.apache.hadoop.hbase.hbql.client.HBqlException.printStackTrace org.apache.hadoop.hbase.hbql.impl.Utils.logException org.apache.hadoop.hbase.hbql.client.HBqlException.getCause'
'org.apache.expreval.expr.compare.GenericCompare.GenericComparable.equalValues','org.apache.hadoop.hbase.util.Bytes.equals'
'co.nubetech.hiho.mapreduce.lib.db.GenericDBOutputFormat.setOutput','org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.expreval.expr.literal.GenericLiteral.getFilter','org.apache.hadoop.hbase.hbql.impl.InvalidServerFilterException.<init>'
'co.nubetech.hiho.mapred.avro.GenericRecordReader.GenericRecordReader','org.apache.hadoop.mapred.FileSplit.getPath'
'com.nexr.rhive.hive.udf.GenericUDFArrayToString.evaluate','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getList org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMap org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.ql.exec.TaskExecutionException.<init>'
'com.nexr.rhive.hive.udf.GenericUDFArrayToString.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init>'
'com.nexr.platform.hive.udf.GenericUDFDecode.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init>'
'com.nexr.platform.hive.udf.GenericUDFDecode.evaluate','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject'
'com.nexr.platform.hive.udf.GenericUDFLag.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardObjectInspector'
'com.nexr.platform.hive.udf.GenericUDFLag.evaluate','org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter.convert'
'com.nexr.platform.hive.udf.GenericUDFLnnvl.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter'
'com.nexr.platform.hive.udf.GenericUDFLnnvl.evaluate','org.apache.hadoop.io.BooleanWritable.set org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveWritableObject org.apache.hadoop.io.BooleanWritable.get org.apache.hadoop.io.BooleanWritable.set'
'com.nexr.platform.hive.udf.GenericUDFMin.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardObjectInspector'
'com.nexr.platform.hive.udf.GenericUDFMin.evaluate','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter.convert org.apache.hadoop.hive.serde2.io.DoubleWritable.get org.apache.hadoop.hive.serde2.io.DoubleWritable.set org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter.convert org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set'
'com.nexr.platform.hive.udf.GenericUDFTrunc.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init>'
'com.nexr.platform.hive.udf.GenericUDFTrunc.evaluate','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject org.apache.hadoop.io.Text.set'
'com.nexr.rhive.hive.udf.GenericUDTFExpand.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector'
'com.nexr.rhive.hive.udf.GenericUDTFExpand.getColumnWritable','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.hive.serde2.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init>'
'com.nexr.rhive.hive.udf.GenericUDTFExpand.process','org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.io.Text.set org.apache.hadoop.hive.serde2.io.DoubleWritable.set org.apache.hadoop.io.IntWritable.set org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'org.msgpack.hadoop.hive.udf.GenericUDTFMessagePackArray.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector'
'org.msgpack.hadoop.hive.udf.GenericUDTFMessagePackArray.process','org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveWritableObject'
'org.msgpack.hadoop.hive.udf.GenericUDTFMessagePackArray.setText','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'com.nexr.rhive.hive.udf.GenericUDTFUnFold.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector'
'com.nexr.rhive.hive.udf.GenericUDTFUnFold.getColumnWritable','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.hive.serde2.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init>'
'com.nexr.rhive.hive.udf.GenericUDTFUnFold.process','org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.hive.serde2.io.DoubleWritable.set org.apache.hadoop.io.IntWritable.set org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.twitter.elephantbird.pig.util.GenericWritableConverter.toWritable','org.apache.hadoop.io.DataInputBuffer.reset'
'ivory.regression.basic.Genomics05_Basic.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'com.manning.hip.ch10.GeolocUDF.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector'
'com.manning.hip.ch10.GeolocUDF.lookup','org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'client.GetExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.accumulo.core.util.shell.commands.GetSplitsCommand.execute','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.util.shell.commands.GetSplitsCommand.encode','org.apache.hadoop.io.Text.getLength'
'edu.isi.mavuno.score.GetTopResults.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.set'
'edu.isi.mavuno.score.GetTopResults.MyReducer.setup','org.apache.hadoop.conf.Configuration.get'
'edu.isi.mavuno.score.GetTopResults.MyReducer.reduce','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.set org.apache.hadoop.io.DoubleWritable.get'
'edu.isi.mavuno.score.GetTopResults.MyReducer.cleanup','org.apache.hadoop.io.Text.getLength'
'edu.isi.mavuno.score.GetTopResults.run','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.isi.mavuno.score.GetTopResults.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.giraph.graph.GiraphJob.run','org.apache.hadoop.ipc.Client.setPingInterval org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getJar org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.giraph.GiraphRunner.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.conf.Configuration.set'
'org.apache.giraph.GiraphRunner.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.giraph.examples.GlobalClusteringCoefficientVertex.compute','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.toString org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.GlobalClusteringCoefficientVertex.GlobalClusteringCoefficientContext.preSuperstep','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.GlobalClusteringCoefficientVertex.GlobalClusteringCoefficientVertexReader.getCurrentVertex','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.examples.GlobalClusteringCoefficientVertex.GlobalClusteringCoefficientVertexWriter.writeVertex','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.examples.GlobalClusteringCoefficientVertex.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.apache.giraph.examples.GlobalClusteringCoefficientVertex.main','org.apache.hadoop.util.ToolRunner.run'
'org.archive.hadoop.mapreduce.GlobalWaybackMergeMapper.setDailyLimit','org.apache.hadoop.conf.Configuration.setInt'
'org.archive.hadoop.mapreduce.GlobalWaybackMergeMapper.setConf','org.apache.hadoop.conf.Configuration.getInt'
'org.archive.hadoop.mapreduce.GlobalWaybackMergeMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.gora.hbase.GoraHBaseTestDriver.GoraHBaseTestDriver','org.apache.hadoop.hbase.HBaseTestingUtility.<init>'
'org.gora.hbase.GoraHBaseTestDriver.setUpClass','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster'
'org.gora.hbase.GoraHBaseTestDriver.tearDownClass','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'org.gora.hbase.GoraHBaseTestDriver.deleteAllTables','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'org.gora.hbase.GoraHBaseTestDriver.getConf','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration'
'org.apache.gora.hbase.GoraHBaseTestDriver.GoraHBaseTestDriver','org.apache.hadoop.hbase.HBaseTestingUtility.<init>'
'org.apache.gora.hbase.GoraHBaseTestDriver.setUpClass','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster'
'org.apache.gora.hbase.GoraHBaseTestDriver.tearDownClass','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'org.apache.gora.hbase.GoraHBaseTestDriver.deleteAllTables','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'org.apache.gora.hbase.GoraHBaseTestDriver.getConf','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration'
'org.gora.mapreduce.GoraMapReduceUtils.setIOSerializations','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.setStrings'
'org.gora.mapreduce.GoraMapReduceUtils.createJobContext','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.JobContext.<init> org.apache.hadoop.mapreduce.JobContext.<init>'
'org.apache.gora.mapreduce.GoraMapReduceUtils.setIOSerializations','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.setStrings'
'org.apache.gora.mapreduce.GoraMapReduceUtils.createJobContext','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.JobContext.<init> org.apache.hadoop.mapreduce.JobContext.<init>'
'org.apache.gora.mapreduce.GoraMapper.initMapperJob','org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass'
'org.gora.mapreduce.GoraOutputFormat.setOutputPath','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getUniqueFile org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath'
'org.gora.mapreduce.GoraOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapreduce.RecordWriter<org.gora.mapreduce.K,org.gora.mapreduce.T>.<init>'
'org.gora.mapreduce.GoraOutputFormat.setOutput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass'
'org.apache.gora.mapreduce.GoraOutputFormat.setOutputPath','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getUniqueFile org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath'
'org.apache.gora.mapreduce.GoraOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.gora.mapreduce.GoraOutputFormat.setOutput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass'
'org.gora.mapreduce.GoraReducer.initReducerJob','org.apache.hadoop.mapreduce.Job.setReducerClass'
'org.gora.sql.GoraSqlTestDriver.tearDownClass','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.gora.sql.GoraSqlTestDriver.tearDownClass','org.apache.hadoop.util.StringUtils.stringifyException'
'ivory.regression.sigir2011.Gov2_Cascade.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.cikm2010.Gov2_Desc_Indep.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.cikm2010.Gov2_Desc_Joint.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.basic.Gov2_NonPositional_Baselines.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.cikm2010.Gov2_Title_Indep.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.sigir2011.Gov2_VaryingTradeoff_Cascade.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.sigir2011.Gov2_VaryingTradeoff_FeaturePrune.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.mahout.vectorizer.collocations.llr.Gram.Gram','org.apache.hadoop.io.Text.encode'
'org.apache.mahout.vectorizer.collocations.llr.Gram.getString','org.apache.hadoop.io.Text.decode'
'org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparator.compare','org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.mahout.vectorizer.collocations.llr.Gram.Gram','org.apache.hadoop.io.Text.encode'
'org.apache.mahout.vectorizer.collocations.llr.Gram.getString','org.apache.hadoop.io.Text.decode'
'org.apache.hama.graph.GraphJobMessage.write','org.apache.hadoop.io.Writable.write org.apache.hadoop.io.Writable.write org.apache.hadoop.io.MapWritable.write org.apache.hadoop.io.Writable.write'
'org.apache.hama.graph.GraphJobMessage.readFields','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.MapWritable.readFields org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields'
'org.sleuthkit.hadoop.GrepCountMapper.map','org.apache.hadoop.io.LongWritable.set'
'org.sleuthkit.hadoop.GrepCountReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get'
'org.sleuthkit.hadoop.GrepCountReducer.cleanup','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'org.sleuthkit.hadoop.GrepJSONBuilder.buildReport','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'org.sleuthkit.hadoop.GrepJSONBuilder.writeFileToStream','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataInputStream.read'
'org.sleuthkit.hadoop.GrepReportGenerator.runPipeline','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.examples.Grep.GrepHandler.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.examples.Grep.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.examples.Grep.main','org.apache.hadoop.util.ToolRunner.run'
'org.gridgain.examples.cache.loaddata.dataloader.GridCacheDataLoaderHdfsExample.loadData','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists'
'org.gridgain.examples.cache.loaddata.dataloader.GridCacheDataLoaderHdfsExample.walk','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.gridgain.examples.cache.loaddata.dataloader.GridCacheDataLoaderHdfsExample.populateHdfs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.create'
'org.gridgain.examples.cache.loaddata.dataloader.GridCacheDataLoaderHdfsExample.cleanupHdfs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.gridgain.examples.cache.loaddata.dataloader.GridCacheDataLoaderHdfsExample.path','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.gridgain.examples.cache.loaddata.dataloader.GridCacheDataLoaderHdfsExample.HdfsWorker.call','org.apache.hadoop.fs.FileSystem.open'
'org.gridgain.examples.cache.store.hbase.GridCacheHBasePersonStore.prepareDb','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTablePool.<init>'
'org.gridgain.examples.cache.store.hbase.GridCacheHBasePersonStore.toBytes','org.apache.hadoop.hbase.util.Bytes.putLong org.apache.hadoop.hbase.util.Bytes.putLong'
'org.gridgain.examples.cache.store.hbase.GridCacheHBasePersonStore.fromBytes','org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toLong'
'org.gridgain.examples.cache.store.hbase.GridCacheHBasePersonStore.person','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.HColumnDescriptor.getName org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.HColumnDescriptor.getName org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.HColumnDescriptor.getName org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'org.gridgain.examples.cache.store.hbase.GridCacheHBasePersonStore.load','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty'
'org.gridgain.examples.cache.store.hbase.GridCacheHBasePersonStore.put','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.HColumnDescriptor.getName org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.HColumnDescriptor.getName org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.getName org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.put'
'org.gridgain.examples.cache.store.hbase.GridCacheHBasePersonStore.remove','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTableInterface.delete'
'org.gridgain.examples.cache.store.hbase.GridCacheHBasePersonStore.close','org.apache.hadoop.hbase.client.HTableInterface.close'
'org.apache.hama.bsp.GroomServer.BSPTasksMonitor.BSPTasksMonitor','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.hama.bsp.GroomServer.GroomServer','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.hama.bsp.GroomServer.initialize','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.net.DNS.getDefaultHost org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.ipc.Server.start org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.net.NetUtils.getServerAddress org.apache.hadoop.net.NetUtils.createSocketAddr org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.ipc.Server.start org.apache.hadoop.ipc.Server.getListenerAddress org.apache.hadoop.conf.Configuration.set org.apache.hadoop.ipc.RPC.waitForProxy org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getClass'
'org.apache.hama.bsp.GroomServer.checkLocalDirs','org.apache.hadoop.util.DiskChecker.checkDir org.apache.hadoop.util.DiskChecker.DiskErrorException.getMessage org.apache.hadoop.util.DiskChecker.DiskErrorException.<init>'
'org.apache.hama.bsp.GroomServer.getLocalDirs','org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.hama.bsp.GroomServer.deleteLocalFiles','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.GroomServer.offerService','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.hama.bsp.GroomServer.startNewTask','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.hama.bsp.GroomServer.localizeJob','org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.RunJar.unJar'
'org.apache.hama.bsp.GroomServer.launchTaskForJob','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.hama.bsp.GroomServer.getObliviousTasks','org.apache.hadoop.conf.Configuration.getLong'
'org.apache.hama.bsp.GroomServer.run','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.hama.bsp.GroomServer.close','org.apache.hadoop.ipc.Server.stop org.apache.hadoop.ipc.RPC.stopProxy org.apache.hadoop.ipc.Server.stop'
'org.apache.hama.bsp.GroomServer.TaskInProgress.localizeTask','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.Path.toString'
'org.apache.hama.bsp.GroomServer.BSPPeerChild.main','org.apache.hadoop.ipc.RPC.getProxy org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FSError.getMessage org.apache.hadoop.fs.FSError.printStackTrace org.apache.hadoop.ipc.RPC.stopProxy'
'org.apache.hama.GroomServerRunner.run','org.apache.hadoop.util.StringUtils.startupShutdownMessage org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.hama.GroomServerRunner.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.stats.entropy.GroupAndCountByKeyAndValueMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.commoncrawl.mapred.pipelineV3.domainmeta.blogs.postfrequency.GroupByDomainStep.reduce','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.blogs.postfrequency.GroupByDomainStep.runStep','org.apache.hadoop.mapred.JobClient.runJob'
'step1.GroupPartitioner.getPartition','org.apache.hadoop.io.IntWritable.get'
'org.apache.crunch.GroupingOptions.configure','org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'tap.compression.gz.GzipCompression.setupOutput','org.apache.hadoop.mapred.TextOutputFormat.setCompressOutput org.apache.hadoop.mapred.TextOutputFormat.setOutputCompressorClass'
'gov.llnl.ontology.text.hbase.GzipTarInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.isDirectory org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init>'
'gov.llnl.ontology.text.hbase.GzipTarInputFormat.GzipTarRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.toString'
'gov.llnl.ontology.text.hbase.GzipTarInputFormat.GzipTarRecordReader.nextKeyValue','org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.io.Text.<init>'
'havrobase.HAB.HAB','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTablePool.putTable org.apache.hadoop.hbase.client.HTableInterface.getTableDescriptor org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.addColumn org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HAB.loadSchemas','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getValue'
'havrobase.HAB.createSchemaTable','org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setInMemory org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTablePool.getTable'
'havrobase.HAB.get','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HAB.create','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTablePool.putTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HAB.getNextRow','org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue'
'havrobase.HAB.put','org.apache.hadoop.hbase.client.HTablePool.putTable org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.checkAndPut org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HAB.delete','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.HTableInterface.delete org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HAB.scan','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HAB.hasNext','org.apache.hadoop.hbase.client.Result.getRow'
'havrobase.HAB.getRowResult','org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getBuffer org.apache.hadoop.hbase.KeyValue.getValueOffset org.apache.hadoop.hbase.KeyValue.getValueLength org.apache.hadoop.hbase.KeyValue.getQualifierOffset org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toInt'
'havrobase.HAB.getVersion','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong'
'havrobase.HAB.storeSchema','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HAB.getHBaseRow','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get'
'havrobase.HAB.loadSchema','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HAB.$_','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'havrobase.HAB.getTable','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTablePool.getTable'
'havrobase.HAB.getColumnDesc','org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setInMemory'
'havrobase.HAB.$','org.apache.hadoop.hbase.util.Bytes.toBytes'
'havrobase.HABTest.HABModule.configure','org.apache.hadoop.hbase.client.HTablePool.<init>'
'havrobase.HABTest.deleteTable','org.apache.hadoop.hbase.client.HTablePool.<init> org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Result.getNoVersionMap org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.HTableInterface.delete org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTableInterface.delete org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HABTest.testEmptyRowId','org.apache.hadoop.hbase.client.HTablePool.<init> org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTableInterface.getScanner'
'havrobase.HABTest.testNoRow','org.apache.hadoop.hbase.util.Bytes.toBytes'
'havrobase.HABTest.testSave','org.apache.hadoop.hbase.util.Bytes.toBytes'
'havrobase.HABTest.testCreateSequential','org.apache.hadoop.hbase.util.Bytes.toString'
'havrobase.HABTest.testSaveFail','org.apache.hadoop.hbase.util.Bytes.toBytes'
'havrobase.HABTest.testSaveJsonFormat','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTablePool.<init> org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTablePool.putTable'
'havrobase.HABTest.testScan','org.apache.hadoop.hbase.util.Bytes.toBytes'
'havrobase.HABTest.testSchemolution','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTablePool.<init> org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTablePool.putTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTablePool.putTable'
'hipi.imagebundle.HARImageBundle.HARImageBundle','org.apache.hadoop.fs.Path.toString'
'hipi.imagebundle.HARImageBundle.openForWrite','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create'
'hipi.imagebundle.HARImageBundle.openForRead','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.HarFileSystem.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.HarFileSystem.initialize org.apache.hadoop.fs.HarFileSystem.listStatus'
'hipi.imagebundle.HARImageBundle.addImage','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.HarFileSystem.getHarHash org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'hipi.imagebundle.HARImageBundle.prepareNext','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.HarFileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.available org.apache.hadoop.fs.FSDataInputStream.read'
'hipi.imagebundle.HARImageBundle.close','org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.HarFileSystem.close org.apache.hadoop.fs.FSDataOutputStream.close'
'hipi.imagebundle.HARImageBundle.closeIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.HarFileSystem.getHarHash org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FSDataOutputStream.close'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.fillInUnboundElements','org.apache.hadoop.hdfs.util.ByteArray.getBytes org.apache.hadoop.hdfs.util.ByteArray.getBytes'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.updateId2ValueMap','org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hdfs.util.ByteArray.<init>'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.doBatchId2String','org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.doRangeScan','org.apache.hadoop.hbase.filter.PrefixFilter.<init> org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.<init> org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.HTableInterface.close'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.parseRangeScanResults','org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.ResultScanner.close'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.retrieveId','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.getValue'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.parseKey','org.apache.hadoop.hdfs.util.ByteArray.<init> org.apache.hadoop.hdfs.util.ByteArray.<init> org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.buildRangeScanKeyFromQuad','org.apache.hadoop.hbase.util.Bytes.putBytes'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.buildRangeScanKeyFromMappedIds','org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Result.toString org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hdfs.util.ByteArray.<init> org.apache.hadoop.hbase.util.Bytes.putBytes org.apache.hadoop.hbase.util.Bytes.putBytes org.apache.hadoop.hbase.util.Bytes.putBytes'
'nl.vu.datalayer.hbase.util.HBPrefixMatchUtil.buildString2IdGets','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hdfs.util.ByteArray.<init>'
'com.urbanairship.hbackup.HBackupConfig.forTests','org.apache.hadoop.conf.Configuration.<init>'
'com.urbanairship.hbackup.HBackupConfig.fromEnv','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'com.nearinfinity.hbase.dsl.HBase.HBase','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTablePool.<init>'
'com.nearinfinity.hbase.dsl.HBase.truncateTable','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.nearinfinity.hbase.dsl.HBase.save','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.nearinfinity.hbase.dsl.HBase.fetch','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.nearinfinity.hbase.dsl.HBase.defineTable','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.nearinfinity.hbase.dsl.HBase.scan','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init>'
'com.nearinfinity.hbase.dsl.HBase.removeTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'com.nearinfinity.hbase.dsl.HBase.flush','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.nearinfinity.hbase.dsl.HBase.delete','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.nearinfinity.hbase.dsl.HBase.table','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.nearinfinity.hbase.dsl.HBase.getResult','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTablePool.putTable'
'com.nearinfinity.hbase.dsl.HBase.flushDeletes','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.delete org.apache.hadoop.hbase.client.HTablePool.putTable'
'com.nearinfinity.hbase.dsl.HBase.flushPuts','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTablePool.putTable'
'com.urbanairship.datacube.backfill.HBaseBackfillMergeMapper.map','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'com.urbanairship.datacube.backfill.HBaseBackfillMergeMapper.getDeserializer','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.urbanairship.datacube.backfill.HBaseBackfillMergeMapper.makeNewLiveCubeOp','org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.client.Result.getRow'
'com.urbanairship.datacube.backfill.HBaseBackfillMerger.runWithCheckedExceptions','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getStartEndKeys org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'com.urbanairship.datacube.backfill.HBaseBackfillMerger.scansThisCubeOnly','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow'
'com.urbanairship.datacube.backfill.HBaseBackfillMerger.truncateScan','org.apache.hadoop.hbase.client.Scan.getStartRow org.apache.hadoop.hbase.client.Scan.getStopRow org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow'
'org.apache.hcatalog.hbase.HBaseBaseOutputFormat.checkOutputSpecs','org.apache.hadoop.mapred.OutputFormat<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.checkOutputSpecs'
'org.apache.hcatalog.hbase.HBaseBaseOutputFormat.getRecordWriter','org.apache.hadoop.mapred.OutputFormat<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.getRecordWriter'
'org.apache.hcatalog.hbase.HBaseBaseOutputFormat.getOutputFormat','org.apache.hadoop.mapred.JobConf.get'
'org.lilyproject.repository.impl.HBaseBlobStoreAccess.HBaseBlobStoreAccess','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.lilyproject.repository.impl.HBaseBlobStoreAccess.getOutputStream','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'org.lilyproject.repository.impl.HBaseBlobStoreAccess.getInputStream','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.getValue'
'org.lilyproject.repository.impl.HBaseBlobStoreAccess.delete','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTableInterface.delete'
'org.lilyproject.repository.impl.HBaseBlobStoreAccess.HBaseBlobOutputStream.close','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTableInterface.put'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputFormat','org.apache.hadoop.mapred.SequenceFileOutputFormat<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.<init>'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.checkOutputSpecs','org.apache.hadoop.mapred.SequenceFileOutputFormat<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.checkOutputSpecs'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.getRecordWriter','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.SequenceFileOutputFormat<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.getRecordWriter'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.addJTDelegationToken','org.apache.hadoop.hbase.security.User.isSecurityEnabled org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.JobClient.getDelegationToken'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkRecordWriter.write','org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.write'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkRecordWriter.close','org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.close'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.HBaseBulkOutputCommitter','org.apache.hadoop.mapred.FileOutputCommitter.<init>'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.abortTask','org.apache.hadoop.mapred.OutputCommitter.abortTask'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.commitTask','org.apache.hadoop.mapred.OutputCommitter.commitTask'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.needsTaskCommit','org.apache.hadoop.mapred.OutputCommitter.needsTaskCommit'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.setupJob','org.apache.hadoop.mapred.OutputCommitter.setupJob'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.setupTask','org.apache.hadoop.mapred.OutputCommitter.setupTask'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.abortJob','org.apache.hadoop.mapred.OutputCommitter.abortJob org.apache.hadoop.mapred.JobContext.getConfiguration org.apache.hadoop.mapred.JobContext.getConfiguration'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.commitJob','org.apache.hadoop.mapred.OutputCommitter.commitJob org.apache.hadoop.mapred.JobContext.getConfiguration org.apache.hadoop.mapred.JobContext.getJobConf org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.hbase.HBaseBulkOutputFormat.HBaseBulkOutputCommitter.cleanIntermediate','org.apache.hadoop.mapred.JobContext.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobContext.getJobConf org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete'
'com.impetus.kundera.tests.cli.HBaseCli.startCluster','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.<init> org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setDefaultClientPort org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setTickTime org.apache.hadoop.hbase.HBaseTestingUtility.getClusterTestDir org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup org.apache.hadoop.hbase.HBaseTestingUtility.setZkCluster org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseCluster'
'com.impetus.kundera.tests.cli.HBaseCli.createTable','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HBaseTestingUtility.createTable'
'com.impetus.kundera.tests.cli.HBaseCli.addColumnFamily','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin'
'com.impetus.kundera.tests.cli.HBaseCli.stopCluster','org.apache.hadoop.hbase.HBaseTestingUtility.cleanupTestDir org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster org.apache.hadoop.fs.FileUtil.fullyDelete org.apache.hadoop.fs.FileUtil.fullyDelete'
'com.impetus.client.hbase.junits.HBaseCli.startCluster','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.client.HTablePool.<init> org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.<init> org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setDefaultClientPort org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setTickTime org.apache.hadoop.hbase.HBaseTestingUtility.getClusterTestDir org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup org.apache.hadoop.hbase.HBaseTestingUtility.setZkCluster org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseCluster'
'com.impetus.client.hbase.junits.HBaseCli.createTable','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HBaseTestingUtility.createTable'
'com.impetus.client.hbase.junits.HBaseCli.addColumnFamily','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin'
'com.impetus.client.hbase.junits.HBaseCli.dropTable','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin'
'com.impetus.client.hbase.junits.HBaseCli.cleanUp','org.apache.hadoop.hbase.HBaseTestingUtility.cleanupTestDir org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster org.apache.hadoop.fs.FileUtil.fullyDelete org.apache.hadoop.fs.FileUtil.fullyDelete'
'com.cloudera.hbase.HBaseClient.main','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.close'
'com.yahoo.ycsb.db.HBaseClient.init','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.yahoo.ycsb.db.HBaseClient.cleanup','org.apache.hadoop.hbase.client.HTable.flushCommits'
'com.yahoo.ycsb.db.HBaseClient.getHTable','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.client.HTable.setWriteBufferSize'
'com.yahoo.ycsb.db.HBaseClient.read','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.yahoo.ycsb.db.HBaseClient.scan','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'com.yahoo.ycsb.db.HBaseClient.update','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.yahoo.ycsb.db.HBaseClient.delete','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'com.yahoo.ycsb.db.HBaseClient.init','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.yahoo.ycsb.db.HBaseClient.cleanup','org.apache.hadoop.hbase.client.HTable.flushCommits'
'com.yahoo.ycsb.db.HBaseClient.getHTable','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.client.HTable.setWriteBufferSize'
'com.yahoo.ycsb.db.HBaseClient.read','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.yahoo.ycsb.db.HBaseClient.scan','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.client.ResultScanner.close'
'com.yahoo.ycsb.db.HBaseClient.update','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.yahoo.ycsb.db.HBaseClient.delete','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'org.apache.james.mailbox.hbase.HBaseClusterSingleton.HBaseClusterSingleton','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.MiniHBaseCluster.waitForActiveAndReadyMaster org.apache.hadoop.hbase.MiniHBaseCluster.getConfiguration'
'org.apache.james.mailbox.hbase.HBaseClusterSingleton.run','org.apache.hadoop.hbase.MiniHBaseCluster.shutdown'
'org.apache.james.mailbox.hbase.HBaseClusterSingleton.ensureTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HBaseTestingUtility.createTable'
'org.apache.james.mailbox.hbase.HBaseClusterSingleton.clearTable','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'com.stumbleupon.hbaseadmin.HBaseCompact.compactAllServers','org.apache.hadoop.hbase.HRegionInfo.getRegionNameAsString org.apache.hadoop.hbase.HRegionInfo.getRegionName org.apache.hadoop.hbase.client.HBaseAdmin.majorCompact'
'com.stumbleupon.hbaseadmin.HBaseCompact.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'org.apache.camel.component.hbase.HBaseComponent.doStart','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTablePool.<init>'
'org.apache.camel.component.hbase.HBaseComponent.doStop','org.apache.hadoop.hbase.client.HTablePool.close'
'com.ripariandata.timberwolf.writer.hbase.HBaseConfigurator.createConfiguration','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.ripariandata.timberwolf.writer.hbase.HBaseConfiguratorTest.testCreate','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.facebook.tsdb.tsdash.server.data.hbase.HBaseConnection.configure','org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.setStrings'
'com.facebook.tsdb.tsdash.server.data.hbase.HBaseConnection.getDataTableConn','org.apache.hadoop.hbase.client.HTable.<init>'
'com.facebook.tsdb.tsdash.server.data.hbase.HBaseConnection.getIDsTableConn','org.apache.hadoop.hbase.client.HTable.<init>'
'org.lilyproject.server.modules.general.HBaseConnectionDisposer.stop','org.apache.hadoop.hbase.client.HConnectionManager.deleteAllConnections'
'com.harioca.shell.hbase.HBaseConnectionPool.init','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.HTablePool.<init>'
'com.harioca.shell.hbase.HBaseConnectionPool.getTable','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTablePool.getTable'
'com.harioca.shell.hbase.HBaseConnectionPool.putTable','org.apache.hadoop.hbase.client.HTablePool.putTable'
'com.harioca.shell.hbase.HBaseConnectionPool.closeTablePool','org.apache.hadoop.hbase.client.HTablePool.closeTablePool org.apache.hadoop.hbase.client.HTablePool.closeTablePool'
'com.atlantbh.jmeter.plugins.hbasecomponents.config.HBaseConnectionVariable.getConfig','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.atlantbh.jmeter.plugins.hbasecomponents.config.HBaseConnectionVariable.getTable','org.apache.hadoop.hbase.client.HTablePool.<init> org.apache.hadoop.hbase.client.HTablePool.getTable'
'com.atlantbh.jmeter.plugins.hbasecomponents.config.HBaseConnectionVariable.putTable','org.apache.hadoop.hbase.client.HTablePool.putTable'
'storm.contrib.hbase.utils.HBaseConnector.getHBaseConf','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.HBaseConfiguration.addResource'
'org.apache.camel.component.hbase.HBaseConvertionsTest.testPutMultiRows','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.HBaseDataModel','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.HTableFactory.<init> org.apache.hadoop.hbase.client.HTablePool.<init>'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.bootstrap','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.close'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.getPreferencesFromUser','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toFloat'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.getItemIDsFromUser','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toLong'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.getPreferencesForItem','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toFloat'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.getPreferenceValue','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.containsColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toFloat'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.getPreferenceTime','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.containsColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getColumnLatest org.apache.hadoop.hbase.KeyValue.getTimestamp'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.getNumUsersWithPreferenceFor','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toLong'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.setPreference','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTableInterface.close'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.removePreference','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.delete org.apache.hadoop.hbase.client.HTableInterface.close'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.close','org.apache.hadoop.hbase.client.HTablePool.close'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.refreshItemIDs','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.KeyOnlyFilter.<init> org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.<init> org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.HTableInterface.close'
'org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel.refreshUserIDs','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.KeyOnlyFilter.<init> org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.<init> org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.HTableInterface.close'
'com.facebook.tsdb.tsdash.server.data.hbase.HBaseDataProvider.pickDataPoints','org.apache.hadoop.hbase.util.Bytes.startsWith org.apache.hadoop.hbase.util.Bytes.startsWith'
'com.facebook.tsdb.tsdash.server.data.hbase.HBaseDataProvider.fetchMetric','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getFamilyMap'
'com.facebook.tsdb.tsdash.server.data.hbase.HBaseDataProvider.fetchMetricHeader','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow'
'com.bah.culvert.databaseadapter.HBaseDatabaseAdapter.setConf','org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'com.bah.culvert.databaseadapter.HBaseDatabaseAdapter.create','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'com.bah.culvert.databaseadapter.HBaseDatabaseAdapter.delete','org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'com.bah.culvert.databaseadapter.HBaseDatabaseAdapter.verify','org.apache.hadoop.hbase.client.HBaseAdmin.isMasterRunning'
'com.bah.culvert.databaseadapter.HBaseDatabaseAdapter.getTableAdapter','org.apache.hadoop.conf.Configuration.<init>'
'com.bah.culvert.databaseadapter.HBaseDatabaseAdapter.tableExists','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'com.bah.culvert.databaseadapter.HBaseDatabaseAdapterIT.setup','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getMiniHBaseCluster'
'com.bah.culvert.databaseadapter.HBaseDatabaseAdapterIT.tearDown','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'com.urbanairship.datacube.dbharnesses.HBaseDbHarness.get','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.value'
'com.urbanairship.datacube.dbharnesses.HBaseDbHarness.increment','org.apache.hadoop.hbase.util.Bytes.toLong'
'com.urbanairship.datacube.dbharnesses.HBaseDbHarness.readCombineCas','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'com.urbanairship.datacube.dbharnesses.HBaseDbHarness.overwrite','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'siena.hbase.HBaseDdlGenerator.updateSchema','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'siena.hbase.HBaseDdlGenerator.createTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'siena.hbase.HBaseDdlGenerator.dropTables','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'org.apache.camel.component.hbase.HBaseDeleteHandler.remove','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTableInterface.delete'
'org.apache.hcatalog.hbase.HBaseDirectOutputFormat.HBaseDirectOutputFormat','org.apache.hadoop.hbase.mapred.TableOutputFormat.<init>'
'org.apache.hcatalog.hbase.HBaseDirectOutputFormat.getRecordWriter','org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter'
'org.apache.hcatalog.hbase.HBaseDirectOutputFormat.checkOutputSpecs','org.apache.hadoop.hbase.mapred.TableOutputFormat.checkOutputSpecs'
'org.apache.hcatalog.hbase.HBaseDirectOutputFormat.HBaseDirectRecordWriter.write','org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.write'
'org.apache.hcatalog.hbase.HBaseDirectOutputFormat.HBaseDirectRecordWriter.close','org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.hbase.client.Put>.close'
'org.apache.hcatalog.hbase.HBaseDirectOutputFormat.HBaseDirectOutputCommitter.abortJob','org.apache.hadoop.mapred.JobContext.getConfiguration org.apache.hadoop.mapred.JobContext.getConfiguration'
'org.apache.hcatalog.hbase.HBaseDirectOutputFormat.HBaseDirectOutputCommitter.commitJob','org.apache.hadoop.mapred.JobContext.getConfiguration org.apache.hadoop.mapred.JobContext.getConfiguration'
'org.infinispan.loaders.hbase.HBaseFacade.HBaseFacade','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set'
'org.infinispan.loaders.hbase.HBaseFacade.close','org.apache.hadoop.hbase.client.HBaseAdmin.close'
'org.infinispan.loaders.hbase.HBaseFacade.createTable','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.infinispan.loaders.hbase.HBaseFacade.deleteTable','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'org.infinispan.loaders.hbase.HBaseFacade.tableExists','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.isTableAvailable'
'org.infinispan.loaders.hbase.HBaseFacade.addRow','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'org.infinispan.loaders.hbase.HBaseFacade.readRow','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.client.HTable.close'
'org.infinispan.loaders.hbase.HBaseFacade.readRows','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getColumn org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.ResultScanner.close'
'org.infinispan.loaders.hbase.HBaseFacade.removeRow','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.exists org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.close'
'org.infinispan.loaders.hbase.HBaseFacade.removeRows','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.close'
'org.infinispan.loaders.hbase.HBaseFacade.scan','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.containsColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.ResultScanner.close'
'org.infinispan.loaders.hbase.HBaseFacade.scanForKeys','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.ResultScanner.close'
'org.infinispan.loaders.hbase.HBaseFacade.getKeyFromResult','org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.toString'
'com.atlantbh.jmeter.plugins.hbasecomponents.utils.filter.HBaseFilterParser.parse','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.setFilterIfMissing'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.configureInputJobProperties','org.apache.hadoop.hive.ql.plan.TableDesc.getJobProperties org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.configureOutputJobProperties','org.apache.hadoop.hive.ql.plan.TableDesc.getJobProperties org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.preCreateTable','org.apache.hadoop.hive.metastore.MetaStoreUtils.isExternalTable org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hive.metastore.api.Table.getParameters org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HTableDescriptor.hasFamily org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.hive.metastore.api.MetaException.<init>'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.getHBaseAdmin','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.hive.metastore.api.MetaException.<init>'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.getFullyQualifiedHBaseTableName','org.apache.hadoop.hive.metastore.api.Table.getParameters org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.getTableName'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.getConf','org.apache.hadoop.hbase.HBaseConfiguration.create'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.setConf','org.apache.hadoop.hbase.HBaseConfiguration.create'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.checkDeleteTable','org.apache.hadoop.hive.metastore.MetaStoreUtils.isExternalTable org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.hive.metastore.api.MetaException.<init>'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.addOutputDependencyJars','org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJars'
'org.apache.hcatalog.hbase.HBaseHCatStorageHandler.addResources','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources org.apache.hadoop.conf.Configuration.get'
'util.HBaseHelper.HBaseHelper','org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'util.HBaseHelper.existsTable','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'util.HBaseHelper.createTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'util.HBaseHelper.disableTable','org.apache.hadoop.hbase.client.HBaseAdmin.disableTable'
'util.HBaseHelper.dropTable','org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'util.HBaseHelper.fillTable','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'util.HBaseHelper.put','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'util.HBaseHelper.dump','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'util.HBaseHelper.HBaseHelper','org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'util.HBaseHelper.existsTable','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'util.HBaseHelper.createTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'util.HBaseHelper.disableTable','org.apache.hadoop.hbase.client.HBaseAdmin.disableTable'
'util.HBaseHelper.dropTable','org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'util.HBaseHelper.fillTable','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'util.HBaseHelper.put','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'util.HBaseHelper.dump','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.camel.component.hbase.processor.idempotent.HBaseIdempotentRepository.HBaseIdempotentRepository','org.apache.hadoop.hbase.client.HTable.<init>'
'org.apache.camel.component.hbase.processor.idempotent.HBaseIdempotentRepository.add','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits'
'org.apache.camel.component.hbase.processor.idempotent.HBaseIdempotentRepository.contains','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.exists'
'org.apache.camel.component.hbase.processor.idempotent.HBaseIdempotentRepository.remove','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.exists org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'org.apache.camel.component.hbase.processor.idempotent.HBaseIdempotentRepository.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.camel.component.hbase.processor.idempotent.HBaseIdempotentRepositoryTest.setUp','org.apache.hadoop.hbase.client.HTable.<init>'
'org.hbasene.index.HBaseIndexReader.HBaseIndexReader','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.hbasene.index.HBaseIndexReader.docFreq','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.client.HTablePool.putTable'
'org.hbasene.index.HBaseIndexReader.document','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTablePool.putTable'
'org.hbasene.index.HBaseIndexReader.numDocs','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.HTablePool.putTable'
'org.hbasene.index.HBaseIndexStore.HBaseIndexStore','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.conf.Configuration.getInt'
'org.hbasene.index.HBaseIndexStore.close','org.apache.hadoop.hbase.client.HTable.close'
'org.hbasene.index.HBaseIndexStore.doStoreReverseMapping','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.setWriteToWAL org.apache.hadoop.hbase.client.HTable.getWriteBuffer'
'org.hbasene.index.HBaseIndexStore.doCommitTermVector','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.putInt org.apache.hadoop.hbase.util.Bytes.putInt org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.setWriteToWAL org.apache.hadoop.hbase.client.HTable.getWriteBuffer org.apache.hadoop.hbase.client.HTable.flushCommits'
'org.hbasene.index.HBaseIndexStore.doCommitTermFrequencies','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.getWriteBuffer org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.getWriteBuffer org.apache.hadoop.hbase.client.HTable.flushCommits'
'org.hbasene.index.HBaseIndexStore.doIncrementSegmentId','org.apache.hadoop.hbase.client.HTable.incrementColumnValue'
'org.hbasene.index.HBaseIndexStore.doStoreFields','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.setWriteToWAL org.apache.hadoop.hbase.client.HTable.getWriteBuffer'
'org.hbasene.index.HBaseIndexStore.getCurrentRow','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.hbasene.index.HBaseIndexStore.dropLuceneIndexTable','org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'org.hbasene.index.HBaseIndexStore.doDropTable','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.isTableAvailable org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'org.hbasene.index.HBaseIndexStore.createLuceneIndexTable','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits'
'org.hbasene.index.HBaseIndexStore.createUniversionLZO','org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions'
'org.apache.hcatalog.hbase.HBaseInputFormat.HBaseInputFormat','org.apache.hadoop.hbase.mapreduce.TableInputFormat.<init>'
'org.apache.hcatalog.hbase.HBaseInputFormat.getRecordReader','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf org.apache.hadoop.hbase.mapreduce.TableInputFormat.getScan org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.mapred.TableSplit.getStartRow org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.mapred.TableSplit.getEndRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.HTable.<init>'
'org.apache.hcatalog.hbase.HBaseInputFormat.getSplits','org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf org.apache.hadoop.mapred.HCatMapRedUtil.createJobContext org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits'
'org.apache.hcatalog.hbase.HBaseInputFormat.convertSplits','org.apache.hadoop.hbase.mapred.TableSplit.<init>'
'com.bah.culvert.tableadapters.HBaseLocalTableAdapter.get','org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor.getEnvironment org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.regionserver.HRegion.getScanner org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.KeyValue.getKey org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'com.bah.culvert.tableadapters.HBaseLocalTableAdapter.getStartKey','org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor.getEnvironment org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.HRegion.getStartKey'
'com.bah.culvert.tableadapters.HBaseLocalTableAdapter.getEndKey','org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor.getEnvironment org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.HRegion.getEndKey'
'com.ripariandata.timberwolf.writer.hbase.HBaseMailWriter.HBaseMailWriter','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.ripariandata.timberwolf.writer.hbase.HBaseMailWriter.write','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.ripariandata.timberwolf.writer.hbase.HBaseMailWriterTest.assertMailboxItemDescription','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.ripariandata.timberwolf.writer.hbase.HBaseMailWriterTest.testInterfaces','org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.james.mailbox.hbase.mail.HBaseMailboxMapperTest.testSave','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTable.get'
'org.apache.james.mailbox.hbase.mail.HBaseMailboxMapperTest.testChunkStream','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.james.mailbox.hbase.HBaseMailboxSessionMapperFactory.HBaseMailboxSessionMapperFactory','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'com.ripariandata.timberwolf.writer.hbase.HBaseManager.HBaseManager','org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'com.ripariandata.timberwolf.writer.hbase.HBaseManager.tableExistsRemotely','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'com.ripariandata.timberwolf.writer.hbase.HBaseManager.getTableRemotely','org.apache.hadoop.hbase.client.HTable.<init>'
'com.ripariandata.timberwolf.writer.hbase.HBaseManager.createTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTable.<init>'
'com.ripariandata.timberwolf.writer.hbase.HBaseManager.deleteTable','org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'com.larsgeorge.hadoop.hbase.HBaseManager.process','org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'com.larsgeorge.hadoop.hbase.HBaseManager.listTables','org.apache.hadoop.hbase.HTableDescriptor.getNameAsString'
'com.larsgeorge.hadoop.hbase.HBaseManager.createConfiguration','org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.HTableDescriptor.isDeferredLogFlush org.apache.hadoop.hbase.HTableDescriptor.getMaxFileSize org.apache.hadoop.hbase.HTableDescriptor.getMemStoreFlushSize org.apache.hadoop.hbase.HTableDescriptor.isReadOnly org.apache.hadoop.hbase.HTableDescriptor.getColumnFamilies org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.HColumnDescriptor.getMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.getCompressionType org.apache.hadoop.hbase.HColumnDescriptor.isInMemory org.apache.hadoop.hbase.HColumnDescriptor.isBlockCacheEnabled org.apache.hadoop.hbase.HColumnDescriptor.getBlocksize org.apache.hadoop.hbase.HColumnDescriptor.getTimeToLive org.apache.hadoop.hbase.HColumnDescriptor.getBloomFilterType org.apache.hadoop.hbase.HColumnDescriptor.getScope'
'com.larsgeorge.hadoop.hbase.HBaseManager.getConfiguration','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.larsgeorge.hadoop.hbase.HBaseManager.createOrChangeTable','org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.HTableDescriptor.getFamilies org.apache.hadoop.hbase.HColumnDescriptor.getName org.apache.hadoop.hbase.HTableDescriptor.getFamily org.apache.hadoop.hbase.HColumnDescriptor.equals org.apache.hadoop.hbase.HTableDescriptor.getFamilies org.apache.hadoop.hbase.HTableDescriptor.getFamilies org.apache.hadoop.hbase.HTableDescriptor.getFamilies org.apache.hadoop.hbase.HTableDescriptor.getFamilies org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.client.HBaseAdmin.modifyColumn org.apache.hadoop.hbase.client.HBaseAdmin.addColumn org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.client.HBaseAdmin.deleteColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.modifyTable org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'com.larsgeorge.hadoop.hbase.HBaseManager.hasSameProperties','org.apache.hadoop.hbase.HTableDescriptor.isDeferredLogFlush org.apache.hadoop.hbase.HTableDescriptor.isDeferredLogFlush org.apache.hadoop.hbase.HTableDescriptor.getMaxFileSize org.apache.hadoop.hbase.HTableDescriptor.getMaxFileSize org.apache.hadoop.hbase.HTableDescriptor.getMemStoreFlushSize org.apache.hadoop.hbase.HTableDescriptor.getMemStoreFlushSize org.apache.hadoop.hbase.HTableDescriptor.isReadOnly org.apache.hadoop.hbase.HTableDescriptor.isReadOnly'
'com.larsgeorge.hadoop.hbase.HBaseManager.convertSchemaToDescriptor','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.setDeferredLogFlush org.apache.hadoop.hbase.HTableDescriptor.setMaxFileSize org.apache.hadoop.hbase.HTableDescriptor.setMemStoreFlushSize org.apache.hadoop.hbase.HTableDescriptor.setReadOnly org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'com.larsgeorge.hadoop.hbase.HBaseManager.getTable','org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.HTableDescriptor.getNameAsString'
'com.larsgeorge.hadoop.hbase.HBaseManager.tableExists','org.apache.hadoop.hbase.HTableDescriptor.getNameAsString'
'com.larsgeorge.hadoop.hbase.HBaseManager.getTables','org.apache.hadoop.hbase.client.HBaseAdmin.listTables'
'org.gora.hbase.store.HBaseMapping.addTable','org.apache.hadoop.hbase.HTableDescriptor.<init>'
'org.gora.hbase.store.HBaseMapping.addColumnFamily','org.apache.hadoop.hbase.io.hfile.Compression.Algorithm.valueOf org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setBlockCacheEnabled org.apache.hadoop.hbase.HColumnDescriptor.setBlocksize org.apache.hadoop.hbase.HColumnDescriptor.setBloomfilter org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.setTimeToLive org.apache.hadoop.hbase.HColumnDescriptor.setInMemory org.apache.hadoop.hbase.HColumnDescriptor.setMapFileIndexInterval org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.getFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'org.gora.hbase.store.HBaseMapping.addField','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.gora.hbase.store.HBaseMapping.getTableName','org.apache.hadoop.hbase.HTableDescriptor.getNameAsString'
'org.apache.gora.hbase.store.HBaseMapping.HBaseMappingBuilder.addFamilyProps','org.apache.hadoop.hbase.io.hfile.Compression.Algorithm.valueOf org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setBlockCacheEnabled org.apache.hadoop.hbase.HColumnDescriptor.setBlocksize org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.valueOf org.apache.hadoop.hbase.HColumnDescriptor.setBloomFilterType org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.setTimeToLive org.apache.hadoop.hbase.HColumnDescriptor.setInMemory'
'org.apache.gora.hbase.store.HBaseMapping.HBaseMappingBuilder.addField','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.gora.hbase.store.HBaseMapping.HBaseMappingBuilder.getOrCreateFamily','org.apache.hadoop.hbase.HColumnDescriptor.<init>'
'org.apache.gora.hbase.store.HBaseMapping.HBaseMappingBuilder.build','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.findMessagesInMailbox','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.filter.PrefixFilter.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.setFilterIfMissing org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.findMessagesInMailboxWithUID','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.setFilterIfMissing org.apache.hadoop.hbase.client.Get.setFilter org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.findMessagesInMailboxAfterUID','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.setFilterIfMissing org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.findMessagesInMailboxBetweenUIDs','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.setFilterIfMissing org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.countMessagesInMailbox','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.countUnseenMessagesInMailbox','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.filter.SingleColumnValueExcludeFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScannerCaching org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.delete','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.findFirstUnseenMessageUid','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScannerCaching org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.findRecentMessageUidsInMailbox','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScannerCaching org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.updateFlags','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.save','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.deleteDeletedMessagesInMailboxWithUID','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.deleteDeletedMessagesInMailboxBetweenUIDs','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.deleteDeletedMessagesInMailboxAfterUID','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseMessageMapper.deleteDeletedMessagesInMailbox','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.filter.PrefixFilter.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'org.lilyproject.clientmetrics.HBaseMetrics.reportMetrics','org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus org.apache.hadoop.hbase.ClusterStatus.getServerInfo org.apache.hadoop.hbase.HServerInfo.getHostname'
'org.lilyproject.clientmetrics.HBaseMetrics.getHBaseTableInfo','org.apache.hadoop.hbase.ClusterStatus.getServerInfo org.apache.hadoop.hbase.HServerInfo.getLoad org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.HTableInterface.close'
'org.lilyproject.clientmetrics.HBaseMetrics.outputHBaseState','org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus org.apache.hadoop.hbase.ClusterStatus.getHBaseVersion org.apache.hadoop.hbase.ClusterStatus.getRegionsInTransition'
'org.lilyproject.clientmetrics.HBaseMetrics.outputRegionServersInfo','org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus org.apache.hadoop.hbase.ClusterStatus.getServerInfo org.apache.hadoop.hbase.HServerInfo.getHostname'
'org.lilyproject.clientmetrics.HBaseMetrics.outputRegionCountByServer','org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus org.apache.hadoop.hbase.ClusterStatus.getServerInfo org.apache.hadoop.hbase.HServerInfo.getLoad org.apache.hadoop.hbase.HServerInfo.getHostname'
'com.sematext.hadoop.metrics.HBaseMetricsContext.init','org.apache.hadoop.metrics.MetricsException.<init>'
'com.sematext.hadoop.metrics.HBaseMetricsContext.getTable','org.apache.hadoop.metrics.MetricsException.<init> org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.metrics.MetricsException.<init>'
'com.sematext.hadoop.metrics.HBaseMetricsContext.emitRecord','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.metrics.spi.OutputRecord.getTagNames org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.metrics.spi.OutputRecord.getTag org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.metrics.spi.OutputRecord.getTagNames org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.metrics.spi.OutputRecord.getTag org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.metrics.spi.OutputRecord.getMetricNames org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.metrics.spi.OutputRecord.getMetric org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.sematext.hadoop.metrics.HBaseMetricsContext.close','org.apache.hadoop.hbase.client.HTable.close'
'org.lilyproject.clientmetrics.HBaseMetricsPlugin.getExtraInfoLines','org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus org.apache.hadoop.hbase.ClusterStatus.getAverageLoad org.apache.hadoop.hbase.ClusterStatus.getDeadServers org.apache.hadoop.hbase.ClusterStatus.getServers org.apache.hadoop.hbase.ClusterStatus.getRegionsCount'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.booleanToBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.bytesToBoolean','org.apache.hadoop.hbase.util.Bytes.toBoolean'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.shortToBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.bytesToShort','org.apache.hadoop.hbase.util.Bytes.toShort'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.integerToBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.bytesToInteger','org.apache.hadoop.hbase.util.Bytes.toInt'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.longToBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.bytesToLong','org.apache.hadoop.hbase.util.Bytes.toLong'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.doubleToBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.bytesToDouble','org.apache.hadoop.hbase.util.Bytes.toDouble'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.floatToBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.bytesToFloat','org.apache.hadoop.hbase.util.Bytes.toFloat'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.stringToBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.camel.component.hbase.converter.HBaseModelConverter.bytesToString','org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.whirr.service.hbase.integration.HBaseOldServiceTest.test','org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.<init> org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.<init> org.apache.hadoop.hbase.thrift.generated.Mutation.<init> org.apache.hadoop.hbase.util.Bytes.toString'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseOrderedKeyColumnValueStore.HBaseOrderedKeyColumnValueStore','org.apache.hadoop.hbase.client.HTablePool.<init>'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseOrderedKeyColumnValueStore.close','org.apache.hadoop.hbase.client.HTablePool.close'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseOrderedKeyColumnValueStore.get','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.size'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseOrderedKeyColumnValueStore.containsKey','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.exists org.apache.hadoop.hbase.client.HTableInterface.close'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseOrderedKeyColumnValueStore.getSlice','org.apache.hadoop.hbase.filter.ColumnRangeFilter.<init> org.apache.hadoop.hbase.filter.ColumnPaginationFilter.<init> org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.ColumnRangeFilter.<init>'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseOrderedKeyColumnValueStore.getHelper','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.Get.setFilter org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.getFamilyMap'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseOrderedKeyColumnValueStore.mutate','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.batch org.apache.hadoop.hbase.client.HTableInterface.flushCommits org.apache.hadoop.hbase.client.HTableInterface.close'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseOrderedKeyColumnValueStore.mutateMany','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.batch org.apache.hadoop.hbase.client.HTableInterface.flushCommits org.apache.hadoop.hbase.client.HTableInterface.close'
'nl.vu.datalayer.hbase.schema.HBasePredicateCFSchema.cachePredicates','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'nl.vu.datalayer.hbase.schema.HBasePredicateCFSchema.createTableStruct','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.hasFamily org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.apache.camel.component.hbase.HBaseProducer.process','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTableInterface.flushCommits org.apache.hadoop.hbase.client.HTableInterface.delete org.apache.hadoop.hbase.client.HTableInterface.close'
'org.apache.camel.component.hbase.HBaseProducer.createPut','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'org.apache.camel.component.hbase.HBaseProducer.getCells','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.getColumn'
'org.apache.camel.component.hbase.HBaseProducer.createDeleteRow','org.apache.hadoop.hbase.client.Delete.<init>'
'org.apache.camel.component.hbase.HBaseProducer.scanCells','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.Filter.getClass org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.ResultScanner.next'
'org.lilyproject.testfw.HBaseProxy.start','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.testfw.HBaseProxy.getZkConnectString','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.lilyproject.testfw.HBaseProxy.addUserProps','org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.testfw.HBaseProxy.addHBaseTestProps','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.testfw.HBaseProxy.stop','org.apache.hadoop.hbase.client.HConnectionManager.deleteAllConnections'
'org.lilyproject.testfw.HBaseProxy.run','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'org.lilyproject.testfw.HBaseProxy.getBlobFS','org.apache.hadoop.hbase.HBaseTestingUtility.getDFSCluster org.apache.hadoop.fs.FileSystem.get'
'org.lilyproject.testfw.HBaseProxy.cleanTables','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.client.HBaseAdmin.flush org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.client.HBaseAdmin.majorCompact'
'org.lilyproject.testfw.HBaseProxy.insertTimestampTableTestRecord','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'org.lilyproject.testfw.HBaseProxy.waitForTimestampTables','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'org.lilyproject.testfw.HBaseProxy.waitForCompact','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getValue'
'org.lilyproject.testfw.HBaseProxy.majorCompact','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HBaseAdmin.flush org.apache.hadoop.hbase.client.HBaseAdmin.majorCompact org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.impetus.client.hbase.query.HBaseQuery.QueryTranslator.getFilter','org.apache.hadoop.hbase.filter.FilterList.<init>'
'com.impetus.client.hbase.query.HBaseQuery.QueryTranslator.onParseFilter','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.equals org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.equals org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.equals org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.equals org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.equals'
'com.talis.hbase.rdf.connection.HBaseRdfConnectionFactory.createHBaseAdmin','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'com.talis.hbase.rdf.connection.HBaseRdfConnectionFactory.createHBaseConfiguration','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource'
'com.talis.hbase.rdf.layout.vpindexed.HBaseRdfSPOIterator.hasNext','org.apache.hadoop.hbase.client.ResultScanner.next'
'com.talis.hbase.rdf.layout.vpindexed.HBaseRdfSPOIterator._next','org.apache.hadoop.hbase.client.ResultScanner.next'
'com.talis.hbase.rdf.layout.vpindexed.HBaseRdfSPOIterator.getTriple','org.apache.hadoop.hbase.client.Result.getRow'
'com.talis.hbase.rdf.layout.vpindexed.HBaseRdfSingleTableIterator.close','org.apache.hadoop.hbase.client.ResultScanner.close'
'com.talis.hbase.rdf.layout.vpindexed.HBaseRdfSingleTableIterator.hasNext','org.apache.hadoop.hbase.client.ResultScanner.next'
'com.talis.hbase.rdf.layout.vpindexed.HBaseRdfSingleTableIterator._next','org.apache.hadoop.hbase.client.ResultScanner.next'
'com.talis.hbase.rdf.layout.indexed.HBaseRdfSingleTableIterator.close','org.apache.hadoop.hbase.client.ResultScanner.close'
'com.talis.hbase.rdf.layout.indexed.HBaseRdfSingleTableIterator.hasNext','org.apache.hadoop.hbase.client.ResultScanner.next'
'com.talis.hbase.rdf.layout.indexed.HBaseRdfSingleTableIterator._next','org.apache.hadoop.hbase.client.ResultScanner.next'
'com.talis.hbase.rdf.layout.indexed.HBaseRdfSingleTableIterator.getTriple','org.apache.hadoop.hbase.client.Result.getRow'
'com.talis.hbase.rdf.layout.verticalpartitioning.HBaseRdfSingleTableIterator.close','org.apache.hadoop.hbase.client.ResultScanner.close'
'com.talis.hbase.rdf.layout.verticalpartitioning.HBaseRdfSingleTableIterator.hasNext','org.apache.hadoop.hbase.client.ResultScanner.next'
'com.talis.hbase.rdf.layout.verticalpartitioning.HBaseRdfSingleTableIterator._next','org.apache.hadoop.hbase.client.ResultScanner.next'
'com.talis.hbase.rdf.layout.hash.HBaseRdfSorOSingleRowIterator.HBaseRdfSorOSingleRowIterator','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.Result.getFamilyMap'
'com.talis.hbase.rdf.layout.hash.HBaseRdfSorOSingleRowIterator.getNodeFromHash','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toString'
'com.talis.hbase.rdf.layout.hash.HBaseRdfSorOSingleRowIterator._next','org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toLong'
'org.apache.james.rrt.hbase.HBaseRecipientRewriteTable.getUserDomainMappingsInternal','org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.rrt.hbase.HBaseRecipientRewriteTable.feedUserDomainMappingsList','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getColumn org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.james.rrt.hbase.HBaseRecipientRewriteTable.getAllMappingsInternal','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScannerCaching org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.rrt.hbase.HBaseRecipientRewriteTable.mapAddressInternal','org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.rrt.hbase.HBaseRecipientRewriteTable.getMapping','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getColumn org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.james.rrt.hbase.HBaseRecipientRewriteTable.doRemoveMapping','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.rrt.hbase.HBaseRecipientRewriteTable.doAddMapping','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.close'
'org.lilyproject.repository.impl.HBaseRepository.createOrUpdate','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBoolean'
'org.lilyproject.repository.impl.HBaseRepository.create','org.apache.hadoop.hbase.client.HTableInterface.lockRow org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTableInterface.unlockRow'
'org.lilyproject.repository.impl.HBaseRepository.updateRecord','org.apache.hadoop.hbase.client.Put.<init>'
'org.lilyproject.repository.impl.HBaseRepository.calculateRecordChanges','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseRepository.calculateChangedFields','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseRepository.calculateUpdateFields','org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseRepository.updateMutableFields','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseRepository.copyValueToNextVersionIfNeeded','org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseRepository.getLatestVersion','org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong'
'org.lilyproject.repository.impl.HBaseRepository.getRow','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.Get.setTimeRange org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBoolean'
'org.lilyproject.repository.impl.HBaseRepository.recordExists','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBoolean'
'org.lilyproject.repository.impl.HBaseRepository.extractRecordType','org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.Result.getMap'
'org.lilyproject.repository.impl.HBaseRepository.extractVersionRecordType','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong'
'org.lilyproject.repository.impl.HBaseRepository.extractFields','org.apache.hadoop.hbase.client.Result.getMap'
'org.lilyproject.repository.impl.HBaseRepository.addFieldsToGet','org.apache.hadoop.hbase.client.Get.addColumn'
'org.lilyproject.repository.impl.HBaseRepository.addSystemColumnsToGet','org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn'
'org.lilyproject.repository.impl.HBaseRepository.delete','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseRepository.getVariants','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.PrefixFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow'
'pl.edu.icm.coansys.importers.io.readers.hbaserest.HBaseRestReader_Toy.readDocumentMetadataHBase','org.apache.hadoop.hbase.rest.client.Cluster.<init> org.apache.hadoop.hbase.rest.client.Cluster.add org.apache.hadoop.hbase.rest.client.Client.<init> org.apache.hadoop.hbase.rest.client.RemoteHTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.rest.client.RemoteHTable.getScanner org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.client.Result.value org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'org.apache.gora.hbase.query.HBaseResult.readNext','org.apache.hadoop.hbase.client.Result.getRow'
'com.atlantbh.jmeter.plugins.hbasecomponents.samplers.HBaseRowkeySampler.sample','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get'
'com.atlantbh.jmeter.plugins.hbasecomponents.samplers.HBaseScanSampler.sample','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.filter.PageFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator'
'com.atlantbh.jmeter.plugins.hbasecomponents.samplers.HBaseScanSampler.parseFilterString','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter'
'com.impetus.client.hbase.schemamanager.HBaseSchemaManager.update','org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HTableDescriptor.getNameAsString org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.HTableDescriptor.getColumnFamilies org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.client.HBaseAdmin.addColumn org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.client.HBaseAdmin.addColumn org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'com.impetus.client.hbase.schemamanager.HBaseSchemaManager.validate','org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HTableDescriptor.getColumnFamilies org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.HTableDescriptor.getColumnFamilies org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString'
'com.impetus.client.hbase.schemamanager.HBaseSchemaManager.create','org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'com.impetus.client.hbase.schemamanager.HBaseSchemaManager.dropSchema','org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.TableNotFoundException.getMessage'
'com.impetus.client.hbase.schemamanager.HBaseSchemaManager.initiateClient','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.MasterNotRunningException.getMessage org.apache.hadoop.hbase.ZooKeeperConnectionException.getMessage'
'com.impetus.client.hbase.schemamanager.HBaseSchemaManager.getTableMetaData','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.setValue'
'com.impetus.client.hbase.schemamanager.HBaseSchemaManager.setColumnFamilyProperties','org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.setValue org.apache.hadoop.hbase.HColumnDescriptor.setTimeToLive org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.setMinVersions org.apache.hadoop.hbase.HColumnDescriptor.setCompactionCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType'
'org.apache.whirr.service.hbase.integration.HBaseServiceTest.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.whirr.service.hbase.integration.HBaseServiceTest.test','org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.<init> org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.<init> org.apache.hadoop.hbase.thrift.generated.Mutation.<init> org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.flume.sink.hbase.HBaseSink.HBaseSink','org.apache.hadoop.hbase.HBaseConfiguration.create'
'org.apache.flume.sink.hbase.HBaseSink.start','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.client.HTable.getTableDescriptor org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.flume.sink.hbase.HBaseSink.stop','org.apache.hadoop.hbase.client.HTable.close'
'org.apache.flume.sink.hbase.HBaseSink.putEventsAndCommit','org.apache.hadoop.hbase.client.HTable.batch org.apache.hadoop.hbase.client.HTable.increment'
'com.manning.hip.ch2.HBaseSinkMapReduce.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch2.HBaseSinkMapReduce.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.cloudera.flume.hbase.HBaseSink.HBaseSink','org.apache.hadoop.hbase.HBaseConfiguration.create'
'com.cloudera.flume.hbase.HBaseSink.append','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.setWriteToWAL org.apache.hadoop.hbase.client.HTable.put'
'com.cloudera.flume.hbase.HBaseSink.close','org.apache.hadoop.hbase.client.HTable.close'
'com.cloudera.flume.hbase.HBaseSink.open','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.client.HTable.setWriteBufferSize'
'com.cloudera.flume.hbase.HBaseSink.validateColFams','org.apache.hadoop.hbase.client.HTable.getTableDescriptor'
'com.cloudera.flume.hbase.HBaseSink.build','org.apache.hadoop.hbase.HBaseConfiguration.create'
'com.manning.hip.ch2.HBaseSourceMapReduce.map','org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.io.Text.set org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch2.HBaseSourceMapReduce.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch2.HBaseSourceSinkMapReduce.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.client.Put.add'
'com.manning.hip.ch2.HBaseSourceSinkMapReduce.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'.HBaseStationCli.getStationInfo','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get'
'.HBaseStationCli.getValue','org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'.HBaseStationCli.run','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HTable.<init>'
'.HBaseStationCli.main','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseStorageManager.HBaseStorageManager','org.apache.hadoop.hbase.HBaseConfiguration.create'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseStorageManager.openDatabase','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HTableDescriptor.getFamily org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.modifyTable org.apache.hadoop.hbase.client.HBaseAdmin.enableTable'
'com.thinkaurelius.titan.diskstorage.hbase.HBaseStorageManager.clearStorage','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setBatch org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'com.infochimps.hbase.pig.HBaseStorage.HBaseStorage','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toByteArrays org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.get'
'com.infochimps.hbase.pig.HBaseStorage.initScan','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Bytes.toBytesBinary org.apache.hadoop.hbase.util.Bytes.toBytesBinary org.apache.hadoop.hbase.util.Bytes.toBytesBinary org.apache.hadoop.hbase.util.Bytes.toBytesBinary'
'com.infochimps.hbase.pig.HBaseStorage.addFilter','org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.toString org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.client.Scan.getFilter org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.BinaryComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.setFilter'
'com.infochimps.hbase.pig.HBaseStorage.getNext','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentKey org.apache.hadoop.mapreduce.RecordReader.getCurrentValue org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Result.getFamilyMap'
'com.infochimps.hbase.pig.HBaseStorage.setLocation','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setScannerCaching org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'com.infochimps.hbase.pig.HBaseStorage.convertScanToString','org.apache.hadoop.hbase.client.Scan.write org.apache.hadoop.hbase.util.Base64.encodeBytes'
'com.infochimps.hbase.pig.HBaseStorage.getOutputFormat','org.apache.hadoop.hbase.mapreduce.TableOutputFormat.<init>'
'com.infochimps.hbase.pig.HBaseStorage.putNext','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.setWriteToWAL org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.isEmpty org.apache.hadoop.mapreduce.RecordWriter.write'
'com.infochimps.hbase.pig.HBaseStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.infochimps.hbase.pig.HBaseStorage.pushProjection','org.apache.hadoop.hbase.util.Bytes.toStringBinary'
'org.gora.hbase.store.HBaseStore.initialize','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.client.HTable.<init>'
'org.gora.hbase.store.HBaseStore.createSchema','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.gora.hbase.store.HBaseStore.deleteSchema','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HTable.getWriteBuffer org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'org.gora.hbase.store.HBaseStore.schemaExists','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.gora.hbase.store.HBaseStore.get','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get'
'org.gora.hbase.store.HBaseStore.put','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.delete'
'org.gora.hbase.store.HBaseStore.delete','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'org.gora.hbase.store.HBaseStore.deleteByQuery','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'org.gora.hbase.store.HBaseStore.flush','org.apache.hadoop.hbase.client.HTable.flushCommits'
'org.gora.hbase.store.HBaseStore.getPartitions','org.apache.hadoop.hbase.client.HTable.getStartEndKeys org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.client.HTable.getRegionLocation org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond'
'org.gora.hbase.store.HBaseStore.execute','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get'
'org.gora.hbase.store.HBaseStore.createScanner','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.HTable.getScanner'
'org.gora.hbase.store.HBaseStore.addFields','org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteColumn'
'org.gora.hbase.store.HBaseStore.addTimeRange','org.apache.hadoop.hbase.client.Get.setTimeStamp org.apache.hadoop.hbase.client.Get.setTimeRange'
'org.gora.hbase.store.HBaseStore.newInstance','org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getNoVersionMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.client.Result.getValue'
'org.gora.hbase.store.HBaseStore.close','org.apache.hadoop.hbase.client.HTable.close'
'org.apache.gora.hbase.store.HBaseStore.initialize','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.gora.hbase.store.HBaseStore.createSchema','org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.apache.gora.hbase.store.HBaseStore.deleteSchema','org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'org.apache.gora.hbase.store.HBaseStore.schemaExists','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.apache.gora.hbase.store.HBaseStore.get','org.apache.hadoop.hbase.client.Get.<init>'
'org.apache.gora.hbase.store.HBaseStore.put','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add'
'org.apache.gora.hbase.store.HBaseStore.delete','org.apache.hadoop.hbase.client.Delete.<init>'
'org.apache.gora.hbase.store.HBaseStore.deleteByQuery','org.apache.hadoop.hbase.client.Delete.<init>'
'org.apache.gora.hbase.store.HBaseStore.getPartitions','org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond'
'org.apache.gora.hbase.store.HBaseStore.execute','org.apache.hadoop.hbase.client.Get.<init>'
'org.apache.gora.hbase.store.HBaseStore.createScanner','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow'
'org.apache.gora.hbase.store.HBaseStore.addFields','org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteColumn'
'org.apache.gora.hbase.store.HBaseStore.addTimeRange','org.apache.hadoop.hbase.client.Get.setTimeStamp org.apache.hadoop.hbase.client.Get.setTimeRange'
'org.apache.gora.hbase.store.HBaseStore.newInstance','org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getNoVersionMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.client.Result.getValue'
'com.ripariandata.timberwolf.writer.hbase.HBaseTable.HBaseTable','org.apache.hadoop.hbase.client.HTableInterface.getTableName org.apache.hadoop.hbase.util.Bytes.toString'
'com.ripariandata.timberwolf.writer.hbase.HBaseTable.get','org.apache.hadoop.hbase.client.HTableInterface.get'
'com.ripariandata.timberwolf.writer.hbase.HBaseTable.flush','org.apache.hadoop.hbase.client.HTableInterface.put'
'com.ripariandata.timberwolf.writer.hbase.HBaseTable.close','org.apache.hadoop.hbase.client.HTableInterface.close'
'com.bah.culvert.tableadapters.HBaseTableAdapterIT.setup','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getMiniHBaseCluster'
'com.bah.culvert.tableadapters.HBaseTableAdapterIT.cleanupTable','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'com.bah.culvert.tableadapters.HBaseTableAdapterIT.tearDown','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat.getSplits','org.apache.hadoop.hbase.mapreduce.TableSplit.getStartRow org.apache.hadoop.hbase.mapreduce.TableSplit.getEndRow'
'org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat.skipRegion','org.apache.hadoop.hbase.filter.BinaryComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.filter.RowFilter.filterRowKey'
'org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat.HBaseTableRecordReader.setScan','org.apache.hadoop.hbase.client.Scan.getStartRow org.apache.hadoop.hbase.client.Scan.getStopRow org.apache.hadoop.hbase.util.Bytes.padTail org.apache.hadoop.hbase.util.Bytes.padTail org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.add'
'org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat.HBaseTableRecordReader.getProgress','org.apache.hadoop.hbase.util.Bytes.padTail org.apache.hadoop.hbase.util.Bytes.padTail org.apache.hadoop.hbase.util.Bytes.add'
'pl.edu.icm.coansys.commons.hbase.HBaseTableUtils.isTableCreated','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'pl.edu.icm.coansys.commons.hbase.HBaseTableUtils.dropTable','org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'pl.edu.icm.coansys.commons.hbase.HBaseTableUtils.truncateTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'pl.edu.icm.coansys.commons.hbase.HBaseTableUtils.createSimpleTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'pl.edu.icm.coansys.commons.hbase.HBaseTableUtils.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'com.twitter.maple.hbase.HBaseTapCollector.HBaseTapCollector','org.apache.hadoop.mapred.JobConf.<init>'
'com.twitter.maple.hbase.HBaseTapCollector.initialize','org.apache.hadoop.mapred.JobConf.getOutputFormat org.apache.hadoop.mapred.OutputFormat.getClass org.apache.hadoop.mapred.OutputFormat.getRecordWriter'
'com.twitter.maple.hbase.HBaseTapCollector.close','org.apache.hadoop.mapreduce.RecordWriter.close'
'com.twitter.maple.hbase.HBaseTapCollector.collect','org.apache.hadoop.mapreduce.RecordWriter.write'
'org.apache.crunch.io.hbase.HBaseTarget.accept','org.apache.hadoop.hbase.client.Put.equals'
'org.apache.crunch.io.hbase.HBaseTarget.configureForMapReduce','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJars'
'.HBaseTemperatureCli.getStationObservations','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.client.ResultScanner.close'
'.HBaseTemperatureCli.run','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HTable.<init>'
'.HBaseTemperatureCli.main','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.util.ToolRunner.run'
'.HBaseTemperatureImporter.HBaseTemperatureMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'.HBaseTemperatureImporter.HBaseTemperatureMapper.configure','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HTable.<init>'
'.HBaseTemperatureImporter.HBaseTemperatureMapper.close','org.apache.hadoop.hbase.client.HTable.close'
'.HBaseTemperatureImporter.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'.HBaseTemperatureImporter.main','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.util.ToolRunner.run'
'cascading.hbase.HBaseTestCase.loadTable','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.io.BatchUpdate.<init> org.apache.hadoop.hbase.io.BatchUpdate.put org.apache.hadoop.hbase.client.HTable.commit org.apache.hadoop.hbase.client.HTable.close'
'cascading.hbase.HBaseTestCase.verify','org.apache.hadoop.hbase.util.Bytes.toByteArrays org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.io.RowResult.get org.apache.hadoop.hbase.client.Scanner.close'
'org.hbase.tdg.HBaseTesting.setUpEnvironment','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster'
'org.hbase.tdg.HBaseTesting.tearDownEnvironment','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'org.hbase.tdg.HBaseTesting.setUp','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'org.hbase.tdg.HBaseTesting.tearDown','org.apache.hadoop.hbase.client.HBaseAdmin.isTableAvailable org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'org.hbase.tdg.HBaseTesting.testRegionObserver','org.apache.hadoop.hbase.HTableDescriptor.addCoprocessor org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getMap'
'org.hbase.tdg.HBaseTesting.testMasterObserver','org.apache.hadoop.hbase.client.HBaseAdmin.getMasterCoprocessors org.apache.hadoop.hbase.HBaseTestingUtility.getTestFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileUtil.stat2Paths'
'radlab.rain.workload.hbase.HBaseTransport.HBaseTransport','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt'
'radlab.rain.workload.hbase.HBaseTransport.initialize','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.client.HTable.setWriteBufferSize org.apache.hadoop.hbase.client.HTable.setOperationTimeout'
'radlab.rain.workload.hbase.HBaseTransport.createTable','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'radlab.rain.workload.hbase.HBaseTransport.deleteTable','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'radlab.rain.workload.hbase.HBaseTransport.closeTable','org.apache.hadoop.hbase.client.HTable.close'
'radlab.rain.workload.hbase.HBaseTransport.get','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getValue'
'radlab.rain.workload.hbase.HBaseTransport.put','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'radlab.rain.workload.hbase.HBaseTransport.putMany','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'radlab.rain.workload.hbase.HBaseTransport.scan','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.ResultScanner.close'
'radlab.rain.workload.hbase.HBaseTransport.flushCommits','org.apache.hadoop.hbase.client.HTable.flushCommits'
'org.lilyproject.repository.impl.HBaseTypeManager.createRecordType','org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.checkAndPut'
'org.lilyproject.repository.impl.HBaseTypeManager.putMixinOnRecordType','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseTypeManager.updateRecordType','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.exists org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.checkAndPut'
'org.lilyproject.repository.impl.HBaseTypeManager.updateName','org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseTypeManager.getRecordTypeByIdWithoutCache','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong'
'org.lilyproject.repository.impl.HBaseTypeManager.updateFieldTypeEntries','org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseTypeManager.putFieldTypeEntry','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.exists org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseTypeManager.updateMixins','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.HBaseTypeManager.extractFieldTypeEntries','org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.client.Result.getFamilyMap'
'org.lilyproject.repository.impl.HBaseTypeManager.extractMixins','org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong'
'org.lilyproject.repository.impl.HBaseTypeManager.encodeFieldTypeEntry','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'org.lilyproject.repository.impl.HBaseTypeManager.decodeFieldTypeEntry','org.apache.hadoop.hbase.util.Bytes.toBoolean'
'org.lilyproject.repository.impl.HBaseTypeManager.createFieldType','org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.checkAndPut'
'org.lilyproject.repository.impl.HBaseTypeManager.updateFieldType','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.exists org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.checkAndPut'
'org.lilyproject.repository.impl.HBaseTypeManager.getFieldTypeByIdWithoutCache','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.repository.impl.HBaseTypeManager.getFieldTypesWithoutCache','org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.Result.getRow'
'org.lilyproject.repository.impl.HBaseTypeManager.getRecordTypesWithoutCache','org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.Result.getRow'
'org.lilyproject.repository.impl.HBaseTypeManager.encodeName','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'org.lilyproject.repository.impl.HBaseTypeManager.decodeName','org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.repository.impl.HBaseTypeManager.idToBytes','org.apache.hadoop.hbase.util.Bytes.putLong org.apache.hadoop.hbase.util.Bytes.putLong org.apache.hadoop.hbase.util.Bytes.putLong org.apache.hadoop.hbase.util.Bytes.putLong'
'org.lilyproject.repository.impl.HBaseTypeManager.idFromBytes','org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toLong'
'org.lilyproject.repository.impl.HBaseTypeManager.getValidUUID','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.exists org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue'
'org.apache.james.mailbox.hbase.mail.HBaseUidProvider.lastUid','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.HTable.close'
'org.apache.james.mailbox.hbase.mail.HBaseUidProvider.nextUid','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.HTable.close'
'com.ripariandata.timberwolf.writer.hbase.HBaseUserFolderSyncStateStorageTest.statePut','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.impetus.client.hbase.config.HBaseUserTest.test','org.apache.hadoop.hbase.HTableDescriptor.getColumnFamilies org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.io.hfile.Compression.Algorithm.valueOf org.apache.hadoop.hbase.HColumnDescriptor.getCompactionCompressionType org.apache.hadoop.hbase.HColumnDescriptor.getTimeToLive org.apache.hadoop.hbase.io.hfile.Compression.Algorithm.valueOf org.apache.hadoop.hbase.HColumnDescriptor.getCompressionType org.apache.hadoop.hbase.HColumnDescriptor.getMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.getMinVersions org.apache.hadoop.hbase.HColumnDescriptor.getNameAsString org.apache.hadoop.hbase.io.hfile.Compression.Algorithm.valueOf org.apache.hadoop.hbase.HColumnDescriptor.getCompactionCompressionType org.apache.hadoop.hbase.HColumnDescriptor.getTimeToLive org.apache.hadoop.hbase.io.hfile.Compression.Algorithm.valueOf org.apache.hadoop.hbase.HColumnDescriptor.getCompressionType org.apache.hadoop.hbase.HColumnDescriptor.getMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.getMinVersions'
'org.apache.hcatalog.hbase.HBaseUtil.parseColumnMapping','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.hcatalog.hbase.HBaseUtil.addHBaseDelegationToken','org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled org.apache.hadoop.hbase.security.User.getCurrent'
'tv.floe.lumberyard.hbase.isax.index.HBaseUtils.DeleteRow','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.close'
'tv.floe.lumberyard.hbase.isax.index.HBaseUtils.Put','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'tv.floe.lumberyard.hbase.isax.index.HBaseUtils.Get','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.Result.value'
'tv.floe.lumberyard.hbase.isax.index.HBaseUtils.CreateNewTable','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'tv.floe.lumberyard.hbase.isax.index.HBaseUtils.DoesTableExist','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'tv.floe.lumberyard.hbase.isax.index.HBaseUtils.DropTable','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'tv.floe.lumberyard.hbase.isax.index.HBaseUtils.ListTables','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.MasterNotRunningException.printStackTrace org.apache.hadoop.hbase.ZooKeeperConnectionException.printStackTrace org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.HTableDescriptor.getNameAsString'
'org.apache.james.mailbox.hbase.HBaseUtilsTest.testMailboxToPut','org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.has'
'org.apache.james.mailbox.hbase.HBaseUtilsTest.testSubscriptionToPut','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.has'
'org.apache.james.mailbox.hbase.HBaseUtilsTest.testFlagsToPut','org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.client.Put.has'
'org.apache.james.mailbox.hbase.HBaseUtils.mailboxFromResult','org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toLong'
'org.apache.james.mailbox.hbase.HBaseUtils.mailboxRowKey','org.apache.hadoop.hbase.util.Bytes.putLong org.apache.hadoop.hbase.util.Bytes.putLong'
'org.apache.james.mailbox.hbase.HBaseUtils.UUIDFromRowKey','org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toLong'
'org.apache.james.mailbox.hbase.HBaseUtils.toPut','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.apache.james.mailbox.hbase.HBaseUtils.metadataToPut','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add'
'org.apache.james.mailbox.hbase.HBaseUtils.messageRowKey','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'org.apache.james.mailbox.hbase.HBaseUtils.customMessageRowKey','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'org.apache.james.mailbox.hbase.HBaseUtils.messageMetaFromResult','org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.startsWith org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.startsWith org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.startsWith org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toLong'
'org.apache.james.mailbox.hbase.HBaseUtils.flagsToPut','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add'
'org.apache.james.mailbox.hbase.HBaseUtils.flagsToDelete','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteColumn'
'com.impetus.client.hbase.utils.HBaseUtils.getBytes','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.impetus.client.hbase.utils.HBaseUtils.fromBytes','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toBoolean org.apache.hadoop.hbase.util.Bytes.toDouble org.apache.hadoop.hbase.util.Bytes.toFloat org.apache.hadoop.hbase.util.Bytes.toShort org.apache.hadoop.hbase.util.Bytes.toBigDecimal'
'org.apache.giraph.io.hbase.HBaseVertexInputFormat.setConf','org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf'
'org.apache.giraph.io.hbase.HBaseVertexInputFormat.HBaseVertexReader.initialize','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.initialize'
'org.apache.giraph.io.hbase.HBaseVertexInputFormat.HBaseVertexReader.close','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.close'
'org.apache.giraph.io.hbase.HBaseVertexInputFormat.HBaseVertexReader.getProgress','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result>.getProgress'
'org.apache.giraph.io.hbase.HBaseVertexInputFormat.getSplits','org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits'
'com.manning.hip.ch2.HBaseWriteAvroStock.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.client.HTable.setWriteBufferSize org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.close'
'com.manning.hip.ch2.HBaseWriteAvroStock.createTableAndColumn','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.archive.modules.writer.HBaseWriterProcessor.isRecordNew','org.apache.hadoop.hbase.util.Keying.createKey org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.HTable.get'
'com.nesscomputing.hbase.HBaseWriter.flushToHBase','org.apache.hadoop.hbase.client.HTable.put'
'com.impetus.client.hbase.service.HBaseWriter.writeColumns','org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.impetus.client.hbase.service.HBaseWriter.writeColumn','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.impetus.client.hbase.service.HBaseWriter.writeRelations','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.impetus.client.hbase.service.HBaseWriter.writeForeignKeys','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.impetus.client.hbase.service.HBaseWriter.delete','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'com.impetus.client.hbase.service.HBaseWriter.persistRows','org.apache.hadoop.hbase.client.HTable.put'
'com.impetus.client.hbase.service.HBaseWriter.preparePut','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.hbasene.index.util.HBaseneUtil.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.putBytes'
'org.hbasene.index.util.HBaseneUtil.toOpenBitSet','org.apache.hadoop.hbase.util.Bytes.toLong'
'org.hbasene.index.util.HBaseneUtil.createTermVectorQualifier','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanRow','org.apache.hadoop.hbase.KeyValue.getRow'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.addPredecessor','org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getValue'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.getBeanTable','org.apache.hadoop.hbase.client.HTable.<init>'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.setReferences','org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getValue'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.getReferenceKeyValue','org.apache.hadoop.hbase.KeyValue.<init>'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.getPropertiesKeyValue','org.apache.hadoop.hbase.KeyValue.<init>'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.getProperties','org.apache.hadoop.hbase.KeyValue.getValue'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.getReferences','org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getValue'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.merge','org.apache.hadoop.hbase.KeyValue.deepCopy'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.set','org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.<init>'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.getPropertyName','org.apache.hadoop.hbase.KeyValue.getQualifier'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.isProperty','org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.util.Bytes.equals'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.isReference','org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.util.Bytes.equals'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.isPredecessor','org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.util.Bytes.equals'
'org.deephacks.tools4j.config.internal.core.hbase.HBeanRow.HBeanBytes.setColumns','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Get.setFilter org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Get.addFamily'
'org.apache.hcatalog.pig.HCatBaseLoader.getNext','org.apache.hadoop.mapreduce.RecordReader<?,?>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<?,?>.getCurrentValue'
'org.apache.hcatalog.pig.HCatBaseLoader.getSizeInBytes','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileStatus.getLen'
'org.apache.hcatalog.pig.HCatBaseStorer.putNext','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.WritableComparable<?>,org.apache.hcatalog.data.HCatRecord>.write'
'org.apache.hcatalog.api.HCatClient.create','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hive.common.JavaUtils.getClassLoader'
'org.apache.hcatalog.common.HCatContext.setupHCatContext','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.hcatalog.cli.HCatDriver.run','org.apache.hadoop.hive.ql.CommandNeedRetryException.toString org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.<init> org.apache.hadoop.hive.ql.session.SessionState.get org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.<init> org.apache.hadoop.hive.ql.session.SessionState.getConf org.apache.hadoop.hive.ql.session.SessionState.getConf'
'org.apache.hcatalog.cli.HCatDriver.setFSPermsNGrp','org.apache.hadoop.hive.ql.session.SessionState.getConf org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.hive.ql.metadata.Hive.get org.apache.hadoop.hive.ql.metadata.Hive.getTable org.apache.hadoop.hive.ql.metadata.Table.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.setOwner org.apache.hadoop.hive.ql.metadata.Hive.dropTable org.apache.hadoop.hive.ql.metadata.Hive.get org.apache.hadoop.hive.metastore.Warehouse.<init> org.apache.hadoop.hive.ql.metadata.Hive.getDatabase org.apache.hadoop.hive.metastore.Warehouse.getDatabasePath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.setOwner org.apache.hadoop.hive.ql.metadata.Hive.get'
'org.apache.hcatalog.shims.HCatHadoopShims.Instance.selectShim','org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion'
'org.apache.hcatalog.shims.HCatHadoopShims20S.createTaskID','org.apache.hadoop.mapreduce.TaskID.<init>'
'org.apache.hcatalog.shims.HCatHadoopShims20S.createTaskAttemptID','org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'org.apache.hcatalog.shims.HCatHadoopShims20S.createTaskAttemptContext','org.apache.hadoop.mapreduce.TaskAttemptContext.<init>'
'org.apache.hcatalog.shims.HCatHadoopShims20S.createJobContext','org.apache.hadoop.mapreduce.JobContext.<init>'
'org.apache.hcatalog.shims.HCatHadoopShims20S.commitJob','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.OutputFormat.getOutputCommitter'
'org.apache.hcatalog.shims.HCatHadoopShims20S.abortJob','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.OutputFormat.getOutputCommitter'
'org.apache.hcatalog.shims.HCatHadoopShims20S.getResourceManagerAddress','org.apache.hadoop.mapred.JobTracker.getAddress'
'org.apache.hcatalog.shims.HCatHadoopShims23.createTaskID','org.apache.hadoop.mapreduce.TaskID.<init>'
'org.apache.hcatalog.shims.HCatHadoopShims23.createTaskAttemptID','org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'org.apache.hcatalog.shims.HCatHadoopShims23.createTaskAttemptContext','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.<init>'
'org.apache.hcatalog.shims.HCatHadoopShims23.createJobContext','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.task.JobContextImpl.<init> org.apache.hadoop.mapred.JobConf.<init>'
'org.apache.hcatalog.shims.HCatHadoopShims23.getResourceManagerAddress','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.net.NetUtils.createSocketAddr'
'org.apache.hcatalog.mapreduce.HCatOutputFormat.setOutput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listIndexNames org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getIndex org.apache.hadoop.hive.metastore.api.Index.isDeferredRebuild org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.metastore.api.StorageDescriptor.isCompressed org.apache.hadoop.hive.metastore.api.StorageDescriptor.getBucketCols org.apache.hadoop.hive.metastore.api.StorageDescriptor.getBucketCols org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSortCols org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSortCols org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.hive.metastore.api.FieldSchema.getName org.apache.hadoop.hive.metastore.api.FieldSchema.getName org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.ql.metadata.Table.getParameters org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys org.apache.hadoop.hive.metastore.api.FieldSchema.getName org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.hive.ql.metadata.Table.getTTable org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.getDefault org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.permission.FsPermission.setUMask'
'org.apache.hcatalog.mapreduce.HCatOutputFormat.setSchema','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.hcatalog.mapreduce.HCatOutputFormat.getMaxDynamicPartitions','org.apache.hadoop.hive.conf.HiveConf.getIntVar'
'org.apache.hcatalog.mapreduce.HCatOutputFormat.getHarRequested','org.apache.hadoop.hive.conf.HiveConf.getBoolVar'
'org.apache.hcatalog.api.HCatPartition.HCatPartition','org.apache.hadoop.hive.metastore.api.Partition.getTableName org.apache.hadoop.hive.metastore.api.Partition.getDbName org.apache.hadoop.hive.metastore.api.Partition.getCreateTime org.apache.hadoop.hive.metastore.api.Partition.getLastAccessTime org.apache.hadoop.hive.metastore.api.Partition.getParameters org.apache.hadoop.hive.metastore.api.Partition.getValues org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.hive.metastore.api.StorageDescriptor.getCols'
'org.apache.hcatalog.api.HCatPartition.getInputFormat','org.apache.hadoop.hive.metastore.api.StorageDescriptor.getInputFormat'
'org.apache.hcatalog.api.HCatPartition.getOutputFormat','org.apache.hadoop.hive.metastore.api.StorageDescriptor.getOutputFormat'
'org.apache.hcatalog.api.HCatPartition.getStorageHandler','org.apache.hadoop.hive.metastore.api.StorageDescriptor.getParameters'
'org.apache.hcatalog.api.HCatPartition.getLocation','org.apache.hadoop.hive.metastore.api.StorageDescriptor.getLocation'
'org.apache.hcatalog.api.HCatPartition.getSerDe','org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSerdeInfo'
'org.apache.hcatalog.api.HCatPartition.getBucketCols','org.apache.hadoop.hive.metastore.api.StorageDescriptor.getBucketCols'
'org.apache.hcatalog.api.HCatPartition.getNumBuckets','org.apache.hadoop.hive.metastore.api.StorageDescriptor.getNumBuckets'
'org.apache.hcatalog.api.HCatPartition.getSortCols','org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSortCols'
'org.apache.hcatalog.data.HCatRecordSerDe.initialize','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfoFromTypeString'
'org.apache.hcatalog.data.HCatRecordSerDe.deserialize','org.apache.hadoop.hive.serde2.SerDeException.<init>'
'org.apache.hcatalog.data.HCatRecordSerDe.serialize','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.SerDeException.<init>'
'org.apache.hcatalog.data.HCatRecordSerDe.serializeStruct','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldsDataAsList'
'org.apache.hcatalog.data.HCatRecordSerDe.serializeField','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.SerDeException.<init>'
'org.apache.hcatalog.data.HCatRecordSerDe.serializeMap','org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapKeyObjectInspector org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapValueObjectInspector org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMap'
'org.apache.hcatalog.data.HCatRecordSerDe.serializeList','org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getList org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElementObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.SerDeException.<init>'
'org.apache.hcatalog.data.HCatRecordSerDe.serializePrimitiveField','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject'
'org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.getDbName','org.apache.hadoop.hive.ql.metadata.Hive.getCurrentDatabase'
'org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.getAuthProvider','org.apache.hadoop.hive.ql.session.SessionState.get'
'org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorizeDDL','org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext.getConf org.apache.hadoop.hive.conf.HiveConf.getBoolVar org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext.getHive org.apache.hadoop.hive.ql.exec.Task<? extends java.io.Serializable>.getWork org.apache.hadoop.hive.ql.exec.Task<? extends java.io.Serializable>.getWork org.apache.hadoop.hive.ql.parse.SemanticException.<init>'
'org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorize','org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.authorize org.apache.hadoop.hive.ql.parse.SemanticException.<init> org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.authorize org.apache.hadoop.hive.ql.parse.SemanticException.<init> org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.authorize org.apache.hadoop.hive.ql.parse.SemanticException.<init> org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.authorize org.apache.hadoop.hive.ql.parse.SemanticException.<init>'
'org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorizeTable','org.apache.hadoop.hive.ql.metadata.Hive.getTable'
'org.apache.hcatalog.pig.HCatStorer.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.getCredentials org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getCredentials'
'org.apache.hcatalog.mapreduce.HCatTableInfo.getTableLocation','org.apache.hadoop.hive.metastore.api.Table.getSd'
'org.apache.hcatalog.mapreduce.HCatTableInfo.valueOf','org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.metastore.api.Table.getParameters org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.getTableName'
'org.apache.hcatalog.mapreduce.HCatTableInfo.equals','org.apache.hadoop.hive.metastore.api.Table.equals'
'org.apache.hcatalog.mapreduce.HCatTableInfo.hashCode','org.apache.hadoop.hive.metastore.api.Table.hashCode'
'org.apache.hcatalog.utils.HCatTypeCheckHive.evaluate','org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'org.apache.hcatalog.utils.HCatTypeCheckHive.getJavaObject','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getList org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElementObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMap org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapKeyObjectInspector org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapValueObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldsDataAsList org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject'
'org.apache.hcatalog.utils.HCatTypeCheckHive.initialize','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector'
'org.apache.hcatalog.data.transfer.HCatWriter.HCatWriter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.backend.hadoop.datastorage.HConfiguration.HConfiguration','org.apache.hadoop.conf.Configuration.iterator'
'org.apache.pig.backend.hadoop.datastorage.HConfiguration.getConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'eu.scape_project.tb.wc.archd.hd.HDApp.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'eu.scape_project.tb.wc.archd.hd.HDApp.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'eu.scape_project.tb.wc.archd.hd.HDApp.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'eu.scape_project.tb.wc.archd.hd.HDApp.Reduce.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.getCurrentUserFSObjects','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.copy','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isDirectory org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileUtil.copy'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.deleteFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.deleteFolder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.renameFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.renameFolder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.moveFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.makeDirectory','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.createFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.getPermission','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.setPermission','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.setGroup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.setOwner'
'org.wso2.carbon.hdfs.mgt.HDFSAdmin.setOwner','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.setOwner'
'org.archive.wayback.resourceindex.ziplines.HDFSBlockLoader.init','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get'
'org.archive.wayback.resourceindex.ziplines.HDFSBlockLoader.getBlock','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataInputStream.close'
'org.archive.wayback.resourceindex.ziplines.HDFSBlockLoader.setDefaultFSURI','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get'
'org.archive.hadoop.cdx.HDFSBlockLoader.readBlock','org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.readFully'
'com.asakusafw.cleaner.main.HDFSCleanerTest.createPath','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.cleaner.main.StubHDFSCleaner.StubHDFSCleaner','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'com.asakusafw.cleaner.main.StubHDFSCleaner.createPath','org.apache.hadoop.fs.Path.<init>'
'com.griddynamics.jagger.storage.fs.hdfs.HDFSClientBean.run','org.apache.hadoop.fs.FileSystem.get'
'com.griddynamics.jagger.storage.fs.hdfs.HDFSClientBean.close','org.apache.hadoop.fs.FileSystem.close'
'org.wso2.carbon.hdfs.datanode.HDFSDataNode.HDFSDataNode','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode org.apache.hadoop.hdfs.server.datanode.DataNode.runDatanodeDaemon'
'org.apache.flume.sink.hdfs.HDFSDataStream.open','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.append org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.flume.sink.hdfs.HDFSDataStream.sync','org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.sync'
'org.apache.flume.sink.hdfs.HDFSDataStream.close','org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.sync org.apache.hadoop.fs.FSDataOutputStream.close'
'com.griddynamics.jagger.storage.fs.hdfs.HDFSDatanodeServer.run','org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode org.apache.hadoop.hdfs.server.datanode.DataNode.run'
'com.griddynamics.jagger.storage.fs.hdfs.HDFSDatanodeServer.shutdown','org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown'
'com.twitter.elephanttwin.lucene.HDFSDirectory.HDFSDirectory','org.apache.hadoop.fs.Path.<init>'
'com.twitter.elephanttwin.lucene.HDFSDirectory.close','org.apache.hadoop.fs.FileSystem.close'
'com.twitter.elephanttwin.lucene.HDFSDirectory.fileExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.twitter.elephanttwin.lucene.HDFSDirectory.fileLength','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.twitter.elephanttwin.lucene.HDFSDirectory.fileModified','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.twitter.elephanttwin.lucene.HDFSDirectory.listAll','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.twitter.elephanttwin.lucene.HDFSDirectory.openInput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'com.twitter.elephanttwin.lucene.HDFSDirectory.HDFSIndexInput.HDFSIndexInput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.twitter.elephanttwin.lucene.HDFSDirectory.HDFSIndexInput.getFilePointer','org.apache.hadoop.fs.FSDataInputStream.getPos'
'com.twitter.elephanttwin.lucene.HDFSDirectory.HDFSIndexInput.length','org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.twitter.elephanttwin.lucene.HDFSDirectory.HDFSIndexInput.readByte','org.apache.hadoop.fs.FSDataInputStream.readByte'
'com.twitter.elephanttwin.lucene.HDFSDirectory.HDFSIndexInput.readBytes','org.apache.hadoop.fs.FSDataInputStream.readFully'
'com.twitter.elephanttwin.lucene.HDFSDirectory.HDFSIndexInput.seek','org.apache.hadoop.fs.FSDataInputStream.seek'
'utilities.filesystem.hdfs.HDFSDriver.HDFSDriver','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'utilities.filesystem.hdfs.HDFSDriver.openR','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'utilities.filesystem.hdfs.HDFSDriver.openW','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'utilities.filesystem.hdfs.HDFSDriver.exists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'utilities.filesystem.hdfs.HDFSDriver.length','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen'
'utilities.filesystem.hdfs.HDFSDriver.move','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'utilities.filesystem.hdfs.HDFSDriver.remove','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'utilities.filesystem.hdfs.HDFSDriver.getBlockSize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getBlockSize'
'utilities.filesystem.hdfs.HDFSDriver.getFileBlockLocations','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileBlockLocations org.apache.hadoop.fs.BlockLocation.getHosts org.apache.hadoop.fs.BlockLocation.getOffset org.apache.hadoop.fs.BlockLocation.getLength'
'org.apache.flume.sink.hdfs.HDFSEventSink.getCodec','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses'
'org.apache.flume.sink.hdfs.HDFSEventSink.authenticate','org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled org.apache.hadoop.security.SecurityUtil.getServerPrincipal org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.getUserName org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.getAuthenticationMethod org.apache.hadoop.security.UserGroupInformation.getUserName org.apache.hadoop.security.UserGroupInformation.isFromKeytab org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.getAuthenticationMethod org.apache.hadoop.security.UserGroupInformation.getUserName org.apache.hadoop.security.UserGroupInformation.isFromKeytab org.apache.hadoop.security.UserGroupInformation.getUserName'
'org.apache.flume.sink.hdfs.HDFSEventSink.kerberosLogin','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.getUserName org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab org.apache.hadoop.security.UserGroupInformation.getLoginUser'
'com.cloudera.hadoop.hdfs.nfs.nfs4.state.HDFSFile.close','org.apache.hadoop.fs.FSDataInputStream.close'
'com.cloudera.kitten.appmaster.util.HDFSFileFinderTest.setup','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem'
'com.cloudera.kitten.appmaster.util.HDFSFileFinderTest.tearDown','org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'com.cloudera.kitten.appmaster.util.HDFSFileFinderTest.testFileFinder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hdfs.DistributedFileSystem.copyFromLocalFile'
'org.commoncrawl.service.listcrawler.HDFSFileIndex.HDFSFileIndex','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile'
'org.commoncrawl.service.listcrawler.HDFSFileIndex.getIndexTimestamp','org.apache.hadoop.fs.Path.getName'
'org.commoncrawl.service.listcrawler.HDFSFileIndex.findItem','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.commoncrawl.service.listcrawler.HDFSFileIndex.main','org.apache.hadoop.util.StringUtils.byteToHexString'
'com.senseidb.util.HDFSIndexCopier.copy','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.copyToLocalFile'
'org.archive.hadoop.util.HDFSMove.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.archive.hadoop.util.HDFSMove.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'com.griddynamics.jagger.storage.fs.hdfs.HDFSNamenodeServer.start','org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode org.apache.hadoop.hdfs.server.namenode.NameNode.setSafeMode'
'com.griddynamics.jagger.storage.fs.hdfs.HDFSNamenodeServer.shutdown','org.apache.hadoop.hdfs.server.namenode.NameNode.stop'
'com.griddynamics.jagger.storage.fs.hdfs.HDFSNamenodeServer.formatStorage','org.apache.hadoop.hdfs.server.namenode.NameNode.format'
'io.beancounter.profiler.hdfs.HDFSProfileWriter.init','org.apache.hadoop.hdfs.DistributedFileSystem.initialize org.apache.hadoop.hdfs.DistributedFileSystem.getClient'
'io.beancounter.profiler.hdfs.HDFSProfileWriter.close','org.apache.hadoop.hdfs.DFSClient.close org.apache.hadoop.hdfs.DistributedFileSystem.close'
'io.beancounter.profiler.hdfs.HDFSProfileWriter.createApplicationDir','org.apache.hadoop.hdfs.DFSClient.mkdirs'
'io.beancounter.profiler.hdfs.HDFSProfileWriter.write','org.apache.hadoop.hdfs.DFSClient.create'
'io.beancounter.profiler.hdfs.HDFSProfileWriter.append','org.apache.hadoop.hdfs.DFSClient.append'
'io.beancounter.profiler.hdfs.HDFSProfileWriter.checkIfApplicationDirExists','org.apache.hadoop.hdfs.DFSClient.exists'
'io.beancounter.profiler.hdfs.HDFSProfileWriter.checkIfUserFileExists','org.apache.hadoop.hdfs.DFSClient.exists'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.setUp','org.apache.hadoop.conf.Configuration.<init>'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.createNewHDFSProfileWriter','org.apache.hadoop.hdfs.DistributedFileSystem.<init> org.apache.hadoop.conf.Configuration.<init>'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.givenNoErrorsWhenClosingTheWriterThenCloseTheDFS','org.apache.hadoop.hdfs.DistributedFileSystem.getClient'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.givenErrorWhenClosingTheWriterThenThrowException','org.apache.hadoop.hdfs.DistributedFileSystem.getClient'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.writeUserProfileForNewApplication','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.create'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.givenErrorWhenWritingUserProfileForNewApplicationThenThrowException','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.create'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.givenErrorWhenClosingOutputStreamAfterWritingUserProfileForNewApplicationThenThrowException','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.create'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.writeUserProfileForExistingApplicationButNewUser','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.create'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.givenErrorWhenWritingUserProfileForExistingApplicationThenThrowException','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.create'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.givenErrorWhenClosingOutputStreamAfterWritingUserProfileForExistingApplicationThenThrowException','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.create'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.appendUserProfileForExistingApplication','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.append'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.givenErrorWhenAppendingUserProfileForExistingApplicationThenThrowException','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.append'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.givenErrorWhenClosingOutputStreamAfterAppendingUserProfileForExistingApplicationThenThrowException','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.append'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.appendingNewUserProfileSnapshotToSingleExistingUserProfile','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.append'
'io.beancounter.profiler.hdfs.HDFSProfileWriterTest.appendingNewUserProfileSnapshotToMultipleExistingUserProfiles','org.apache.hadoop.hdfs.DistributedFileSystem.getClient org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.exists org.apache.hadoop.hdfs.DFSClient.append'
'com.twitter.elephanttwin.lucene.retrieval.HDFSRetrievalDemo.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get'
'com.twitter.elephanttwin.lucene.retrieval.HDFSRetrievalDemo.main','org.apache.hadoop.util.ToolRunner.run'
'org.archive.util.binsearch.impl.HDFSSeekableLineReader.seek','org.apache.hadoop.fs.FSDataInputStream.seek'
'org.archive.util.binsearch.impl.HDFSSeekableLineReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'org.archive.util.binsearch.impl.HDFSSeekableLineReaderFactory.get','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.open'
'org.apache.flume.sink.hdfs.HDFSSequenceFile.open','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.append org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.hcatalog.templeton.tool.HDFSStorage.saveField','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.hcatalog.templeton.tool.HDFSStorage.getField','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.hcatalog.templeton.tool.HDFSStorage.getFields','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.hcatalog.templeton.tool.HDFSStorage.delete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.hcatalog.templeton.tool.HDFSStorage.getAllForType','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.hcatalog.templeton.tool.HDFSStorage.getAllForTypeAndKey','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.hcatalog.templeton.tool.HDFSStorage.openStorage','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get'
'org.archive.streamcontext.HDFSStream.doRead','org.apache.hadoop.fs.FSDataInputStream.read'
'org.archive.streamcontext.HDFSStream.doSeek','org.apache.hadoop.fs.FSDataInputStream.seek'
'org.archive.streamcontext.HDFSStream.doClose','org.apache.hadoop.fs.FSDataInputStream.close'
'storm.state.hdfs.HDFSUtils.normalizePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'storm.state.hdfs.HDFSUtils.getFS','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.RawLocalFileSystem.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.RawLocalFileSystem.initialize'
'storm.state.hdfs.HDFSUtils.getSortedVersions','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'storm.state.hdfs.HDFSUtils.clearDir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'storm.state.hdfs.HDFSUtils.mkdirs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'storm.state.hdfs.HDFSUtils.deleteFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.flume.sink.hdfs.HDFSWritableFormatter.makeByteWritable','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.set'
'org.apache.flume.sink.hdfs.HDFSWritableFormatter.getKey','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.flume.sink.hdfs.HDFSWritableFormatter.getBytes','org.apache.hadoop.io.BytesWritable.getBytes'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.init','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.close','org.apache.hadoop.fs.FileSystem.close'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.updateConfiguration','org.apache.hadoop.fs.FileSystem.getConf'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.getStatistics','org.apache.hadoop.fs.FileSystem.getUsed org.apache.hadoop.hdfs.DistributedFileSystem.getRawCapacity org.apache.hadoop.hdfs.DistributedFileSystem.getRawUsed'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.setActiveContainer','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.setWorkingDirectory'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.getActiveContainer','org.apache.hadoop.fs.FileSystem.getWorkingDirectory'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.isFile'
'org.apache.pig.backend.hadoop.datastorage.HDataStorage.asCollection','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.pig.backend.hadoop.datastorage.HDirectory.iterator','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.getClassLoader org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.hdfs.DistributedFileSystem.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.get'
'org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.recomputeProperties','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.iterator'
'com.infochimps.hbase.pig.HFileStorage.HFileStorage','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.infochimps.hbase.pig.HFileStorage.getOutputFormat','org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.<init>'
'com.infochimps.hbase.pig.HFileStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'com.infochimps.hbase.pig.HFileStorage.putNext','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.mapreduce.RecordWriter.write'
'com.infochimps.hbase.pig.HFileStorage.sortedKeyValues','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.<init> org.apache.hadoop.hbase.KeyValue.clone'
'.HFileWithMRTest.testJob','org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getTestFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hbase.HBaseTestingUtility.getTestFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.example.hits.HFormatterWG.HFormatMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect'
'edu.umd.cloud9.example.hits.HFormatterWG.HFormatReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect'
'edu.umd.cloud9.example.hits.HFormatterWG.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.hits.HFormatterWG.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.setStrings org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.example.hits.HFormatterWG.readStopList','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'edu.umd.cloud9.example.hits.HFormatterWG.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.hooka.alignment.hmm.HMM.writePartialCounts','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.hooka.alignment.PartialCountContainer>.collect'
'edu.umd.hooka.alignment.hmm.HMM.processTrainingInstance','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.cloud9.io.map.HMapKFWTest.testBasic','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.map.HMapKFWTest.testSerialize1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.map.HMapKFWTest.testTypeSafety','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'edu.umd.cloud9.util.map.HMapKITest.testBasic','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.util.map.HMapKITest.testPlus','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.util.map.HMapKITest.testDot','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.util.map.HMapKITest.testSortedEntriesValue1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.util.map.HMapKITest.testSortedEntriesValue2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.util.map.HMapKITest.testSortedEntriesKey1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.util.map.HMapKITest.testSortedEntriesKey2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.map.HMapKIWTest.testBasic','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.map.HMapKIWTest.testSerialize1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.io.map.HMapKIWTest.testTypeSafety','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.seek','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.seek'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.tell','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.read','org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.read'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.available','org.apache.hadoop.fs.FSDataInputStream.available'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.skip','org.apache.hadoop.fs.FSDataInputStream.skip'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.close','org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.mark','org.apache.hadoop.fs.FSDataInputStream.mark'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.reset','org.apache.hadoop.fs.FSDataInputStream.reset'
'org.apache.pig.backend.hadoop.datastorage.HSeekableInputStream.markSupported','org.apache.hadoop.fs.FSDataInputStream.markSupported'
'org.apache.nutch.analysis.lang.HTMLLanguageParser.setConf','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getStrings'
'org.commoncrawl.protocol.shared.HTMLMetaTags.newInstance','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.archive.wayback.hadoop.HTTPImportJob.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setMaxInputSplitSize org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.archive.wayback.hadoop.HTTPImportJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.archive.wayback.hadoop.HTTPImportMapper.setup','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.Text.<init>'
'org.archive.wayback.hadoop.HTTPImportMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.archive.wayback.hadoop.HTTPImportMapper.getPathLength','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen'
'org.archive.wayback.hadoop.HTTPImportMapper.doCopy','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'meetup.beeno.util.HUtil.getTable','org.apache.hadoop.hbase.client.HTablePool.getTable'
'meetup.beeno.util.HUtil.releaseTable','org.apache.hadoop.hbase.client.HTable.getTableName org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTablePool.putTable'
'meetup.beeno.util.HUtil.convertValue','org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toFloat org.apache.hadoop.hbase.util.Bytes.toDouble org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toString'
'meetup.beeno.util.HUtil.convertToBytes','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'meetup.beeno.util.HUtil.toOrderedBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'meetup.beeno.util.HUtil.HCol.HCol','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'meetup.beeno.util.HUtil.HCol.parse','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.parseColumn'
'org.hackreduce.examples.ngram.two_gram.HackRecordCounter.HackRecordCounterMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.hackreduce.examples.ngram.two_gram.HackRecordCounter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'cascading.tap.hadoop.util.Hadoop18TapUtil.setupJob','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.set'
'cascading.tap.hadoop.util.Hadoop18TapUtil.setupTask','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.get'
'cascading.tap.hadoop.util.Hadoop18TapUtil.needsTaskCommit','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'cascading.tap.hadoop.util.Hadoop18TapUtil.commitTask','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.delete'
'cascading.tap.hadoop.util.Hadoop18TapUtil.cleanupTap','org.apache.hadoop.fs.Path.<init>'
'cascading.tap.hadoop.util.Hadoop18TapUtil.cleanupJob','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath'
'cascading.tap.hadoop.util.Hadoop18TapUtil.cleanTempPath','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'cascading.tap.hadoop.util.Hadoop18TapUtil.getFSSafe','org.apache.hadoop.fs.Path.getFileSystem'
'cascading.tap.hadoop.util.Hadoop18TapUtil.isInflow','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'cascading.tap.hadoop.util.Hadoop18TapUtil.getTaskOutputPath','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.makeQualified'
'cascading.tap.hadoop.util.Hadoop18TapUtil.setWorkOutputPath','org.apache.hadoop.mapred.JobConf.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set'
'cascading.tap.hadoop.util.Hadoop18TapUtil.makeTempPath','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'cascading.tap.hadoop.util.Hadoop18TapUtil.moveTaskOutputs','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileStatus.getPath'
'cascading.tap.hadoop.util.Hadoop18TapUtil.getFinalPath','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'cascading.tap.hadoop.util.Hadoop18TapUtil.writeDirectlyToWorkingPath','org.apache.hadoop.fs.FileSystem.getClass org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.fs.FileSystem.getName'
'org.apache.oozie.service.HadoopAccessorService.createJobClient','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobClient.<init>'
'org.apache.oozie.service.HadoopAccessorService.createFileSystem','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'org.apache.oozie.service.HadoopAccessorService.createConfiguration','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.service.HadoopAccessorService.addFileToClassPath','org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.filecache.DistributedCache.addFileToClassPath'
'edu.umd.hooka.alignment.HadoopAlign.loadATable','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open'
'edu.umd.hooka.alignment.HadoopAlign.loadVocab','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.open'
'edu.umd.hooka.alignment.HadoopAlign.AEListener.notifyUnalignablePair','org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.hooka.alignment.HadoopAlign.AlignmentBase.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.fs.FileSystem.get'
'edu.umd.hooka.alignment.HadoopAlign.EMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'edu.umd.hooka.alignment.HadoopAlign.AlignMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,edu.umd.hooka.PhrasePair>.collect'
'edu.umd.hooka.alignment.HadoopAlign.EMReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.hooka.alignment.PartialCountContainer>.collect'
'edu.umd.hooka.alignment.HadoopAlign.FileReaderZip.SFRComp.getKey','org.apache.hadoop.io.IntWritable.get'
'edu.umd.hooka.alignment.HadoopAlign.FileReaderZip.SFRComp.compareTo','org.apache.hadoop.io.IntWritable.get'
'edu.umd.hooka.alignment.HadoopAlign.FileReaderZip.next','org.apache.hadoop.io.IntWritable.set'
'edu.umd.hooka.alignment.HadoopAlign.ModelMergeMapper2.run','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.Reporter.progress org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.delete'
'edu.umd.hooka.alignment.HadoopAlign.ModelMergeMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init>'
'edu.umd.hooka.alignment.HadoopAlign.ModelMergeMapper.map','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.Reporter.progress org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.collect'
'edu.umd.hooka.alignment.HadoopAlign.ComputeAER','org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter'
'edu.umd.hooka.alignment.HadoopAlign.startPServers','org.apache.hadoop.fs.FileSystem.get'
'edu.umd.hooka.alignment.HadoopAlign.doAlignment','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.getCounters'
'edu.umd.hooka.alignment.HadoopAlignConfig.getTTablePath','org.apache.hadoop.fs.Path.<init>'
'edu.umd.hooka.alignment.HadoopAlignConfig.getATablePath','org.apache.hadoop.fs.Path.<init>'
'edu.umd.hooka.alignment.HadoopAlignConfig.getFVocPath','org.apache.hadoop.fs.Path.<init>'
'edu.umd.hooka.alignment.HadoopAlignConfig.getEVocPath','org.apache.hadoop.fs.Path.<init>'
'edu.umd.hooka.alignment.HadoopAlignConfig.setTTablePath','org.apache.hadoop.fs.Path.toString'
'edu.umd.hooka.alignment.HadoopAlignConfig.setATablePath','org.apache.hadoop.fs.Path.toString'
'edu.umd.hooka.alignment.HadoopAlignConfig.setEVocFile','org.apache.hadoop.fs.Path.toString'
'edu.umd.hooka.alignment.HadoopAlignConfig.setFVocFile','org.apache.hadoop.fs.Path.toString'
'org.wso2.carbon.hadoop.security.HadoopCarbonSecurity.clean','org.apache.hadoop.security.UserGroupInformationThreadLocal.remove'
'org.apache.whirr.examples.HadoopClusterExample.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init>'
'org.apache.whirr.examples.HadoopClusterExample.runWordCountingJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.whirr.examples.HadoopClusterExample.getHadoopConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.whirr.examples.HadoopClusterExample.waitToExitSafeMode','org.apache.hadoop.mapred.JobClient.getFs org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode'
'org.apache.whirr.examples.HadoopClusterExample.waitForTaskTrackers','org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers'
'org.springframework.data.hadoop.mapreduce.HadoopCodeExecutor.runCode','org.apache.hadoop.conf.Configuration.getClassLoader org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser org.apache.hadoop.security.UserGroupInformation.doAs'
'org.springframework.data.hadoop.mapreduce.HadoopCodeExecutor.resolveTargetClass','org.apache.hadoop.conf.Configuration.setClassLoader'
'org.apache.avro.mapred.HadoopCombiner.getReducer','org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.avro.mapred.HadoopCombiner.PairCollector.collect','org.apache.hadoop.mapred.OutputCollector<org.apache.avro.mapred.AvroKey<org.apache.avro.mapred.K>,org.apache.avro.mapred.AvroValue<org.apache.avro.mapred.V>>.collect'
'org.kevoree.library.hadoop.HadoopComponent.loadProperties','org.apache.hadoop.conf.Configuration.set'
'fr.insarennes.fafdti.builder.HadoopConfSerializer.serializeToConf','org.apache.hadoop.conf.Configuration.set'
'fr.insarennes.fafdti.builder.HadoopConfSerializer.deserializeFromConf','org.apache.hadoop.conf.Configuration.get'
'com.asakusafw.runtime.core.HadoopConfiguration.HadoopConfiguration','org.apache.hadoop.conf.Configuration.<init>'
'com.asakusafw.runtime.core.HadoopConfiguration.get','org.apache.hadoop.conf.Configuration.get'
'com.asakusafw.runtime.core.HadoopConfiguration.set','org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.runtime.core.HadoopConfiguration.getClassLoader','org.apache.hadoop.conf.Configuration.getClassLoader'
'org.lilyproject.server.modules.general.HadoopConfigurationFactoryImpl.getHBaseConf','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.server.modules.general.HadoopConfigurationFactoryImpl.getMapReduceConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.server.modules.general.HadoopConfigurationFactoryImpl.getMapReduceJobConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceCoreTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configurable.setConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceCoreTest.delete_sharetemp','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceCoreTest.MockFileFormat.createInput','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FSDataInputStream.close'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceCoreTest.MockFileFormat.createOutput','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceProfile.HadoopDataSourceProfile','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getLocal'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceProfile.convert','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.makeQualified'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceProfile.getFsIdentity','org.apache.hadoop.fs.FileSystem.getUri'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceProfile.takeFsPath','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceProfile.takeTempPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.loadProfiles_simple','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.loadProfiles_path','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.loadProfiles_attribute','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.loadProfiles_multiple','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.loadRepository','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.transactionInfo','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.minimalCovered_deep','org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.collect','org.apache.hadoop.fs.FileStatus.isDir'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.normalize','org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.getTempFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.getBase','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.directio.hadoop.HadoopDataSourceUtilTest.getPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.configure','org.apache.hadoop.fs.FileUtil.chmod org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.close','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.writeErrorToHDFS','org.apache.hadoop.mapreduce.TaskAttemptID.forName'
'org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.processError','org.apache.hadoop.fs.FSDataOutputStream.writeBytes'
'org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.writeDebugHeader','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.mapreduce.MapContext.getInputSplit org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.putFile','org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.deletePathRecursivly','org.apache.hadoop.fs.FileSystem.delete'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.move','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.putFileToTmpDirectoryOverwirtingOldFilesAppendingPath','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.getFile','org.apache.hadoop.fs.FileSystem.copyToLocalFile'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.listPath','org.apache.hadoop.fs.FileSystem.listStatus'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.createPathFromURI','org.apache.hadoop.fs.Path.<init>'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.throwExceptionIfRemotePathAlreadyExist','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toString'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchive.openFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.beforeMethod','org.apache.hadoop.fs.Path.<init>'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.afterMethod','org.apache.hadoop.fs.FileSystem.delete'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.getFile_validInput_fileShouldBeRetrived','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.getFile_whenLocalFileAllreadyExist_fileOverwriteException','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.getFile_whenLocalFileAllreadyExist_localFileIsNotOverwritten','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFile_validInput_fileShouldBePutToFilesSystem','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFile_whenLocalFileDoNotExist_fileNotFoundException','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFile_whenRemoteFileExists_fileOverwriteException','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFile_whenRemoteFileExists_remoteFileShouldNotBeOverwriten','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFile_withDirectoryContainingAnotherDirectory_bothDirectoriesExistsInTheArchive','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFileAtomically_validInput_fileShouldBePutToFilesSystem','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFileAtomically_whenLocalFileDoNotExist_fileNotFoundException','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFileAtomically_whenRemoteFileExists_fileOverwriteException','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFileAtomically_whenRemoteFileExists_remoteFileShouldNotBeOverwriten','org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFileAtomically_withDirectoryContainingAnotherDirectory_bothDirectoriesExistsInTheArchive','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFileAtomically_withFileAllreadyInTmpFolder_theFilesinTmpFolderDoesNotAffectTheTrasfer','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.deletePathRecursivly_givenAFile_thePathShouldBeDeleted','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.deletePathRecursivly_givenADirectory_thePathShouldBeDeleted','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.deletePathRecursivly_givenADirectoryWithFilesInIt_thePathShouldBeDeleted','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.putFileToTmpDirectoryAppendingPath_existingFile_fileIsCopiedToTheTmpDirectory','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.move_existingFileOnHadoop_fileIsMoved','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'com.splunk.shuttl.archiver.filesystem.HadoopFileSystemArchiveTest.openFile_existingFileOnHadoop_inputStreamToFile','org.apache.hadoop.fs.Path.toUri'
'cascading.flow.hadoop.HadoopFlow.getMaxConcurrentSteps','org.apache.hadoop.mapred.JobConf.getInt'
'cascading.flow.hadoop.HadoopFlow.initConfig','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.HadoopFlow.setConfigProperty','org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.HadoopFlow.newConfig','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.<init>'
'cascading.flow.hadoop.HadoopFlow.getConfig','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.flow.hadoop.HadoopFlow.getConfigCopy','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.flow.hadoop.HadoopFlow.getProperty','org.apache.hadoop.mapred.JobConf.get'
'cascading.flow.hadoop.HadoopFlow.stepsAreLocal','org.apache.hadoop.mapred.JobConf.get'
'cascading.flow.hadoop.HadoopFlowProcess.HadoopFlowProcess','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.flow.hadoop.HadoopFlowProcess.getConfigCopy','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.flow.hadoop.HadoopFlowProcess.getCurrentNumMappers','org.apache.hadoop.mapred.JobConf.getNumMapTasks'
'cascading.flow.hadoop.HadoopFlowProcess.getCurrentNumReducers','org.apache.hadoop.mapred.JobConf.getNumReduceTasks'
'cascading.flow.hadoop.HadoopFlowProcess.getCurrentSliceNum','org.apache.hadoop.mapred.JobConf.getInt'
'cascading.flow.hadoop.HadoopFlowProcess.getProperty','org.apache.hadoop.mapred.JobConf.get'
'cascading.flow.hadoop.HadoopFlowProcess.newInstance','org.apache.hadoop.util.ReflectionUtils.newInstance'
'cascading.flow.hadoop.HadoopFlowProcess.keepAlive','org.apache.hadoop.mapred.Reporter.progress'
'cascading.flow.hadoop.HadoopFlowProcess.increment','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'cascading.flow.hadoop.HadoopFlowProcess.setStatus','org.apache.hadoop.mapred.Reporter.setStatus'
'cascading.flow.hadoop.HadoopFlowProcess.openTrapForWrite','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.HadoopFlowProcess.collect','org.apache.hadoop.mapred.OutputCollector.collect'
'cascading.flow.hadoop.HadoopFlowProcess.copyConfig','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.flow.hadoop.HadoopFlowStep.getInitializedConfig','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setOutputValueGroupingComparator org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setOutputValueGroupingComparator org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.HadoopFlowStep.isHadoopLocalMode','org.apache.hadoop.mapred.JobConf.get'
'cascading.flow.hadoop.HadoopFlowStep.clean','org.apache.hadoop.mapred.JobConf.get'
'cascading.flow.hadoop.HadoopFlowStep.addComparators','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt'
'cascading.flow.hadoop.HadoopFlowStep.initFromTraps','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.flow.hadoop.HadoopFlowStep.initFromSources','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.HadoopFlowStep.set','org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.HadoopFlowStep.update','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.HadoopFlowStep.get','org.apache.hadoop.mapred.JobConf.get'
'cascading.flow.hadoop.HadoopFlowStep.initFromSink','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'com.asakusafw.windgate.hadoopfs.HadoopFsProvider.configure','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.trevni.avro.HadoopInput.HadoopInput','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.trevni.avro.HadoopInput.read','org.apache.hadoop.fs.FSDataInputStream.read'
'org.apache.trevni.avro.HadoopInput.close','org.apache.hadoop.fs.FSDataInputStream.close'
'edu.ucla.sspace.hadoop.HadoopJob.HadoopJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'edu.ucla.sspace.hadoop.HadoopJob.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'edu.ucla.sspace.hadoop.HadoopJob.OutputFilePathFilter.accept','org.apache.hadoop.fs.Path.getName'
'org.wso2.carbon.mapred.jobtracker.HadoopJobTrackerContorller.HadoopJobTrackerContorller','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set'
'org.wso2.carbon.mapred.jobtracker.HadoopJobTrackerContorller.run','org.apache.hadoop.mapred.JobTracker.startTracker org.apache.hadoop.mapred.JobTracker.offerService'
'org.wso2.carbon.mapred.jobtracker.HadoopJobTrackerContorller.stop','org.apache.hadoop.mapred.JobTracker.stopTracker'
'org.lilyproject.testfw.HadoopLauncher.run','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.MiniMRCluster.getJobTrackerPort org.apache.hadoop.mapred.MiniMRCluster.getJobTrackerRunner org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.MiniHBaseCluster.getMaster'
'org.lilyproject.testfw.HadoopLauncher.startMiniCluster','org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.hbase.util.FSUtils.setVersion org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.MiniHBaseCluster.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next'
'org.lilyproject.testfw.HadoopLauncher.setupClusterTestBuildDir','org.apache.hadoop.fs.Path.toString'
'org.lilyproject.testfw.HadoopLauncher.getTestDir','org.apache.hadoop.fs.Path.<init>'
'org.lilyproject.testfw.HadoopLauncher.shutdownMiniCluster','org.apache.hadoop.hbase.MiniHBaseCluster.shutdown org.apache.hadoop.hbase.MiniHBaseCluster.join org.apache.hadoop.hdfs.MiniDFSCluster.shutdown org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.FSUtils.deleteDirectory'
'org.lilyproject.testfw.HadoopLauncher.startMiniZKCluster','org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.<init> org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setClientPort org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.testfw.HadoopLauncher.shutdownMiniZKCluster','org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown'
'org.lilyproject.testfw.HadoopLauncher.startMiniDFSCluster','org.apache.hadoop.hdfs.MiniDFSCluster.<init>'
'org.lilyproject.testfw.HadoopLauncher.startMiniMapReduceCluster','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.mapred.MiniMRCluster.createJobConf org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.testfw.HadoopLauncher.shutdownMiniMapReduceCluster','org.apache.hadoop.mapred.MiniMRCluster.shutdown org.apache.hadoop.conf.Configuration.set'
'org.apache.avro.mapred.HadoopMapper.configure','org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapred.JobConf.getNumReduceTasks'
'org.apache.avro.mapred.HadoopMapper.MapCollector.collect','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.avro.mapred.KO,org.apache.avro.mapred.VO>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.avro.mapred.KO,org.apache.avro.mapred.VO>.collect'
'org.apache.hama.bsp.message.HadoopMessageManagerImpl.startRPCServer','org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.ipc.RPC.Server.start'
'org.apache.hama.bsp.message.HadoopMessageManagerImpl.close','org.apache.hadoop.ipc.RPC.Server.stop'
'org.apache.hama.bsp.message.HadoopMessageManagerImpl.getBSPPeerConnection','org.apache.hadoop.ipc.RPC.getProxy'
'hitune.analysis.mapreduce.processor.HadoopMetrics.MapClass.init','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'hitune.analysis.mapreduce.processor.HadoopMetrics.MapClass.map','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.OutputCollector<hitune.analysis.mapreduce.processor.K,hitune.analysis.mapreduce.processor.V>.collect'
'hitune.analysis.mapreduce.processor.HadoopMetrics.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'org.kevoree.library.hadoop.HadoopNameNode.start','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hdfs.server.namenode.NameNode.format org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.kevoree.library.hadoop.HadoopNameNode.runNameNode','org.apache.hadoop.hdfs.server.namenode.NameNode.<init> org.apache.hadoop.hdfs.server.namenode.NameNode.join'
'org.kevoree.library.hadoop.HadoopNameNode.stop','org.apache.hadoop.hdfs.server.namenode.NameNode.stop'
'com.datasalt.pangool.tuplemr.mapred.lib.output.HadoopOutputFormat.instantiateWhenNeeded','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.datasalt.pangool.tuplemr.mapred.lib.output.HadoopOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.OutputFormat.getRecordWriter'
'com.datasalt.pangool.tuplemr.mapred.lib.output.HadoopOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.OutputFormat.checkOutputSpecs'
'com.datasalt.pangool.tuplemr.mapred.lib.output.HadoopOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.OutputFormat.getOutputCommitter'
'cascading.flow.hadoop.planner.HadoopPlanner.createJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.planner.HadoopPlanner.initialize','org.apache.hadoop.mapred.JobConf.getJar org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.getJar org.apache.hadoop.mapred.JobConf.setJar org.apache.hadoop.mapred.JobConf.getJar org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.getJar org.apache.hadoop.mapred.JobConf.getJar'
'cascading.test.HadoopPlatform.setUp','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.mapred.MiniMRCluster.createJobConf org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setMapSpeculativeExecution org.apache.hadoop.mapred.JobConf.setReduceSpeculativeExecution org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks'
'cascading.test.HadoopPlatform.getJobConf','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.test.HadoopPlatform.copyFromLocal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileUtil.copy'
'cascading.test.HadoopPlatform.copyToLocal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileUtil.copy'
'cascading.test.HadoopPlatform.remoteExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.manning.hip.ch3.xml.HadoopPropertyXMLMapReduce.Map.map','org.apache.hadoop.io.Text.toString'
'com.manning.hip.ch3.xml.HadoopPropertyXMLMapReduce.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.ucla.sspace.util.HadoopResourceFinder.HadoopResourceFinder','org.apache.hadoop.fs.FileSystem.get'
'edu.ucla.sspace.util.HadoopResourceFinder.open','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'com.cloudera.lib.service.hadoop.HadoopService.init','org.apache.hadoop.util.VersionInfo.getVersion org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.security.UserGroupInformation.setConfiguration org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.security.UserGroupInformation.setConfiguration'
'com.cloudera.lib.service.hadoop.HadoopService.getUGI','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser'
'com.cloudera.lib.service.hadoop.HadoopService.setRequiredServiceHadoopConf','org.apache.hadoop.conf.Configuration.set'
'com.cloudera.lib.service.hadoop.HadoopService.createHadoopConf','org.apache.hadoop.mapred.JobConf.<init>'
'com.cloudera.lib.service.hadoop.HadoopService.createFileSystem','org.apache.hadoop.fs.FileSystem.get'
'com.cloudera.lib.service.hadoop.HadoopService.closeFileSystem','org.apache.hadoop.fs.FileSystem.close'
'com.cloudera.lib.service.hadoop.HadoopService.createJobClient','org.apache.hadoop.mapred.JobClient.<init>'
'com.cloudera.lib.service.hadoop.HadoopService.closeJobClient','org.apache.hadoop.mapred.JobClient.close'
'com.cloudera.lib.service.hadoop.HadoopService.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getTrimmed org.apache.hadoop.conf.Configuration.get org.apache.hadoop.security.UserGroupInformation.doAs org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getTrimmed org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getTrimmed org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.security.UserGroupInformation.doAs'
'com.cloudera.lib.service.hadoop.HadoopService.createFileSystemInternal','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.whirr.service.yarn.integration.HadoopServiceController.startup','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init>'
'org.apache.whirr.service.yarn.integration.HadoopServiceController.getConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.whirr.service.yarn.integration.HadoopServiceController.getJobConf','org.apache.hadoop.mapred.JobConf.<init>'
'org.apache.whirr.service.yarn.integration.HadoopServiceController.waitToExitSafeMode','org.apache.hadoop.mapred.JobClient.getFs org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode'
'org.apache.whirr.service.yarn.integration.HadoopServiceController.waitForTaskTrackers','org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers'
'org.apache.whirr.service.hadoop.integration.HadoopServiceController.startup','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init>'
'org.apache.whirr.service.hadoop.integration.HadoopServiceController.getConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.whirr.service.hadoop.integration.HadoopServiceController.getJobConf','org.apache.hadoop.mapred.JobConf.<init>'
'org.apache.whirr.service.hadoop.integration.HadoopServiceController.waitToExitSafeMode','org.apache.hadoop.mapred.JobClient.getFs org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode'
'org.apache.whirr.service.hadoop.integration.HadoopServiceController.waitForTaskTrackers','org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers'
'org.apache.whirr.service.hadoop.integration.benchmark.HadoopServiceTeraSortBenchmark.testTeraSort','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.whirr.service.hadoop.integration.benchmark.HadoopServiceTeraSortBenchmark.runTeraGen','org.apache.hadoop.examples.terasort.TeraGen.<init> org.apache.hadoop.examples.terasort.TeraGen.setConf org.apache.hadoop.examples.terasort.TeraGen.run'
'org.apache.whirr.service.hadoop.integration.benchmark.HadoopServiceTeraSortBenchmark.runTeraSort','org.apache.hadoop.examples.terasort.TeraSort.<init> org.apache.hadoop.examples.terasort.TeraSort.setConf org.apache.hadoop.examples.terasort.TeraSort.run'
'org.apache.whirr.service.hadoop.integration.benchmark.HadoopServiceTeraSortBenchmark.runTeraValidate','org.apache.hadoop.examples.terasort.TeraValidate.<init> org.apache.hadoop.examples.terasort.TeraValidate.setConf org.apache.hadoop.examples.terasort.TeraValidate.run'
'org.apache.whirr.service.hadoop.integration.benchmark.HadoopServiceTestDFSIOBenchmark.test','org.apache.hadoop.fs.TestDFSIO.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.TestDFSIO.setConf org.apache.hadoop.fs.TestDFSIO.run org.apache.hadoop.fs.TestDFSIO.run org.apache.hadoop.fs.TestDFSIO.run'
'org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.cloneJobContext','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.ContextFactory.cloneContext'
'org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.createTaskAttemptContext','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init> org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>'
'org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.createJobContext','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.task.JobContextImpl.<init> org.apache.hadoop.mapreduce.task.JobContextImpl.<init>'
'org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.isMap','org.apache.hadoop.mapreduce.TaskAttemptID.getTaskType'
'org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.getNewTaskAttemptID','org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.createTaskAttemptID','org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.commitOrCleanup','org.apache.hadoop.mapreduce.OutputCommitter.commitJob'
'org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.getDefaultBlockSize','org.apache.hadoop.fs.FileSystem.getDefaultBlockSize'
'cascading.tuple.hadoop.collect.HadoopSpillableTupleList.getCodec','org.apache.hadoop.util.ReflectionUtils.newInstance'
'cascading.tuple.hadoop.collect.HadoopSpillableTupleList.createTupleOutputStream','org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'cascading.tuple.hadoop.collect.HadoopSpillableTupleList.close','org.apache.hadoop.io.compress.CodecPool.returnCompressor org.apache.hadoop.io.compress.CodecPool.returnDecompressor'
'cascading.tuple.hadoop.collect.HadoopSpillableTupleList.getCompressor','org.apache.hadoop.io.compress.CodecPool.getCompressor org.apache.hadoop.io.compress.CompressionCodec.getClass org.apache.hadoop.io.compress.CodecPool.getCompressor'
'cascading.tuple.hadoop.collect.HadoopSpillableTupleList.createTupleInputStream','org.apache.hadoop.io.compress.CompressionCodec.createInputStream'
'cascading.tuple.hadoop.collect.HadoopSpillableTupleList.getDecompressor','org.apache.hadoop.io.compress.CodecPool.getDecompressor org.apache.hadoop.io.compress.CompressionCodec.getClass org.apache.hadoop.io.compress.CodecPool.getDecompressor'
'com.twitter.elephanttwin.lucene.indexing.HadoopSplitDocument.readFields','org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.readFields'
'com.twitter.elephanttwin.lucene.indexing.HadoopSplitDocument.write','org.apache.hadoop.mapreduce.lib.input.FileSplit.write'
'com.twitter.elephanttwin.lucene.indexing.HadoopSplitIndexingJob.setupJob','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass'
'step2.HadoopStep2.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'step3.HadoopStep3.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'step4.HadoopStep4.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'voldemort.store.readonly.mr.HadoopStoreBuilder.build','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setReduceSpeculativeExecution org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.mapred.Counters.getCounter org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FileStatus.toString org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'voldemort.store.readonly.mr.HadoopStoreBuilder.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'voldemort.store.readonly.mr.HadoopStoreBuilder.IndexFileLastComparator.compare','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'voldemort.store.readonly.mr.HadoopStoreBuilder.sizeOfPath','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen'
'voldemort.store.readonly.mr.HadoopStoreBuilderCollisionTest.CollidingTextStoreMapper.makeKey','org.apache.hadoop.io.Text.toString'
'voldemort.store.readonly.mr.HadoopStoreBuilderCollisionTest.CollidingTextStoreMapper.makeValue','org.apache.hadoop.io.Text.toString'
'voldemort.store.readonly.mr.HadoopStoreBuilderCollisionTest.CollidingTextStoreMapper.map','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect'
'voldemort.store.readonly.mr.HadoopStoreBuilderCollisionTest.testCollisionWithParams','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'voldemort.store.readonly.mr.HadoopStoreBuilderPartitioner.getPartition','org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.get'
'voldemort.store.readonly.mr.HadoopStoreBuilderReducer.configure','org.apache.hadoop.mapred.JobConf.get'
'voldemort.store.readonly.mr.HadoopStoreBuilderTest.TextStoreMapper.makeKey','org.apache.hadoop.io.Text.toString'
'voldemort.store.readonly.mr.HadoopStoreBuilderTest.TextStoreMapper.makeValue','org.apache.hadoop.io.Text.toString'
'voldemort.store.readonly.mr.HadoopStoreBuilderTest.testRowsLessThanNodes','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'voldemort.store.readonly.mr.HadoopStoreBuilderTest.testHadoopBuild','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'voldemort.store.readonly.mr.HadoopStoreBuilderUtils.readFileContents','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read'
'voldemort.store.readonly.mr.HadoopStoreBuilderUtils.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'voldemort.store.readonly.mr.HadoopStoreBuilderUtils.getDataChunkFiles','org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus'
'voldemort.store.readonly.mr.HadoopStoreBuilderUtils.getDataFileChunkSet','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen'
'voldemort.store.readonly.mr.HadoopStoreBuilderUtils.compare','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'voldemort.store.readonly.mr.HadoopStoreJobRunner.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'voldemort.store.readonly.mr.HadoopStoreJobRunner.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete'
'voldemort.store.readonly.mr.HadoopStoreJobRunner.addDepJars','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'voldemort.store.readonly.mr.HadoopStoreJobRunner.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'voldemort.store.readonly.disk.HadoopStoreWriter.conf','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create'
'voldemort.store.readonly.disk.HadoopStoreWriter.write','org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'voldemort.store.readonly.disk.HadoopStoreWriter.close','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.rename'
'org.wso2.carbon.bam.analyzer.test.HadoopTest.TokenizerMapper.map','org.apache.hadoop.io.Text.set'
'org.wso2.carbon.bam.analyzer.test.HadoopTest.ReducerToFilesystem.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.wso2.carbon.bam.analyzer.test.HadoopTest.ReducerToCassandra.reduce','org.apache.hadoop.io.IntWritable.get'
'org.wso2.carbon.bam.analyzer.test.HadoopTest.ReducerToCassandra.getMutation','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.wso2.carbon.bam.analyzer.test.HadoopTest.analyze','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.tomslabs.grid.avro.HadoopTestBase.beforeClass','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get'
'com.tomslabs.grid.avro.HadoopTestBase.afterClass','org.apache.hadoop.fs.FileSystem.close'
'com.tomslabs.grid.avro.HadoopTestBase.localResourceToPath','org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'com.tomslabs.grid.avro.HadoopTestBase.localFileToPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified'
'com.rapleaf.hank.hadoop.HadoopTestCase.HadoopTestCase','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'com.rapleaf.hank.hadoop.HadoopTestCase.setUp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'com.rapleaf.hank.hadoop.HadoopTestCase.outputFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'com.rapleaf.hank.hadoop.HadoopTestCase.getContents','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.close'
'com.ning.metrics.serialization.hadoop.HadoopThriftWritableSerialization.HadoopThriftWritableDeserializer.deserialize','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.mahout.common.HadoopUtil.prepareJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.common.HadoopUtil.getCustomJobName','org.apache.hadoop.mapreduce.JobContext.getJobName'
'org.apache.mahout.common.HadoopUtil.delete','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.common.HadoopUtil.openStream','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.common.HadoopUtil.getFileStatus','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileSystem.globStatus'
'org.apache.mahout.common.HadoopUtil.listStatus','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.mahout.common.HadoopUtil.cacheFiles','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.setCacheFiles'
'org.apache.mahout.common.HadoopUtil.cachedFile','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.common.HadoopUtil.setSerializations','org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.common.HadoopUtil.writeInt','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeInt'
'org.apache.mahout.common.HadoopUtil.readInt','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt'
'cascading.flow.hadoop.util.HadoopUtil.initLog4j','org.apache.hadoop.mapred.JobConf.get'
'cascading.flow.hadoop.util.HadoopUtil.createJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.util.HadoopUtil.getHDFSShutdownHook','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.getDeclaredField'
'cascading.flow.hadoop.util.HadoopUtil.instantiateSerializer','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configurable.setConf'
'cascading.flow.hadoop.util.HadoopUtil.mergeConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set'
'cascading.flow.hadoop.util.HadoopUtil.writeStateToDistCache','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'cascading.flow.hadoop.util.HadoopUtil.readStateFromDistCache','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'voldemort.store.readonly.mr.utils.HadoopUtils.getFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.hdfs.DistributedFileSystem.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.initialize'
'voldemort.store.readonly.mr.utils.HadoopUtils.setSerializableInCache','org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getName org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'voldemort.store.readonly.mr.utils.HadoopUtils.getFilePathFromDistributedCache','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'voldemort.store.readonly.mr.utils.HadoopUtils.getFileInputStream','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'voldemort.store.readonly.mr.utils.HadoopUtils.readSerializableFromCache','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'voldemort.store.readonly.mr.utils.HadoopUtils.getMetadataFromSequenceFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.<init>'
'voldemort.store.readonly.mr.utils.HadoopUtils.getSchemaFromPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.isDirectory org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'voldemort.store.readonly.mr.utils.HadoopUtils.getRequiredString','org.apache.hadoop.conf.Configuration.get'
'voldemort.store.readonly.mr.utils.HadoopUtils.copyInProps','org.apache.hadoop.conf.Configuration.set'
'voldemort.store.readonly.mr.utils.HadoopUtils.copyInRequiredProps','org.apache.hadoop.conf.Configuration.set'
'voldemort.store.readonly.mr.utils.HadoopUtils.copyInAllProps','org.apache.hadoop.conf.Configuration.set'
'voldemort.store.readonly.mr.utils.HadoopUtils.copyInLocalProps','org.apache.hadoop.conf.Configuration.set'
'voldemort.store.readonly.mr.utils.HadoopUtils.loadHadoopProps','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.get'
'voldemort.store.readonly.mr.utils.HadoopUtils.setPropsInJob','org.apache.hadoop.conf.Configuration.set'
'voldemort.store.readonly.mr.utils.HadoopUtils.getPropsFromJob','org.apache.hadoop.conf.Configuration.get'
'voldemort.store.readonly.mr.utils.HadoopUtils.readCluster','org.apache.hadoop.fs.Path.<init>'
'voldemort.store.readonly.mr.utils.HadoopUtils.readStoreDef','org.apache.hadoop.fs.Path.<init>'
'voldemort.store.readonly.mr.utils.HadoopUtils.getFileFromCache','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles'
'voldemort.store.readonly.mr.utils.HadoopUtils.getFileFromPathList','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'voldemort.store.readonly.mr.utils.HadoopUtils.getFileNames','org.apache.hadoop.fs.FileStatus.getPath'
'voldemort.store.readonly.mr.utils.HadoopUtils.getLowestLevelDirectories','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.PathFilter.accept org.apache.hadoop.fs.Path.toString'
'voldemort.store.readonly.mr.utils.HadoopUtils.hasSubDirectories','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath'
'voldemort.store.readonly.mr.utils.HadoopUtils.addAllSubPaths','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.FileInputFormat.addInputPath'
'voldemort.store.readonly.mr.utils.HadoopUtils.shouldPathBeIgnored','org.apache.hadoop.fs.Path.getName'
'voldemort.store.readonly.mr.utils.HadoopUtils.saveProps','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.create'
'voldemort.store.readonly.mr.utils.HadoopUtils.readProps','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'voldemort.store.readonly.mr.utils.HadoopUtils.readAsString','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open'
'voldemort.store.readonly.mr.utils.HadoopUtils.mkdirs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.mkdirs'
'voldemort.store.readonly.mr.utils.HadoopUtils.deletePathIfExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'voldemort.store.readonly.mr.utils.HadoopUtils.appendTag','org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.getCapacity org.apache.hadoop.io.BytesWritable.setCapacity org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.setSize'
'voldemort.store.readonly.mr.utils.HadoopUtils.readTag','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'voldemort.store.readonly.mr.utils.HadoopUtils.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'voldemort.store.readonly.mr.utils.HadoopUtils.getLatestVersionedPath','org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'voldemort.store.readonly.mr.utils.HadoopUtils.getSanitizedPath','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent'
'voldemort.store.readonly.mr.utils.HadoopUtils.cleanupOlderVersions','org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete'
'voldemort.store.readonly.mr.utils.HadoopUtils.move','org.apache.hadoop.fs.FileSystem.rename'
'voldemort.store.readonly.mr.utils.HadoopUtils.replaceFile','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.rename'
'com.griddynamics.jagger.storage.fs.hdfs.utils.HadoopUtils.toConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'com.infochimps.elasticsearch.hadoop.util.HadoopUtils.uploadLocalFile','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'com.infochimps.elasticsearch.hadoop.util.HadoopUtils.uploadLocalFileIfChanged','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'com.infochimps.elasticsearch.hadoop.util.HadoopUtils.fetchFileFromCache','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'com.infochimps.elasticsearch.hadoop.util.HadoopUtils.fetchArchiveFromCache','org.apache.hadoop.filecache.DistributedCache.getLocalCacheArchives org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'com.infochimps.elasticsearch.hadoop.util.HadoopUtils.shipFileIfNotShipped','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'com.infochimps.elasticsearch.hadoop.util.HadoopUtils.shipArchiveIfNotShipped','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheArchive'
'com.datasalt.utils.commons.HadoopUtils.deleteIfExists','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'com.datasalt.utils.commons.HadoopUtils.synchronize','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileUtil.copy'
'com.datasalt.utils.commons.HadoopUtils.stringToFile','org.apache.hadoop.fs.FileSystem.create'
'com.datasalt.utils.commons.HadoopUtils.fileToString','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'com.datasalt.utils.commons.HadoopUtils.readIntDoubleMap','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.DoubleWritable.get'
'com.datasalt.utils.commons.HadoopUtils.readIntDoubleMapFromGlob','org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.datasalt.utils.commons.HadoopUtils.readIntIntMap','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'com.datasalt.utils.commons.HadoopUtils.readIntIntMapFromGlob','org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.datasalt.utils.commons.HadoopUtils.incCounter','org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter'
'com.datasalt.utils.commons.HadoopUtils.locateFileInDC','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'org.cloumon.collector.writer.HadoopWriter.HadoopWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get'
'org.cloumon.collector.writer.HadoopWriter.write','org.apache.hadoop.fs.FSDataOutputStream.write'
'org.cloumon.collector.writer.HadoopWriter.rolling','org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create'
'org.cloumon.collector.writer.HadoopWriter.FileSyncAndRolling.run','org.apache.hadoop.fs.FSDataOutputStream.sync'
'com.ning.metrics.collector.hadoop.processing.HadoopWriterFactory.pushFileToHadoop','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.HamaConfiguration.addHamaResources','org.apache.hadoop.conf.Configuration.addDefaultResource org.apache.hadoop.conf.Configuration.addDefaultResource'
'org.apache.hama.HamaTestCase.setUp','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified'
'org.apache.hama.HamaTestCase.tearDown','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.hama.HamaTestCase.getUnitTestdir','org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.HamaTestCase.shutdownDfs','org.apache.hadoop.hdfs.MiniDFSCluster.shutdown org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.closeAll'
'edu.isi.mavuno.app.ie.HarvestEspressoPatterns.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'edu.isi.mavuno.app.ie.HarvestEspressoPatterns.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.isi.mavuno.app.mine.HarvestParaphraseCandidates.MyMapper.map','org.apache.hadoop.io.LongWritable.get'
'edu.isi.mavuno.app.mine.HarvestParaphraseCandidates.MyReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.set'
'edu.isi.mavuno.app.mine.HarvestParaphraseCandidates.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'edu.isi.mavuno.app.mine.HarvestParaphraseCandidates.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.isi.mavuno.app.ie.HarvestSAPInstances.MyMapper.map','org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append'
'edu.isi.mavuno.app.ie.HarvestSAPInstances.MyReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set'
'edu.isi.mavuno.app.ie.HarvestSAPInstances.MyCombiner.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set'
'edu.isi.mavuno.app.ie.HarvestSAPInstances.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.isi.mavuno.app.ie.HarvestSAPInstances.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.isi.mavuno.app.ie.HarvestUDAPInstances.MyReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.app.ie.HarvestUDAPInstances.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.isi.mavuno.app.ie.HarvestUDAPInstances.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.odiago.flumebase.exec.HashJoinElement.HashJoinElement','org.apache.hadoop.conf.Configuration.getInt'
'com.lightboxtechnologies.nsrl.HashLoader.HashLoaderMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get'
'com.lightboxtechnologies.nsrl.HashLoader.HashLoaderMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get'
'com.lightboxtechnologies.nsrl.HashLoader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'co.nubetech.hiho.dedup.HashUtility.getMD5Hash','org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.LongWritable.toString org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.IntWritable.toString org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.ArrayWritable.toString org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.BooleanWritable.toString org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.FloatWritable.toString org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.ByteWritable.toString org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.DoubleWritable.toString org.apache.hadoop.io.MD5Hash.digest'
'com.taobao.adfs.util.HashedBytes.HashedBytes','org.apache.hadoop.io.WritableComparator.hashBytes'
'com.infochimps.hbase.HbaseAFollowsBLoader.HbaseAFollowsBLoadMapper.setup','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.infochimps.hbase.HbaseAFollowsBLoader.HbaseAFollowsBLoadMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.<init> org.apache.hadoop.hbase.KeyValue.<init> org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.util.Bytes.toString'
'com.infochimps.hbase.HbaseAFollowsBLoader.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.infochimps.hbase.HbaseAFollowsBLoader.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'com.infochimps.hbase.HbaseBulkloader.HbaseLoadMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init>'
'com.infochimps.hbase.HbaseBulkloader.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.infochimps.hbase.HbaseBulkloader.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'org.springframework.data.hadoop.hbase.HbaseConfigurationFactoryBean.destroy','org.apache.hadoop.hbase.client.HConnectionManager.deleteConnection'
'org.springframework.data.hadoop.hbase.HbaseConfigurationFactoryBean.afterPropertiesSet','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.HBaseConfiguration.create'
'org.springframework.data.hadoop.hbase.HbaseConfigurationFactoryBean.getObjectType','org.apache.hadoop.conf.Configuration.getClass'
'org.apache.oozie.action.hadoop.HbaseCredentials.addtoJobConf','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser org.apache.hadoop.hbase.security.User.create org.apache.hadoop.hbase.security.User.obtainAuthTokenForJob'
'org.apache.oozie.action.hadoop.HbaseCredentials.injectConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.infochimps.hbase.HbaseGraphBulkLoader.HbaseGraphLoadMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.infochimps.hbase.HbaseGraphBulkLoader.HbaseGraphLoadMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.setWriteToWAL org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.util.Bytes.toString'
'com.infochimps.hbase.HbaseGraphBulkLoader.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.infochimps.hbase.HbaseGraphBulkLoader.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'org.mule.module.hbase.config.HbaseNamespaceHandlerTestCase.testFlowGet','org.apache.hadoop.hbase.client.Result.<init>'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.HbaseSnapshotRecordReader','org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.init','org.apache.hadoop.hbase.client.Scan.getStartRow'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.restart','org.apache.hadoop.hbase.client.HTable.getTableName org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setTimeRange org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.getAbortedTransactions','org.apache.hadoop.hbase.client.Scan.getFamilies org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.getMaximumRevision','org.apache.hadoop.hbase.client.Scan.getFamilies org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.createKey','org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init>'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.createValue','org.apache.hadoop.hbase.client.Result.<init>'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.next','org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.io.ImmutableBytesWritable.set org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.hbase.client.Result.write org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.hbase.client.Result.readFields'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.prepareResult','org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.client.Result.<init>'
'org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.close','org.apache.hadoop.hbase.client.ResultScanner.close'
'com.infochimps.hbase.HbaseTableBulkLoader.HbaseTableLoadMapper.setup','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.infochimps.hbase.HbaseTableBulkLoader.HbaseTableLoadMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.setWriteToWAL org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.util.Bytes.toString'
'com.infochimps.hbase.HbaseTableBulkLoader.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.infochimps.hbase.HbaseTableBulkLoader.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'org.springframework.data.hadoop.hbase.HbaseTemplate.applyFlushSetting','org.apache.hadoop.hbase.client.HTable.isAutoFlush org.apache.hadoop.hbase.client.HTable.setAutoFlush'
'org.springframework.data.hadoop.hbase.HbaseTemplate.flushIfNecessary','org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.isAutoFlush org.apache.hadoop.hbase.client.HTable.setAutoFlush'
'org.springframework.data.hadoop.hbase.HbaseTemplate.find','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn'
'org.springframework.data.hadoop.hbase.HbaseTemplate.doInTable','org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTable.get'
'org.mule.module.hbase.HbaseTestCase.testGetByRow','org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.isEmpty'
'com.bah.culvert.utils.HbaseTestProperties.addStandardHBaseProperties','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.inadco.hblr.tests.HblrScratchpad.createSeqFileWriter','org.apache.hadoop.io.compress.GzipCodec.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.hcatalog.templeton.HcatDelegator.descExtendedTable','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission'
'org.apache.hcatalog.HcatTestUtils.getDbPath','org.apache.hadoop.hive.ql.metadata.Hive.getDatabase org.apache.hadoop.hive.metastore.Warehouse.getDatabasePath'
'org.apache.hcatalog.HcatTestUtils.cleanupHMS','org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases org.apache.hadoop.hive.ql.metadata.Hive.getConf org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase org.apache.hadoop.hive.ql.metadata.Hive.getAllTables org.apache.hadoop.hive.ql.metadata.Hive.dropTable'
'org.apache.hcatalog.HcatTestUtils.isHadoop23','org.apache.hadoop.util.VersionInfo.getVersion'
'com.ning.hfind.HdfsAccess.getFileStatus','org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.ning.hfind.HdfsAccess.listStatus','org.apache.hadoop.fs.FileSystem.listStatus'
'com.ning.hfind.HdfsAccess.delete','org.apache.hadoop.fs.FileSystem.delete'
'com.ning.hfind.HdfsAccess.setFileSystem','org.apache.hadoop.fs.FileSystem.get'
'com.ning.hfind.HdfsAccess.getFileSystemSafe','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'azkaban.common.web.HdfsAvroFileViewer.canReadFile','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'azkaban.common.web.HdfsAvroFileViewer.getAvroDataStream','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.open'
'azkaban.common.web.HdfsAvroFileViewer.displayFile','org.apache.hadoop.fs.Path.toUri'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.HdfsBackedLanczosState','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configurable.getConf'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.setupDirs','org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.createDirIfNotExist','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.updateHdfsState','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.persistVector','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.fetchVector','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.getBasisVector','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.getRightSingularVector','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.getScaleFactor','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.decomposer.HdfsBackedLanczosState.getDiagonalMatrix','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.camel.component.hdfs.HdfsComponent.initHdfs','org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>'
'com.ning.hfind.config.HdfsConfig.configureHDFSAccess','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'voldemort.store.readonly.mr.HdfsDataFileChunkTest.testDataFileChunk','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getFileStatus'
'tkv.hdfs.HdfsDataStore.HdfsDataStore','org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init>'
'tkv.hdfs.HdfsDataStore.append','org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.write'
'tkv.hdfs.HdfsDataStore.closeFlieSystem','org.apache.hadoop.fs.FileSystem.close'
'tkv.hdfs.HdfsDataStore.openOutput','org.apache.hadoop.fs.FileSystem.create'
'tkv.hdfs.HdfsDataStore.flushAndCloseOutput','org.apache.hadoop.fs.FSDataOutputStream.flush'
'tkv.hdfs.HdfsDataStore.closeOutput','org.apache.hadoop.fs.FSDataOutputStream.close'
'tkv.hdfs.HdfsDataStore.openInput','org.apache.hadoop.fs.FileSystem.open'
'tkv.hdfs.HdfsDataStore.closeInput','org.apache.hadoop.fs.FSDataInputStream.close'
'tkv.hdfs.HdfsDataStore.get','org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.read'
'tkv.hdfs.HdfsDataStore.deleteRemote','org.apache.hadoop.fs.FileSystem.delete'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.HdfsDirectory','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.createOutput','org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.getRealName','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.openInput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.deleteFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.fileExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.fileLength','org.apache.hadoop.fs.Path.<init>'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.fileModified','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getModificationTime'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.listAll','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.reopenFileSystem','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.HdfsNormalIndexInput.HdfsNormalIndexInput','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.open'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.HdfsNormalIndexInput.readInternal','org.apache.hadoop.fs.FSDataInputStream.read'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.HdfsNormalIndexInput.closeInternal','org.apache.hadoop.fs.FSDataInputStream.close'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.DirectIOHdfsIndexInput.DirectIOHdfsIndexInput','org.apache.hadoop.hdfs.DFSClient.DFSDataInputStream.getVisibleLength'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.DirectIOHdfsIndexInput.closeInternal','org.apache.hadoop.fs.FSDataInputStream.close'
'com.nearinfinity.blur.store.hdfs.HdfsDirectory.DirectIOHdfsIndexInput.readInternal','org.apache.hadoop.fs.FSDataInputStream.readFully'
'com.ning.metrics.action.hdfs.reader.HdfsEntry.HdfsEntry','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getBlockSize org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getReplication org.apache.hadoop.fs.FileStatus.getReplication org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.isDir'
'com.ning.metrics.action.hdfs.reader.HdfsEntry.getPath','org.apache.hadoop.fs.Path.toUri'
'com.nearinfinity.blur.store.hdfs.HdfsFileReader.HdfsFileReader','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.seek'
'com.nearinfinity.blur.store.hdfs.HdfsFileReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'com.nearinfinity.blur.store.hdfs.HdfsFileReader.readBytes','org.apache.hadoop.fs.FSDataInputStream.read'
'com.nearinfinity.blur.store.hdfs.HdfsFileReader.getLength','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.camel.component.hdfs.HdfsFileType.createOutputStream','org.apache.hadoop.util.Progressable.<init> org.apache.hadoop.util.Progressable.<init> org.apache.hadoop.util.Progressable.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.util.Progressable.<init> org.apache.hadoop.util.Progressable.<init> org.apache.hadoop.util.Progressable.<init>'
'org.apache.camel.component.hdfs.HdfsFileType.getHfdsFileToTmpFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileUtil.copy org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileUtil.copyMerge'
'org.apache.camel.component.hdfs.HdfsFileType.append','org.apache.hadoop.io.SequenceFile.Writer.append org.apache.hadoop.io.SequenceFile.Writer.sync'
'org.apache.camel.component.hdfs.HdfsFileType.next','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.camel.component.hdfs.HdfsFileType.getObject','org.apache.hadoop.io.Writable.getClass'
'org.elasticsearch.gateway.hdfs.HdfsGateway.HdfsGateway','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get'
'org.elasticsearch.gateway.hdfs.HdfsGateway.doClose','org.apache.hadoop.fs.FileSystem.close'
'tkv.hdfs.HdfsHelper.createRemoteFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.FileSystem.get'
'tkv.hdfs.HdfsHelper.createLocalFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.setWorkingDirectory'
'org.elasticsearch.common.blobstore.hdfs.HdfsImmutableBlobContainer.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.camel.component.hdfs.HdfsInfo.HdfsInfo','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'com.taobao.top.analysis.HdfsInputAdaptorTest.main','org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.camel.component.hdfs.HdfsInputStream.createInputStream','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.camel.component.hdfs.HdfsInputStream.close','org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.ning.hfind.HdfsItem.HdfsItem','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName'
'com.ning.hfind.HdfsItem.getFullName','org.apache.hadoop.fs.Path.toString'
'radlab.rain.workload.mapreduce.HdfsLoader.RandomInputFormat.getSplits','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileSplit.<init>'
'radlab.rain.workload.mapreduce.HdfsLoader.RandomInputFormat.RandomRecordReader.next','org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.Text.set'
'radlab.rain.workload.mapreduce.HdfsLoader.RandomInputFormat.RandomRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'radlab.rain.workload.mapreduce.HdfsLoader.RandomInputFormat.RandomRecordReader.createValue','org.apache.hadoop.io.Text.<init>'
'radlab.rain.workload.mapreduce.HdfsLoader.RandomInputFormat.getRecordReader','org.apache.hadoop.mapred.FileSplit.getPath'
'radlab.rain.workload.mapreduce.HdfsLoader.Map.map','org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.setStatus'
'radlab.rain.workload.mapreduce.HdfsLoader.Map.configure','org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt'
'radlab.rain.workload.mapreduce.HdfsLoader.randomWrite','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.camel.component.hdfs.HdfsOutputStream.createOutputStream','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.camel.component.hdfs.HdfsOutputStream.close','org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.camel.component.hdfs.integration.HdfsProducerConsumerIntegrationTest.tearDown','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.camel.component.hdfs.HdfsProducerConsumerTest.testSimpleSplitWriteRead','org.apache.hadoop.fs.Path.<init>'
'org.apache.camel.component.hdfs.HdfsProducerConsumerTest.configure','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.camel.component.hdfs.HdfsProducerConsumerTest.tearDown','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.camel.component.hdfs.HdfsProducerSplitTest.testSimpleWriteFileWithIdleSplit','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.camel.component.hdfs.HdfsProducerSplitTest.doTest','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.camel.component.hdfs.HdfsProducerSplitTest.tearDown','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.camel.component.hdfs.HdfsProducerSplitTest.configure','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.camel.component.hdfs.HdfsProducerTest.testProducer','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.toString'
'org.apache.camel.component.hdfs.HdfsProducerTest.testWriteBoolean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.BooleanWritable.get'
'org.apache.camel.component.hdfs.HdfsProducerTest.testWriteByte','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.ByteWritable.get'
'org.apache.camel.component.hdfs.HdfsProducerTest.testWriteInt','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IntWritable.get'
'org.apache.camel.component.hdfs.HdfsProducerTest.testWriteFloat','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.FloatWritable.get'
'org.apache.camel.component.hdfs.HdfsProducerTest.testWriteDouble','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.DoubleWritable.get'
'org.apache.camel.component.hdfs.HdfsProducerTest.testWriteLong','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.LongWritable.get'
'org.apache.camel.component.hdfs.HdfsProducerTest.testWriteText','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.toString'
'org.apache.camel.component.hdfs.HdfsProducerTest.testWriteTextWithKey','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.camel.component.hdfs.HdfsProducerTest.testMapWriteTextWithKey','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.camel.component.hdfs.HdfsProducerTest.testArrayWriteText','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.toString'
'org.apache.camel.component.hdfs.HdfsProducerTest.testBloomMapWriteText','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.camel.component.hdfs.HdfsProducerTest.tearDown','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.camel.component.hdfs.HdfsProducerTest.configure','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.springframework.data.hadoop.fs.HdfsResourceLoader.HdfsResourceLoader','org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.fs.FileSystem.getDefaultUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.compress.CompressionCodecFactory.<init>'
'org.springframework.data.hadoop.fs.HdfsResourceLoader.getClassLoader','org.apache.hadoop.fs.FileSystem.getConf'
'org.springframework.data.hadoop.fs.HdfsResourceLoader.doFindPathMatchingPathResources','org.apache.hadoop.fs.Path.<init>'
'org.springframework.data.hadoop.fs.HdfsResourceLoader.doRetrieveMatchingResources','org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.toUri'
'org.springframework.data.hadoop.fs.HdfsResourceLoader.close','org.apache.hadoop.fs.FileSystem.close'
'org.apache.camel.itest.osgi.hdfs.HdfsRouteTest.testReadString','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'org.apache.camel.itest.osgi.hdfs.HdfsRouteTest.configure','org.apache.hadoop.fs.Path.toUri'
'org.springframework.data.hadoop.scripting.HdfsScriptRunner.detectFS','org.apache.hadoop.fs.FileSystem.get'
'com.oreilly.springdata.hadoop.streaming.HdfsTextFileWriter.prepareOutputStream','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.append'
'com.oreilly.springdata.hadoop.streaming.HdfsTextFileWriter.copy','org.apache.hadoop.fs.FSDataOutputStream.write'
'com.oreilly.springdata.hadoop.streaming.HdfsTextFileWriter.close','org.apache.hadoop.io.IOUtils.closeStream'
'com.oreilly.springdata.batch.item.HdfsTextItemWriter.prepareOutputStream','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.append'
'com.oreilly.springdata.batch.item.HdfsTextItemWriter.copy','org.apache.hadoop.fs.FSDataOutputStream.write'
'com.oreilly.springdata.batch.item.HdfsTextItemWriter.close','org.apache.hadoop.io.IOUtils.closeStream'
'org.cloudata.examples.first.HdfsToCloudataMapReduce.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete'
'org.cloudata.examples.first.HdfsToCloudataMapReduce.HdfsToCloudataMappper.map','org.apache.hadoop.io.Text.toString'
'org.cloudata.examples.first.HdfsToCloudataMapReduce.HdfsToCloudataMappper.configure','org.apache.hadoop.mapred.JobConf.get'
'radlab.rain.workload.mapreduce.HdfsUtil.createPath','org.apache.hadoop.fs.FsShell.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.FsShell.close'
'radlab.rain.workload.mapreduce.HdfsUtil.deletePath','org.apache.hadoop.fs.FsShell.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.FsShell.close'
'com.twitter.elephanttwin.util.HdfsUtils.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'com.twitter.elephanttwin.util.HdfsUtils.reachable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close'
'com.twitter.elephanttwin.util.HdfsUtils.getHdfsFiles','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'com.twitter.elephanttwin.util.HdfsUtils.downloadFileFromHdfs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.getName'
'com.twitter.elephanttwin.util.HdfsUtils.isValidFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getBlockSize'
'com.twitter.elephanttwin.util.HdfsUtils.getHdfsFileSystem','org.apache.hadoop.fs.FileSystem.get'
'com.twitter.elephanttwin.util.HdfsUtils.getHdfsConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.reloadConfiguration'
'com.twitter.elephanttwin.util.HdfsUtils.addInputPathRecursively','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.PathFilter.accept org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.PathFilter.accept'
'com.twitter.elephanttwin.util.HdfsUtils.partFileStatus','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus'
'com.twitter.elephanttwin.util.HdfsUtils.getSubdirectories','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileUtil.stat2Paths'
'com.twitter.elephanttwin.util.HdfsUtils.copyDirContentsToLocal','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'com.twitter.elephanttwin.util.HdfsUtils.openFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.twitter.elephanttwin.util.HdfsUtils.getInputStreamSupplier','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.compress.GzipCodec.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.compress.GzipCodec.setConf org.apache.hadoop.io.compress.GzipCodec.createInputStream'
'com.twitter.elephanttwin.util.HdfsUtils.getModificationTime','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getModificationTime'
'com.twitter.elephanttwin.util.HdfsUtils.isDirectory','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir'
'com.twitter.elephanttwin.util.HdfsUtils.readLines','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'com.twitter.elephanttwin.util.HdfsUtils.writeLines','org.apache.hadoop.fs.FileSystem.create'
'org.apache.accumulo.server.client.HdfsZooInstance.getConnector','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.headings.HeadingsParseFilter.setConf','org.apache.hadoop.conf.Configuration.getStrings'
'com.datasalt.pangool.solr.HeartBeater.HeartBeater','org.apache.hadoop.util.Progressable.getClass'
'com.datasalt.pangool.solr.HeartBeater.run','org.apache.hadoop.util.Progressable.progress'
'com.datasalt.pangool.solr.HeartBeater.needHeartBeat','org.apache.hadoop.util.Progressable.progress'
'com.datasalt.pangool.solr.HeartBeater.setStatus','org.apache.hadoop.mapreduce.TaskInputOutputContext.setStatus'
'org.apache.solr.hadoop.HeartBeater.HeartBeater','org.apache.hadoop.util.Progressable.getClass'
'org.apache.solr.hadoop.HeartBeater.run','org.apache.hadoop.util.Progressable.progress'
'org.apache.solr.hadoop.HeartBeater.needHeartBeat','org.apache.hadoop.util.Progressable.progress'
'org.apache.solr.hadoop.HeartBeater.setStatus','org.apache.hadoop.mapreduce.TaskInputOutputContext.setStatus'
'org.commoncrawl.tutorial.HelloWorld.CSVOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create'
'org.commoncrawl.tutorial.HelloWorld.CSVOutputFormat.CSVRecordWriter.write','org.apache.hadoop.record.CsvRecordOutput.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.record.CsvRecordOutput.writeString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.record.CsvRecordOutput.writeLong'
'org.commoncrawl.tutorial.HelloWorld.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.InputSplit.toString org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobClient.runJob'
'com.linkedin.mr_kluj.HiddenFilePathFilter.accept','org.apache.hadoop.fs.Path.getName'
'edu.jhu.thrax.extraction.HierarchicalRuleExtractor.HierarchicalRuleExtractor','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean'
'edu.jhu.thrax.extraction.HierarchicalRuleExtractor.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.common.HihoTestCase.OutputPathFilter.accept','org.apache.hadoop.fs.Path.getName'
'co.nubetech.hiho.common.HihoTestCase.createTextFileInHDFS','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FSDataOutputStream.close'
'co.nubetech.hiho.common.HihoTestCase.createSequenceFileInHdfs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.dedup.HihoTuple.setKey','org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.dedup.HihoTuple.hashCode','org.apache.hadoop.io.MD5Hash.hashCode'
'co.nubetech.hiho.dedup.HihoTuple.readFields','org.apache.hadoop.io.MD5Hash.<init> org.apache.hadoop.io.MD5Hash.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'co.nubetech.hiho.dedup.HihoTuple.equals','org.apache.hadoop.io.MD5Hash.equals org.apache.hadoop.io.Text.equals'
'co.nubetech.hiho.dedup.HihoTuple.write','org.apache.hadoop.io.MD5Hash.write org.apache.hadoop.io.Text.write'
'co.nubetech.hiho.dedup.HihoTuple.compareTo','org.apache.hadoop.io.MD5Hash.getDigest org.apache.hadoop.io.WritableComparator.compareBytes'
'co.nubetech.hiho.merge.HihoValue.equals','org.apache.hadoop.io.BooleanWritable.equals org.apache.hadoop.io.Text.equals'
'co.nubetech.hiho.merge.HihoValue.getIsOld','org.apache.hadoop.io.BooleanWritable.get'
'co.nubetech.hiho.merge.HihoValue.setVal','org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.merge.HihoValue.setIsOld','org.apache.hadoop.io.BooleanWritable.<init>'
'co.nubetech.hiho.merge.HihoValue.readFields','org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'co.nubetech.hiho.merge.HihoValue.write','org.apache.hadoop.io.BooleanWritable.write org.apache.hadoop.io.Text.write'
'hipi.imagebundle.HipiImageBundle.FileReader.FileReader','org.apache.hadoop.fs.FileSystem.open'
'hipi.imagebundle.HipiImageBundle.writeBundleHeader','org.apache.hadoop.fs.Path.getName'
'hipi.imagebundle.HipiImageBundle.openForWrite','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getDefaultBlockSize org.apache.hadoop.fs.FileSystem.getDefaultReplication org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.create'
'hipi.imagebundle.HipiImageBundle.readBundleHeader','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init>'
'hipi.imagebundle.HipiImageBundle.getDataFile','org.apache.hadoop.fs.FileSystem.get'
'hipi.imagebundle.HipiImageBundle.openForRead','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'hipi.imagebundle.HipiImageBundle.append','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'hipi.unittest.HipiJobTestCase.setUp','org.apache.hadoop.conf.Configuration.<init>'
'hitune.analysis.mapreduce.processor.HistoryLog.MapClass.map','org.apache.hadoop.mapred.OutputCollector<hitune.analysis.mapreduce.processor.K,hitune.analysis.mapreduce.processor.V>.collect'
'hitune.analysis.mapreduce.processor.HistoryLog.ReduceClass.init','org.apache.hadoop.mapred.JobConf.get'
'hitune.analysis.mapreduce.processor.HistoryLog.ReduceClass.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,hitune.analysis.mapreduce.TextArrayWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,hitune.analysis.mapreduce.TextArrayWritable>.collect'
'hitune.analysis.mapreduce.processor.HistoryLog.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'org.springframework.data.hadoop.hive.HiveClientFactoryBean.createHiveClient','org.apache.hadoop.hive.service.HiveClient.<init>'
'org.wso2.carbon.analytics.hive.impl.HiveExecutorServiceImpl.ScriptCallable.call','org.apache.hadoop.hive.metastore.HiveContext.startTenantFlow org.apache.hadoop.hive.metastore.HiveContext.endTenantFlow'
'org.apache.sqoop.hive.HiveImport.getHiveBinPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.sqoop.hive.HiveImport.removeTempLogs','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.sqoop.hive.HiveImport.importTable','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run'
'org.apache.sqoop.hive.HiveImport.cleanUp','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.delete'
'com.ripariandata.timberwolf.writer.hive.HiveMailWriter.loadTempFile','org.apache.hadoop.fs.Path.toString'
'com.ripariandata.timberwolf.writer.hive.HiveMailWriter.getHdfs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'com.ripariandata.timberwolf.writer.hive.HiveMailWriter.writeTemporaryFile','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'com.ripariandata.timberwolf.writer.hive.HiveMailWriter.deleteTempFile','org.apache.hadoop.fs.FileSystem.delete'
'com.ripariandata.timberwolf.writer.hive.HiveMailWriter.closeHdfs','org.apache.hadoop.fs.FileSystem.close'
'com.ml.hadoop.HiveMigrator.execute','org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.newInstance org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.close'
'com.ml.hadoop.HiveMigrator.accept','org.apache.hadoop.fs.Path.getName'
'com.ml.hadoop.HiveMigrator.insertInHive','org.apache.hadoop.hive.jdbc.HiveConnection.createStatement org.apache.hadoop.fs.Path.toUri org.apache.hadoop.hive.jdbc.HiveStatement.execute org.apache.hadoop.hive.jdbc.HiveConnection.close'
'com.ml.hadoop.HiveMigrator.getPartition','org.apache.hadoop.fs.Path.toUri'
'org.apache.pig.piggybank.storage.hiverc.HiveRCRecordReader.close','org.apache.hadoop.hive.ql.io.RCFileRecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable>.close'
'org.apache.pig.piggybank.storage.hiverc.HiveRCRecordReader.getProgress','org.apache.hadoop.hive.ql.io.RCFileRecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable>.getProgress'
'org.apache.pig.piggybank.storage.hiverc.HiveRCRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.hive.ql.io.RCFileRecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable>.<init> org.apache.hadoop.hive.ql.io.RCFileRecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable>.createKey org.apache.hadoop.hive.ql.io.RCFileRecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable>.createValue'
'org.apache.pig.piggybank.storage.hiverc.HiveRCRecordReader.nextKeyValue','org.apache.hadoop.hive.ql.io.RCFileRecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable>.next'
'org.apache.pig.piggybank.storage.hiverc.HiveRCSchemaUtil.extractPigTypeFromHiveType','org.apache.hadoop.hive.serde2.lazy.LazyString.getWritableObject org.apache.hadoop.hive.serde2.lazy.LazyInteger.getWritableObject org.apache.hadoop.hive.serde2.lazy.LazyLong.getWritableObject org.apache.hadoop.hive.serde2.lazy.LazyFloat.getWritableObject org.apache.hadoop.hive.serde2.lazy.LazyDouble.getWritableObject org.apache.hadoop.hive.serde2.lazy.LazyBoolean.getWritableObject org.apache.hadoop.hive.serde2.lazy.LazyByte.getWritableObject org.apache.hadoop.hive.serde2.lazy.LazyShort.getWritableObject'
'org.apache.pig.piggybank.storage.hiverc.HiveRCSchemaUtil.parseLazyMapToPigMap','org.apache.hadoop.hive.serde2.lazy.LazyMap.getMap'
'org.apache.pig.piggybank.storage.hiverc.HiveRCSchemaUtil.parseLazyArrayToPigArray','org.apache.hadoop.hive.serde2.lazy.LazyArray.getList'
'org.springframework.data.hadoop.hive.HiveServerFactoryBean.destroy','org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.clean'
'org.springframework.data.hadoop.hive.HiveServerFactoryBean.afterPropertiesSet','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.common.ServerUtils.cleanUpScratchDir org.apache.hadoop.hive.service.HiveServer.ThriftHiveProcessorFactory.<init>'
'org.wso2.carbon.analytics.hive.internal.HiveServiceComponent.HiveRunnable.initialize','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.common.ServerUtils.cleanUpScratchDir org.apache.hadoop.hive.conf.HiveConf.set'
'com.jointhegrid.hive_test.HiveServicePing.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.jointhegrid.hive_test.HiveServicePing.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'org.springframework.data.hadoop.hive.HiveTemplate.execute','org.apache.hadoop.hive.service.HiveClient.shutdown'
'com.jointhegrid.hive_test.HiveTestBase.getDir','org.apache.hadoop.fs.Path.<init>'
'com.jointhegrid.hive_test.HiveTestBase.setUp','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs'
'com.jointhegrid.hive_test.HiveTestEmbedded.setUp','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.ql.session.SessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start org.apache.hadoop.hive.ql.session.SessionState.getConf'
'com.jointhegrid.hive_test.HiveTestEmbedded.doHiveCommand','org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.get org.apache.hadoop.hive.ql.processors.CommandProcessor.run org.apache.hadoop.hive.ql.processors.CommandProcessor.run'
'com.jointhegrid.hive_test.HiveTestService.setUp','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.hive.service.HiveClient.<init>'
'com.jointhegrid.hive_test.HiveTestService.tearDown','org.apache.hadoop.hive.service.HiveInterface.shutdown'
'co.nubetech.hiho.hive.HiveUtility.createTable','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.getFileOutputStream','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.getTableName','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.getTmpCreateQuery','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.getColumnsForPartitionedCreateTables','org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.getCreateQuery','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.appendDelimitedDataToCreateQuery','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.appendClusteredByToCreateQuery','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.getLoadQuery','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.hive.HiveUtility.runQuery','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.springframework.data.hadoop.hive.HiveUtils.convert','org.apache.hadoop.hive.service.HiveServerException.getErrorCode org.apache.hadoop.hive.service.HiveServerException.getSQLState org.apache.hadoop.hive.service.HiveServerException.getCause org.apache.hadoop.hive.service.HiveServerException.getCause org.apache.hadoop.hive.service.HiveServerException.getMessage'
'org.springframework.data.hadoop.hive.HiveUtils.runWithConversion','org.apache.hadoop.hive.service.HiveClient.shutdown'
'org.springframework.data.hadoop.hive.HiveUtils.run','org.apache.hadoop.io.IOUtils.closeStream'
'org.springframework.data.hadoop.hive.HiveUtils.runCommand','org.apache.hadoop.hive.service.HiveClient.execute org.apache.hadoop.hive.service.HiveClient.fetchAll org.apache.hadoop.hive.service.HiveClient.clean'
'eu.scape_project.tb.lsdr.hocrparser.HocrParser.HocrParserReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init>'
'eu.scape_project.tb.lsdr.hocrparser.HocrParser.HocrParserMapper.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'eu.scape_project.tb.lsdr.hocrparser.HocrParser.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.commoncrawl.mapred.pipelineV3.domainmeta.fuzzydedupe.HostBlacklistByDupesStep.runStep','org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobClient.runJob'
'org.commoncrawl.mapred.pipelineV3.domainmeta.fuzzydedupe.HostBlacklistByIPReducer.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'com.hphoto.util.HphotoConfiguration.create','org.apache.hadoop.hbase.HBaseConfiguration.<init>'
'com.hphoto.util.HphotoConfiguration.get','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.metastore.hsqldb.HsqldbMetaStore.init','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'org.apache.sqoop.metastore.hsqldb.HsqldbMetaStore.waitForServer','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.sqoop.metastore.hsqldb.HsqldbMetaStore.shutdown','org.apache.hadoop.util.StringUtils.stringifyException'
'de.jungblut.crawl.extraction.HtmlExtrator.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.html.HtmlParser.setConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.twitter.corpus.data.HtmlStatusBlockReader.HtmlStatusBlockReader','org.apache.hadoop.fs.FileSystem.getConf'
'com.twitter.corpus.data.HtmlStatusCorpusReader.HtmlStatusCorpusReader','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.listStatus'
'com.twitter.corpus.data.HtmlStatusCorpusReader.next','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.nutch.protocol.httpclient.Http.setConf','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.protocol.httpclient.Http.setCredentials','org.apache.hadoop.conf.Configuration.getConfResourceAsInputStream'
'org.apache.nutch.protocol.http.api.HttpBase.setConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput','org.apache.hadoop.io.Text.toString'
'org.apache.nutch.protocol.http.api.HttpBase.main','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.protocol.httpclient.HttpBasicAuthentication.HttpBasicAuthentication','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.manning.hip.ch2.HttpDownloadMap.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.manning.hip.ch2.HttpDownloadMap.map','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'cascading.tap.hadoop.io.HttpFileSystem.open','org.apache.hadoop.fs.FSDataInputStream.<init>'
'cascading.tap.hadoop.io.HttpFileSystem.getFileStatus','org.apache.hadoop.fs.FileStatus.<init>'
'cascading.tap.hadoop.io.HttpFileSystem.makeUrl','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.HAMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.HAMapperIMC.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.HAMapperIMC.close','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.HAReducer.configure','org.apache.hadoop.mapred.JobConf.getInt'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.HAReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.Norm1Mapper.map','org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.Norm1MapperIMC.close','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.Norm1Combiner.reduce','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.Norm1Reducer.reduce','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.Norm2Mapper.configure','org.apache.hadoop.mapred.JobConf.getFloat org.apache.hadoop.mapred.JobConf.getFloat'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.Norm2Mapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.readSums','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.toString org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.IOUtils.closeStream'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.HACalc','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.Norm','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.example.hits.HubsAndAuthorities.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.sematext.hbase.hut.HutUtil.delete','org.apache.hadoop.hbase.client.Delete.getRow'
'com.sematext.hbase.hut.HutWriteTimeRowsFilter.filterKeyValue','org.apache.hadoop.hbase.KeyValue.getRow'
'com.sematext.hbase.hut.HutWriteTimeRowsFilter.getNextKeyHint','org.apache.hadoop.hbase.KeyValue.createFirstOnRow'
'com.facebook.tsdb.tsdash.server.data.hbase.IDMapSyncLoader.loadMap','org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getValue'
'org.sleuthkit.hadoop.scoring.IIFScoreMapper.map','org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.setSize'
'com.livingsocial.hive.udf.ILike.evaluate','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'org.apache.cassandra.hadoop.fs.INodeTest.Sample1.init','org.apache.hadoop.fs.permission.FsPermission.getDefault'
'org.apache.cassandra.hadoop.fs.INodeTest.Sample2.init','org.apache.hadoop.fs.permission.FsPermission.getDefault'
'com.inadco.hbl.util.IOUtil.cloneWritable','org.apache.hadoop.io.Writable.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Writable.write org.apache.hadoop.io.DataOutputBuffer.close org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.close org.apache.hadoop.util.ReflectionUtils.copy'
'com.inadco.hbl.util.IOUtil.DeletePathOnClose.close','org.apache.hadoop.fs.FileSystem.delete'
'com.inadco.hbl.util.IOUtil.MultipleOutputsCloseable.close','org.apache.hadoop.mapreduce.lib.output.MultipleOutputs<?,?>.close'
'com.inadco.hbl.util.IOUtil.HBaseConnectionCloseable.close','org.apache.hadoop.hbase.client.HConnectionManager.deleteConnection'
'com.inadco.hbl.util.IOUtil.PoolableHtableCloseable.close','org.apache.hadoop.hbase.client.HTablePool.putTable'
'org.apache.gora.util.IOUtils.getOrCreateConf','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.gora.util.IOUtils.serialize','org.apache.hadoop.io.serializer.SerializationFactory.<init> org.apache.hadoop.io.serializer.SerializationFactory.getSerializer org.apache.hadoop.io.serializer.Serializer<org.apache.gora.util.T>.open org.apache.hadoop.io.serializer.Serializer<org.apache.gora.util.T>.serialize org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.serializer.Serializer<org.apache.gora.util.T>.close org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData'
'org.apache.gora.util.IOUtils.deserialize','org.apache.hadoop.io.serializer.SerializationFactory.<init> org.apache.hadoop.io.serializer.SerializationFactory.getDeserializer org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.serializer.Deserializer<org.apache.gora.util.T>.open org.apache.hadoop.io.serializer.Deserializer<org.apache.gora.util.T>.deserialize org.apache.hadoop.io.serializer.Deserializer<org.apache.gora.util.T>.close org.apache.hadoop.io.Text.readString org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.gora.util.IOUtils.writeBoolArray','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.apache.gora.util.IOUtils.readBoolArray','org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.gora.util.IOUtils.writeStringArray','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.Text.writeString'
'org.apache.gora.util.IOUtils.readStringArray','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Text.readString'
'org.apache.gora.util.IOUtils.storeToConf','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.DefaultStringifier.store'
'org.apache.gora.util.IOUtils.loadFromConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.DefaultStringifier.load'
'org.commoncrawl.mapred.pipelineV3.domainmeta.rank.IdSuperDomainsStep.Stage1Reducer.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.rank.IdSuperDomainsStep.Stage2Reducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.rank.IdSuperDomainsStep.runStep','org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobClient.runJob'
'nl.vu.datalayer.hbase.test.IdTest.main','org.apache.hadoop.hbase.util.Bytes.toLong'
'edu.ucsc.srl.damasc.netcdf.tools.Identity.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.ucsc.srl.damasc.netcdf.tools.Identity.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'voldemort.store.readonly.mr.IdentityJsonReducer.reduceObjects','org.apache.hadoop.mapred.OutputCollector<java.lang.Object,java.lang.Object>.collect'
'com.manning.hip.ch13.mrunit.IdentityMapJUnitTest.setUp','org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.<init>'
'com.manning.hip.ch13.mrunit.IdentityMapJUnitTest.testIdentityMapper','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.ucsc.srl.damasc.netcdf.reduce.IdentityReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.LongWritable.get'
'com.manning.hip.ch13.localjobrunner.IdentityTest.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch13.localjobrunner.IdentityTest.runJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch13.localjobrunner.IdentityWithBuilderTest.run','org.apache.hadoop.mapreduce.Job.isSuccessful'
'com.manning.hip.ch13.localjobrunner.IdentityWithBuilderTest.runJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'hipi.imagebundle.mapreduce.ImageBundleRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'org.pentaho.hadoop.mapreduce.converter.converters.ImmutableBytesWritablePassThroughConverter.canConvert','org.apache.hadoop.hbase.io.ImmutableBytesWritable.equals'
'org.apache.giraph.ImmutableClassesGiraphConfiguration.ImmutableClassesGiraphConfiguration','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass'
'org.apache.giraph.ImmutableClassesGiraphConfiguration.createVertexValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.ImmutableClassesGiraphConfiguration.createEdgeValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.ImmutableClassesGiraphConfiguration.createMessageValue','org.apache.hadoop.io.NullWritable.get'
'org.oclc.firefly.hadoop.backup.Import.init','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getName'
'org.oclc.firefly.hadoop.backup.Import.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get'
'org.oclc.firefly.hadoop.backup.Import.getTableNamesFromBackup','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init>'
'org.oclc.firefly.hadoop.backup.Import.importAll','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileUtil.copy'
'org.oclc.firefly.hadoop.backup.Import.doChecks','org.apache.hadoop.hbase.TableExistsException.<init> org.apache.hadoop.hbase.TableNotFoundException.<init>'
'org.oclc.firefly.hadoop.backup.Import.importTable','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.hbase.util.FSTableDescriptors.getTableDescriptor org.apache.hadoop.fs.FileSystem.rename'
'org.oclc.firefly.hadoop.backup.Import.addTableToMeta','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.hbase.HRegionInfo.<init> org.apache.hadoop.hbase.HRegionInfo.readFields org.apache.hadoop.hbase.HRegionInfo.isOffline org.apache.hadoop.hbase.HRegionInfo.setOffline org.apache.hadoop.hbase.HRegionInfo.isSplit org.apache.hadoop.hbase.HRegionInfo.setSplit org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.hbase.HRegionInfo.getRegionName org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Writables.getBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.HRegionInfo.getRegionName org.apache.hadoop.hbase.client.HBaseAdmin.assign'
'org.oclc.firefly.hadoop.backup.Import.tableExists','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.oclc.firefly.hadoop.backup.Import.isValidTable','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.exists'
'gov.llnl.ontology.mapreduce.ingest.ImportCorpusMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.ingest.ImportCorpusMR.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'gov.llnl.ontology.mapreduce.ingest.ImportCorpusMR.ImportCorpusMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'gov.llnl.ontology.mapreduce.ingest.ImportCorpusMR.ImportCorpusMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.sqoop.mapreduce.ImportJobBase.configureOutputFormat','org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.apache.sqoop.mapreduce.ImportJobBase.runJob','org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counters.getGroup'
'org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob','org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.sqoop.mapreduce.ImportJobBase.runImport','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration'
'mapreduce.ImportJsonFromFile.ImportMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init>'
'mapreduce.ImportJsonFromFile.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.commoncrawl.mapred.pipelineV3.domainmeta.quantcast.ImportQuantcastStep.runStep','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob'
'test.modelgen.table.model.ImportRecordLock.setTableName','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ImportTableLock.setTableName','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ImportTarget1.setTextdata1','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ImportTarget2Error.setTextdata2','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.ImportTarget2Error.setErrorCode','org.apache.hadoop.io.Text.modify'
'hbase_mapred1.Importer1.main','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.client.HTable.setWriteBufferSize org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.close'
'com.livingsocial.hive.udf.InArray.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init>'
'com.livingsocial.hive.udf.InArray.evaluate','org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListLength org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElement org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElement org.apache.hadoop.io.BooleanWritable.set'
'org.apache.mahout.classifier.df.mapreduce.inmem.InMemMapper.loadData','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.utils.eval.InMemoryFactorizationEvaluator.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.utils.eval.InMemoryFactorizationEvaluator.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.utils.eval.InMemoryFactorizationEvaluator.readMatrix','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.utils.eval.InMemoryFactorizationEvaluator.readProbePreferences','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'com.bah.culvert.inmemory.InMemoryTable.InMemoryTable','org.apache.hadoop.conf.Configuration.<init>'
'client.IncrementSingleExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.incrementColumnValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.incrementColumnValue'
'org.lilyproject.hbaseindex.IndexEntry.addData','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.hbasene.index.IndexHTablePool.close','org.apache.hadoop.hbase.client.HTable.close'
'org.hbasene.index.IndexHTablePool.newHTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush'
'org.hbasene.index.IndexHTablePool.getTable','org.apache.hadoop.hbase.util.Bytes.toString'
'org.hbasene.index.IndexHTablePool.putTable','org.apache.hadoop.hbase.client.HTable.getTableName org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.mahout.classifier.naivebayes.training.IndexInstancesMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init>'
'org.lilyproject.indexer.engine.test.IndexLockerTest.setUpBeforeClass','org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.<init> org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setClientPort org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup'
'org.lilyproject.indexer.engine.test.IndexLockerTest.tearDownAfterClass','org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown'
'com.nearinfinity.blur.manager.IndexManagerTest.setUp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.hbasene.index.create.mapred.IndexOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.getLocalPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.startLocalOutput org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.hbasene.index.create.LuceneDocumentWrapper>.<init>'
'org.hbasene.index.create.mapred.IndexOutputFormat.write','org.apache.hadoop.util.Progressable.progress'
'org.hbasene.index.create.mapred.IndexOutputFormat.run','org.apache.hadoop.mapred.Reporter.setStatus'
'org.hbasene.index.create.mapred.IndexOutputFormat.close','org.apache.hadoop.fs.FileSystem.completeLocalOutput'
'com.bah.culvert.constraints.IndexRangeConstraint.readFields','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.ObjectWritable.setConf org.apache.hadoop.io.ObjectWritable.readFields org.apache.hadoop.io.ObjectWritable.get'
'com.bah.culvert.constraints.IndexRangeConstraint.write','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.write'
'com.twitter.elephanttwin.verification.IndexScanMapper.setup','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'com.twitter.elephanttwin.verification.IndexScanMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.RecordReader.initialize org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.close org.apache.hadoop.io.LongWritable.<init>'
'com.jaccson.IndexScanner.iterForExpression','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.bixolabs.index.IndexScheme.MyFileOutputFormat.getTaskOutputPath','org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath'
'com.bixolabs.index.IndexScheme.sinkInit','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setClass org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'com.bixolabs.index.IndexScheme.sink','org.apache.hadoop.mapred.OutputCollector.collect'
'com.bixolabs.index.IndexScheme.IndexingOutputFormat.getRecordWriter','org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.RecordWriter<cascading.tuple.Tuple,cascading.tuple.Tuple>.<init>'
'com.bixolabs.index.IndexScheme.IndexingOutputFormat.run','org.apache.hadoop.mapred.Reporter.progress'
'com.bixolabs.index.IndexScheme.IndexingOutputFormat.close','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'org.apache.nutch.searcher.IndexSearcher.IndexSearcher','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'org.apache.nutch.searcher.IndexSearcher.getDirectory','org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.toUri'
'org.apache.nutch.searcher.IndexSearcher.translateHits','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.Text.<init>'
'com.twitter.corpus.demo.IndexStatuses.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.hbasene.index.create.mapred.IndexTableReduce.configure','org.apache.hadoop.mapred.JobConf.get'
'org.hbasene.index.create.mapred.IndexTableReduce.reduce','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.io.ImmutableBytesWritable.getOffset org.apache.hadoop.hbase.io.ImmutableBytesWritable.getLength org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.hbasene.index.create.LuceneDocumentWrapper>.collect'
'org.lilyproject.hbaseindex.test.IndexTest.testSingleStringFieldIndex','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testSingleByteFieldIndex','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testSingleIntFieldIndex','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.hbaseindex.test.IndexTest.testSingleLongFieldIndex','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.hbaseindex.test.IndexTest.testSingleFloatFieldIndex','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testSingleDateTimeFieldIndex','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testSingleDecimalFieldIndex','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testDuplicateValuesIndex','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testMultiFieldIndex','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testDeleteFromIndex','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.hbaseindex.test.IndexTest.testNullIndex','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testIndexEntryVerificationIndex','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testStringPrefixQuery','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testPartialQuery','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testDataTypeChecks','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testExclusiveRanges','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testMinMaxRanges','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testDescendingIntIndex','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testDescendingIntAscendingKeyIndex','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testDescendingStringIndex','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.testData','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.hbaseindex.test.IndexTest.assertResultIds','org.apache.hadoop.hbase.util.Bytes.toString'
'com.senseidb.indexing.hadoop.reduce.IndexUpdateOutputFormat.getRecordWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.RecordWriter<com.senseidb.indexing.hadoop.keyvalueformat.Shard,org.apache.hadoop.io.Text>.<init>'
'com.senseidb.indexing.hadoop.reduce.IndexUpdateOutputFormat.write','org.apache.hadoop.io.Text.equals org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.createNewFile'
'com.bixolabs.tools.IndexWorkflow.createFlow','org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.examples.simple.shard.Index.genPartition','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.examples.simple.shard.Index.index','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.IndexedDocIteratorTest.createSortedMap','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.IndexedDocIteratorTest.createIteratorStack','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.iterators.user.IndexedDocIteratorTest.test1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.iterators.user.IndexedDocIteratorTest.test2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.iterators.user.IndexedDocIteratorTest.test3','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.iterators.user.IndexedDocIteratorTest.test4','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'com.twitter.elephanttwin.retrieval.IndexedFilterRecordReader.initialize','org.apache.hadoop.mapreduce.InputSplit.getLength'
'com.twitter.elephanttwin.retrieval.IndexedFilterRecordReader.computeFileSplits','org.apache.hadoop.mapreduce.lib.input.FileSplit.<init>'
'fm.last.feathers.output.IndexedPlainTypedBytes.IndexingPairWriter.IndexingPairWriter','org.apache.hadoop.typedbytes.TypedBytesOutput.<init>'
'fm.last.feathers.output.IndexedPlainTypedBytes.IndexingPairWriter.write','org.apache.hadoop.typedbytes.TypedBytesWritable.getSize org.apache.hadoop.typedbytes.TypedBytesWritable.get org.apache.hadoop.typedbytes.TypedBytesOutput.writeLong org.apache.hadoop.typedbytes.TypedBytesWritable.getSize'
'fm.last.feathers.output.IndexedPlainTypedBytes.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.cf.taste.hadoop.als.IndexedVarIntWritable.GroupingComparator.compare','org.apache.hadoop.io.WritableComparable.compareTo'
'edu.upenn.cis.cis555.Indexer.Indexer.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.upenn.cis.cis555.Indexer.Indexer.LineIndexMapperModified.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init>'
'edu.upenn.cis.cis555.Indexer.Indexer.LineIndexMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init>'
'edu.upenn.cis.cis555.Indexer.Indexer.LineIndexReducer.reduce','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.lilyproject.indexer.master.IndexerMaster.getJobClient','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init>'
'org.lilyproject.indexer.master.IndexerMaster.startFullIndexBuild','org.apache.hadoop.mapreduce.Job.getJobID org.apache.hadoop.mapreduce.Job.getTrackingURL'
'org.lilyproject.indexer.master.IndexerMaster.prepareDeleteIndex','org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.TaskTracker.RunningJob.killJob'
'org.lilyproject.indexer.master.IndexerMaster.JobStatusWatcher.run','org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.TaskTracker.RunningJob.isComplete org.apache.hadoop.mapred.TaskTracker.RunningJob.getJobState org.apache.hadoop.mapred.TaskTracker.RunningJob.isSuccessful org.apache.hadoop.mapred.TaskTracker.RunningJob.getCounters'
'org.lilyproject.indexer.master.IndexerMaster.JobStatusWatcher.markJobComplete','org.apache.hadoop.mapreduce.Counters.getCounter org.apache.hadoop.mapreduce.Counters.getCounter org.apache.hadoop.mapreduce.Counters.getCounter org.apache.hadoop.mapreduce.Counters.getCounter'
'.IndexerReducer.setup','org.apache.hadoop.io.Text.<init>'
'.IndexerReducer.reduce','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.set'
'.IndexerReducer.addPosting','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.lilyproject.indexer.engine.test.IndexerTest.setUpBeforeClass','org.apache.hadoop.fs.Path.<init>'
'org.lilyproject.indexer.batchbuild.IndexingMapper.setup','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.lilyproject.indexer.batchbuild.IndexingMapper.getIntProp','org.apache.hadoop.conf.Configuration.get'
'org.apache.jena.tdbloader4.InferDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.jena.tdbloader4.InferDriver.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.jena.tdbloader4.InferMapper.setup','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'com.cloudera.recordbreaker.learnstructure.BaseType.readFields','org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.learnstructure.BaseType.write','org.apache.hadoop.io.UTF8.writeString org.apache.hadoop.io.UTF8.writeString'
'org.apache.mahout.math.stats.entropy.InformationGain.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.stats.entropy.InformationGain.prepareArguments','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.stats.entropy.InformationGain.calculateEntropy','org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.stats.entropy.InformationGain.calculateConditionalEntropy','org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.stats.entropy.InformationGain.readDoubleFromPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.math.stats.entropy.InformationGainRatio.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.stats.entropy.InformationGainRatio.run','org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.examples.simple.dirlist.Ingest.buildMutation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.examples.simple.dirlist.Ingest.ingest','org.apache.hadoop.io.Text.<init>'
'gov.llnl.ontology.mapreduce.ingest.IngestCorpusMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.ingest.IngestCorpusMR.setupConfiguration','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'gov.llnl.ontology.mapreduce.ingest.IngestCorpusMR.IngestCorpusMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.cloudera.sqoop.testutil.InjectableConnManager.importTable','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.nutch.crawl.Inlink.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.apache.nutch.crawl.Inlink.skip','org.apache.hadoop.io.Text.skip org.apache.hadoop.io.Text.skip'
'org.apache.nutch.crawl.Inlink.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setIsolated','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setLocalIterators','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setInputInfo','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setZooKeeperInstance','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setMockInstance','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setRanges','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.disableAutoAdjustRanges','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setMaxVersions','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setScanOffline','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.fetchColumns','org.apache.hadoop.conf.Configuration.setStrings'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.setLogLevel','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.addIterator','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.isIsolated','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.usesLocalIterators','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getUsername','org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getPassword','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getTablename','org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getAuthorizations','org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getInstance','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getTabletLocator','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getRanges','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getFetchedColumns','org.apache.hadoop.conf.Configuration.getStringCollection org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getAutoAdjustRanges','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getLogLevel','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.validateOptions','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getMaxVersions','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.isOfflineScan','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getIterators','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.RecordReaderBase.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.binOfflineTable','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.accumulo.core.client.mapreduce.InputFormatBase.RangeInputSplit.getLength','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength'
'org.apache.mahout.clustering.conversion.InputMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.conversion.InputMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getClassByName'
'org.apache.mahout.clustering.conversion.meanshift.InputMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.pig.newplan.logical.rules.InputOutputFileValidator.InputOutputFileVisitor.visit','org.apache.hadoop.mapreduce.Job.<init>'
'com.cloudera.science.matching.crunch.InputPreparer.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.jena.tdbloader4.partitioners.InputSampler.writePartitionFile','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getInputFormatClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.Job.getNumReduceTasks org.apache.hadoop.mapreduce.Job.getSortComparator org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.getMapOutputKeyClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.RawComparator<org.apache.jena.tdbloader4.partitioners.K>.compare'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator.estimateNumberOfReducers','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator.getTotalInputFileSize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus'
'org.apache.giraph.graph.InputSplitPathOrganizer.getLocationsFromZkInputSplitData','org.apache.hadoop.io.Text.readString'
'org.apache.pig.tools.pigstats.InputStats.InputStats','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'org.apache.oozie.action.hadoop.InsertTestToken.addtoJobConf','org.apache.hadoop.security.token.Token<org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier>.<init> org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.io.Text.<init>'
'org.apache.oozie.action.hadoop.InsertTestToken.addtoJobConf','org.apache.hadoop.security.token.Token<org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier>.<init> org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.examples.simple.helloworld.InsertWithBatchWriter.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.examples.simple.helloworld.InsertWithOutputFormat.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,org.apache.accumulo.core.data.Mutation>.write org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,org.apache.accumulo.core.data.Mutation>.close'
'org.apache.accumulo.examples.simple.helloworld.InsertWithOutputFormat.main','org.apache.hadoop.util.ToolRunner.run'
'hitune.analysis.mapreduce.processor.InstrumentDataflow.MapClass.parsePhase','org.apache.hadoop.mapred.JobConf.get'
'hitune.analysis.mapreduce.processor.InstrumentDataflow.MapClass.init','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'hitune.analysis.mapreduce.processor.InstrumentDataflow.MapClass.map','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.OutputCollector<hitune.analysis.mapreduce.processor.K,hitune.analysis.mapreduce.processor.V>.collect'
'hitune.analysis.mapreduce.processor.InstrumentDataflow.ReduceClass.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,hitune.analysis.mapreduce.TextArrayWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,hitune.analysis.mapreduce.TextArrayWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,hitune.analysis.mapreduce.TextArrayWritable>.collect'
'hitune.analysis.mapreduce.processor.InstrumentDataflow.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'hitune.analysis.mapreduce.processor.InstrumentSamplingTop.MapClass.parsePhase','org.apache.hadoop.mapred.JobConf.get'
'hitune.analysis.mapreduce.processor.InstrumentSamplingTop.MapClass.init','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'hitune.analysis.mapreduce.processor.InstrumentSamplingTop.MapClass.map','org.apache.hadoop.mapred.OutputCollector<hitune.analysis.mapreduce.processor.K,hitune.analysis.mapreduce.processor.V>.collect'
'hitune.analysis.mapreduce.processor.InstrumentSamplingTop.ReduceClass.reduce','org.apache.hadoop.mapred.OutputCollector<hitune.analysis.mapreduce.processor.K,hitune.analysis.mapreduce.processor.V>.collect'
'hitune.analysis.mapreduce.processor.InstrumentSamplingTop.TopClass.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean'
'hitune.analysis.mapreduce.processor.InstrumentSamplingTop.TopClass.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,hitune.analysis.mapreduce.TextArrayWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,hitune.analysis.mapreduce.TextArrayWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,hitune.analysis.mapreduce.TextArrayWritable>.collect'
'hitune.analysis.mapreduce.processor.InstrumentSamplingTop.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.oozie.util.Instrumentation.size','org.apache.hadoop.conf.Configuration.size'
'org.apache.oozie.util.Instrumentation.isEmpty','org.apache.hadoop.conf.Configuration.size'
'org.apache.oozie.util.Instrumentation.containsKey','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.Instrumentation.get','org.apache.hadoop.conf.Configuration.get'
'vecAdd.IntArrayWritable.IntArrayWritable','org.apache.hadoop.io.IntWritable.<init>'
'vecAdd.IntArrayWritable.toString','org.apache.hadoop.io.Writable.toString'
'vecAdd.IntArrayWritable.toIntArray','org.apache.hadoop.io.Writable.toString'
'ivory.core.data.document.IntDocVectorsForwardIndex.IntDocVectorsForwardIndex','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readLong'
'ivory.core.data.document.IntDocVectorsForwardIndex.getDocVector','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'ivory.core.data.document.IntDocVectorsForwardIndex.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'skywriting.examples.skyhout.pagerank.IntListCombiner.combine','org.apache.hadoop.io.IntWritable.get'
'skywriting.examples.skyhout.pagerank.IntListCombiner.combineInit','org.apache.hadoop.io.IntWritable.get'
'org.apache.giraph.aggregators.IntMaxAggregator.aggregate','org.apache.hadoop.io.IntWritable.get'
'org.apache.giraph.aggregators.IntMaxAggregator.createInitialValue','org.apache.hadoop.io.IntWritable.<init>'
'org.goldenorb.types.message.IntMessage.IntMessage','org.apache.hadoop.io.IntWritable.<init>'
'org.goldenorb.types.message.IntMessage.get','org.apache.hadoop.io.IntWritable.get'
'org.goldenorb.types.message.IntMessage.set','org.apache.hadoop.io.IntWritable.set'
'org.apache.giraph.aggregators.IntMinAggregator.aggregate','org.apache.hadoop.io.IntWritable.get'
'org.apache.giraph.aggregators.IntMinAggregator.createInitialValue','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.giraph.aggregators.IntOverwriteAggregator.aggregate','org.apache.hadoop.io.IntWritable.get'
'org.apache.giraph.aggregators.IntOverwriteAggregator.createInitialValue','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.common.IntPairWritable.FirstGroupingComparator.compare','org.apache.hadoop.io.WritableComparator.readInt org.apache.hadoop.io.WritableComparator.readInt'
'org.apache.mahout.common.IntPairWritable.FirstGroupingComparator.compare','org.apache.hadoop.io.WritableComparator.readInt org.apache.hadoop.io.WritableComparator.readInt'
'org.chombo.util.IntPair.IntPair','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.chombo.util.IntPair.readFields','org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.IntWritable.readFields'
'org.chombo.util.IntPair.write','org.apache.hadoop.io.IntWritable.write org.apache.hadoop.io.IntWritable.write'
'org.chombo.util.IntPair.compareTo','org.apache.hadoop.io.IntWritable.compareTo org.apache.hadoop.io.IntWritable.compareTo'
'org.chombo.util.IntPair.hashCode','org.apache.hadoop.io.IntWritable.hashCode org.apache.hadoop.io.IntWritable.hashCode'
'org.chombo.util.IntPair.baseHashCode','org.apache.hadoop.io.IntWritable.hashCode'
'org.chombo.util.IntPair.equals','org.apache.hadoop.io.IntWritable.equals org.apache.hadoop.io.IntWritable.equals'
'org.chombo.util.IntPair.toString','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'org.goldenorb.util.message.IntSourceMessage.IntSourceMessage','org.apache.hadoop.io.IntWritable.<init>'
'org.goldenorb.util.message.IntSourceMessage.get','org.apache.hadoop.io.IntWritable.get'
'org.goldenorb.util.message.IntSourceMessage.set','org.apache.hadoop.io.IntWritable.set'
'org.apache.giraph.aggregators.IntSumAggregator.aggregate','org.apache.hadoop.io.IntWritable.get'
'org.apache.giraph.aggregators.IntSumAggregator.createInitialValue','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.common.IntTuple.readFields','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.common.IntTuple.write','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.write'
'org.apache.mahout.common.IntTuple.readFields','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.common.IntTuple.write','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.write'
'com.twitter.elephantbird.pig.util.IntWritableConverter.IntWritableConverter','org.apache.hadoop.io.IntWritable.<init>'
'com.twitter.elephantbird.pig.util.IntWritableConverter.toCharArray','org.apache.hadoop.io.IntWritable.get'
'com.twitter.elephantbird.pig.util.IntWritableConverter.toInteger','org.apache.hadoop.io.IntWritable.get'
'com.twitter.elephantbird.pig.util.IntWritableConverter.toLong','org.apache.hadoop.io.IntWritable.get'
'com.twitter.elephantbird.pig.util.IntWritableConverter.toFloat','org.apache.hadoop.io.IntWritable.get'
'com.twitter.elephantbird.pig.util.IntWritableConverter.toDouble','org.apache.hadoop.io.IntWritable.get'
'org.pentaho.hadoop.mapreduce.converter.converters.IntWritableToLongConverter.canConvert','org.apache.hadoop.io.IntWritable.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.IntWritableToLongConverter.convert','org.apache.hadoop.io.IntWritable.get'
'org.pentaho.hadoop.mapreduce.converter.converters.IntWritableToLongConverterTest.convert','org.apache.hadoop.io.IntWritable.<init>'
'org.lilyproject.hbaseindex.IntegerIndexFieldDefinition.toBytes','org.apache.hadoop.hbase.util.Bytes.putInt'
'org.apache.mahout.fpm.pfpgrowth.convertors.integer.IntegerStringOutputConverter.collect','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns>.collect'
'org.apache.mahout.fpm.pfpgrowth.convertors.integer.IntegerStringOutputConverter.collect','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns>.collect'
'edu.umd.cloud9.integration.collection.trec.IntegrationTest.testDocnoMapping','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.integration.collection.trec.IntegrationTest.testDemoCountDocs','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists'
'edu.umd.cloud9.integration.collection.trec.IntegrationTest.testForwardIndex','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'mia.clustering.ch10.InterClusterDistances.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.df.mapreduce.partial.InterResults.load','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.mahout.df.mapreduce.partial.InterResults.store','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeInt org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.accumulo.core.iterators.user.IntersectingIteratorTest.createSortedMap','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.IntersectingIteratorTest.test1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.IntersectingIteratorTest.test2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.IntersectingIteratorTest.test3','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.IntersectingIteratorTest.test4','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.IntersectingIteratorTest.testWithBatchScanner','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.writeURLFPToStream','org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.readURLFPFromStream','org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.findLatestDatabaseTimestamp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.runJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.mapred.JobConf.setCompressMapOutput org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.Phase1KeyComparator.compare','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.Phase2LinkDataReducer.reduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setClass org.apache.hadoop.mapred.JobConf.setClass org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.protocol.URLFPV2,org.commoncrawl.protocol.CrawlURLMetadata>.collect'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.Phase2LinkDataReducer.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.FileOutputFormat.getWorkOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.InvertedLinkDBReducer.reduce','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.DataInputBuffer.getLength org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.protocol.URLFPV2,org.apache.hadoop.io.BytesWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.InvertedLinkDBReducer.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getInt'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.LinkInverter.map','org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.protocol.URLFPV2,org.apache.hadoop.io.BytesWritable>.collect'
'org.commoncrawl.mapred.pipelineV1.InverseLinkDBWriterV3.LinkInverter.configure','org.apache.hadoop.mapred.JobConf.getNumReduceTasks'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.findLatestDatabaseTimestamp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.runJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.LinkDataInverter.reduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setClass org.apache.hadoop.mapred.JobConf.setClass org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.mapred.Reporter.progress org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.DataInputBuffer.readInt org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeFloat org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.LinkDataInverter.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.fs.FileSystem.get'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.ComplexKeyComparator.compareRaw','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.ComplexKeyComparator._compareRaw','org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.DataInputBuffer.readFloat org.apache.hadoop.io.DataInputBuffer.readFloat org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.getData org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.DataInputBuffer.getData org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.WritableComparator.compareBytes'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.ComplexKeyComparator.compare','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.ComplexKeyComparator.genTestKey','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeFloat org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.ComplexKeyComparator.testComparator','org.apache.hadoop.io.DataOutputBuffer.<init>'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.LinkDataResorter.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.fs.FileSystem.get'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.LinkDataResorter.reduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.getWorkOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.LocalDirAllocator.<init> org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.mapred.Reporter.progress org.apache.hadoop.fs.Path.toString'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.LinkDataResorter.spillRawRecord','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.readLong'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.LinkDataResorter.spillRecord','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readLong'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.LinkDataResorter.recordSpillAndPotentiallyAddIndexItem','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.IntWritable>.collect'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.runPhase1','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.runPhase2','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.AddShardIndexMapper.map','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.commoncrawl.util.FlexBuffer>.collect'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.AddShardIndexMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.TFileIndexWriter.reduce','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readInt org.apache.hadoop.io.DataInputBuffer.readInt org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.TFileIndexWriter.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.FileOutputFormat.getWorkOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.TFileIndexWriter.close','org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.runPhase3','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.runDebugJob','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.DebugMapper.map','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.collect'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.DebugMapper.configure','org.apache.hadoop.mapred.JobConf.getLong'
'org.commoncrawl.mapred.pipelineV1.InverseLinksByDomainDBBuilder.runDebugPhase1','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete'
'com.manning.hip.ch12.crunch.InvertedIndex.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.accumulo.core.client.IsolatedScanner.RowBufferingIterator.readRow','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'org.sifarish.common.ItemDynamicAttributeSimilarity.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.sifarish.common.ItemDynamicAttributeSimilarity.SimilarityMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'org.sifarish.common.ItemDynamicAttributeSimilarity.SimilarityReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'org.sifarish.common.ItemDynamicAttributeSimilarity.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.hadoop.item.ItemFilterMapper.map','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.cf.taste.hadoop.item.ItemIDIndexMapper.setup','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.cf.taste.hadoop.item.ItemIDIndexMapper.map','org.apache.hadoop.io.Text.toString'
'com.asakusafw.example.direct.seqfile.writable.ItemInfoWritable.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'com.asakusafw.example.direct.seqfile.writable.ItemInfoWritable.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.run','org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.MostSimilarItemPairsMapper.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.MostSimilarItemPairsMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testMostSimilarItemsPairsMapper','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testMostSimilarItemPairsReducer','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testCompleteJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testMaxSimilaritiesPerItem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'com.snowballfinance.kddc.job.ItemSimilarityJobTest.testJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testCountUsersReducer','org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testMostSimilarItemsPairsMapper','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testMostSimilarItemPairsReducer','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testCompleteJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest.testMaxSimilaritiesPerItem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'com.snowballfinance.kddc.job.ItemSimilarityJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.snowballfinance.kddc.job.ItemSimilarityJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.snowballfinance.kddc.job.ItemSimilarityJob.CateAndKeyWordMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.FloatWritable.<init>'
'com.snowballfinance.kddc.job.ItemSimilarityJob.ScoreReducer.reduce','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.bfs.IterateBFS.MapClass.map','org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.example.bfs.IterateBFS.MapClass.cleanup','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.example.bfs.IterateBFS.ReduceClass.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.example.bfs.IterateBFS.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.example.bfs.IterateBFS.main','org.apache.hadoop.util.ToolRunner.run'
'nl.waredingen.graphs.partition.IterateWithFlagsJob.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'nl.waredingen.graphs.partition.IterateWithFlagsJob.MaxPartitionToAdjacencyList.complete','org.apache.hadoop.util.StringUtils.joinObjects org.apache.hadoop.util.StringUtils.joinObjects'
'org.apache.accumulo.core.client.IteratorSetting.Column.Column','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.IteratorSetting.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString'
'org.apache.accumulo.core.client.IteratorSetting.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString'
'org.apache.ivory.workflow.IvoryPostProcessing.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'cascading.jdbc.JDBCScheme.sourceInit','org.apache.hadoop.mapred.JobConf.setInputFormat'
'cascading.jdbc.JDBCScheme.sinkInit','org.apache.hadoop.mapred.JobConf.setOutputFormat'
'cascading.jdbc.JDBCScheme.sink','org.apache.hadoop.mapred.OutputCollector.collect org.apache.hadoop.mapred.OutputCollector.collect org.apache.hadoop.mapred.OutputCollector.collect'
'org.wso2.carbon.hadoop.hive.jdbc.storage.input.JDBCSplit.getSplits','org.apache.hadoop.mapred.FileInputFormat.getInputPaths'
'org.wso2.carbon.hadoop.hive.jdbc.storage.JDBCStorageHandler.configureTableJobProperties','org.apache.hadoop.hive.ql.plan.TableDesc.getProperties'
'org.wso2.carbon.hadoop.hive.jdbc.storage.JDBCStorageHandler.preCreateTable','org.apache.hadoop.hive.metastore.MetaStoreUtils.isExternalTable org.apache.hadoop.hive.metastore.api.MetaException.<init> org.apache.hadoop.hive.metastore.api.Table.getParameters'
'com.twitter.maple.jdbc.JDBCTap.getPath','org.apache.hadoop.fs.Path.<init>'
'com.twitter.maple.jdbc.JDBCTap.sourceConfInit','org.apache.hadoop.mapred.FileInputFormat.setInputPaths'
'com.twitter.maple.jdbc.JDBCTap.sinkConfInit','org.apache.hadoop.mapred.JobConf.get'
'com.twitter.maple.jdbc.JDBCTapCollector.JDBCTapCollector','org.apache.hadoop.mapred.JobConf.<init>'
'com.twitter.maple.jdbc.JDBCTapCollector.initialize','org.apache.hadoop.mapred.JobConf.getOutputFormat org.apache.hadoop.mapred.OutputFormat.getClass org.apache.hadoop.mapred.OutputFormat.getRecordWriter'
'com.twitter.maple.jdbc.JDBCTapCollector.close','org.apache.hadoop.mapreduce.RecordWriter.close'
'com.twitter.maple.jdbc.JDBCTapCollector.collect','org.apache.hadoop.mapreduce.RecordWriter.write'
'cascading.jdbc.JDBCTap.getPath','org.apache.hadoop.fs.Path.<init>'
'cascading.jdbc.JDBCTap.sourceInit','org.apache.hadoop.mapred.FileInputFormat.setInputPaths'
'cascading.jdbc.JDBCTap.sinkInit','org.apache.hadoop.mapred.JobConf.get'
'com.inadco.ecoadapters.ecor.tests.JRITest.scratchpad','org.apache.hadoop.mapreduce.Job.setJobName'
'org.sleuthkit.hadoop.JSONClusterNameMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.searcher.response.json.JSONResponseWriter.setConf','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.childtv.hadoop.hbase.mapred.JSONTableInputFormat.formatRowResultWithTimestamp','org.apache.hadoop.hbase.io.RowResult.entrySet'
'org.childtv.hadoop.hbase.mapred.JSONTableInputFormat.formatRowResultWithoutTimestamp','org.apache.hadoop.hbase.io.RowResult.entrySet'
'com.tomslabs.grid.avro.JSONTextToAvroRecordReducer.configure','org.apache.hadoop.mapred.JobConf.get'
'com.tomslabs.grid.avro.JSONTextToAvroRecordReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.avro.mapred.AvroWrapper<org.apache.avro.generic.GenericRecord>,org.apache.hadoop.io.NullWritable>.collect'
'org.springframework.data.hadoop.mapreduce.JarExecutor.preExecution','org.apache.hadoop.conf.Configuration.writeXml'
'org.springframework.data.hadoop.mapreduce.JarTests.testBadMainClassConfiguration','org.apache.hadoop.conf.Configuration.getClassLoader org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.springframework.data.hadoop.mapreduce.JarTests.testOtherMainClassConfiguration','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.JavaActionExecutor.initActionType','org.apache.hadoop.fs.permission.AccessControlException.getName'
'org.apache.oozie.action.hadoop.JavaActionExecutor.checkForDisallowedProps','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.JavaActionExecutor.createBaseHadoopConf','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.injectLauncherProperties','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.setupLauncherConf','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.JavaActionExecutor.parseJobXmlAndConfiguration','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.action.hadoop.JavaActionExecutor.setupActionConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.hadoop.JavaActionExecutor.addToCache','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheArchive org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.createSymlink'
'org.apache.oozie.action.hadoop.JavaActionExecutor.getOozieLauncherJar','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.JavaActionExecutor.prepareActionDir','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.hadoop.JavaActionExecutor.cleanUpActionDir','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.hadoop.JavaActionExecutor.addShareLib','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.oozie.action.hadoop.JavaActionExecutor.addActionLibs','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.oozie.action.hadoop.JavaActionExecutor.setLibFilesArchives','org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.oozie.action.hadoop.JavaActionExecutor.createLauncherConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.getParent org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean'
'org.apache.oozie.action.hadoop.JavaActionExecutor.injectCallback','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.actionConfToLauncherConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.getParent org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.JobClient.getDelegationToken org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>.getKind org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>.getKind org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.RunningJob.getID org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.RunningJob.getTrackingURL org.apache.hadoop.mapred.JobClient.close'
'org.apache.oozie.action.hadoop.JavaActionExecutor.setCredentialPropertyToActionConf','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.check','org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getTrackingURL org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapred.JobClient.close'
'org.apache.oozie.action.hadoop.JavaActionExecutor.getActionData','org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.action.hadoop.JavaActionExecutor.kill','org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.killJob org.apache.hadoop.mapred.JobClient.close'
'org.apache.oozie.action.hadoop.JavaActionExecutor.getShareLibName','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.JavaActionExecutor.initActionType','org.apache.hadoop.fs.permission.AccessControlException.getName'
'org.apache.oozie.action.hadoop.JavaActionExecutor.checkForDisallowedProps','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.JavaActionExecutor.createBaseHadoopConf','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.setupActionConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.action.hadoop.JavaActionExecutor.addToCache','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheArchive org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.createSymlink'
'org.apache.oozie.action.hadoop.JavaActionExecutor.getOozieLauncherJar','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.JavaActionExecutor.prepareActionDir','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.hadoop.JavaActionExecutor.cleanUpActionDir','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.hadoop.JavaActionExecutor.setLibFilesArchives','org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.oozie.action.hadoop.JavaActionExecutor.createLauncherConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.getParent org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.injectCallback','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.getParent org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>.getKind org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>.getKind org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.RunningJob.getID org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.RunningJob.getTrackingURL org.apache.hadoop.mapred.JobClient.close'
'org.apache.oozie.action.hadoop.JavaActionExecutor.setCredentialPropertyToActionConf','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.JavaActionExecutor.check','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getTrackingURL org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapred.JobClient.close'
'org.apache.oozie.action.hadoop.JavaActionExecutor.kill','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.killJob org.apache.hadoop.mapred.JobClient.close'
'com.cloudera.sqoop.manager.JdbcMySQLExportTest.setUp','org.apache.hadoop.util.StringUtils.stringifyException'
'com.asakusafw.dmdl.windgate.jdbc.driver.JdbcSupportEmitterTest.simple','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.windgate.jdbc.driver.JdbcSupportEmitterTest.types','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.hadoop.io.JetS3tARCSource.getInputPrefixes','org.apache.hadoop.mapred.JobConf.get'
'org.commoncrawl.hadoop.io.JetS3tARCSource.setAWSAccessKeyID','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.hadoop.io.JetS3tARCSource.setAWSSecretAccessKey','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.hadoop.io.JetS3tARCSource.setBucketName','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.hadoop.io.JetS3tARCSource.setInputPrefixes','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.hadoop.io.JetS3tARCSource.setMaxRetries','org.apache.hadoop.mapred.JobConf.setInt'
'org.commoncrawl.hadoop.io.JetS3tARCSource.configureImpl','org.apache.hadoop.mapred.JobConf.getInt'
'org.commoncrawl.hadoop.io.JetS3tARCSource.getProperty','org.apache.hadoop.mapred.JobConf.get'
'.JobBuilder.JobBuilder','org.apache.hadoop.mapred.JobConf.<init>'
'.JobBuilder.parseInputAndOutput','org.apache.hadoop.util.Tool.getClass org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'.JobBuilder.printUsage','org.apache.hadoop.util.Tool.getClass org.apache.hadoop.util.GenericOptionsParser.printGenericCommandUsage'
'.JobBuilder.withCommandLineArgs','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.GenericOptionsParser.printGenericCommandUsage org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.commoncrawl.util.JobBuilder.JobBuilder','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.mapred.JobConf.setJobName'
'org.commoncrawl.util.JobBuilder.inputs','org.apache.hadoop.mapred.FileInputFormat.addInputPath'
'org.commoncrawl.util.JobBuilder.input','org.apache.hadoop.mapred.FileInputFormat.addInputPath'
'org.commoncrawl.util.JobBuilder.output','org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'org.commoncrawl.util.JobBuilder.inputFormat','org.apache.hadoop.mapred.JobConf.setInputFormat'
'org.commoncrawl.util.JobBuilder.inputIsSeqFile','org.apache.hadoop.mapred.JobConf.setInputFormat'
'org.commoncrawl.util.JobBuilder.outputFormat','org.apache.hadoop.mapred.JobConf.setOutputFormat'
'org.commoncrawl.util.JobBuilder.outputIsSeqFile','org.apache.hadoop.mapred.JobConf.setOutputFormat'
'org.commoncrawl.util.JobBuilder.jarByClass','org.apache.hadoop.mapred.JobConf.setJarByClass'
'org.commoncrawl.util.JobBuilder.mapper','org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setJarByClass'
'org.commoncrawl.util.JobBuilder.mapRunner','org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapred.JobConf.setJarByClass'
'org.commoncrawl.util.JobBuilder.mapperKeyValue','org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.commoncrawl.util.JobBuilder.reducer','org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setJarByClass'
'org.commoncrawl.util.JobBuilder.outputKeyValue','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.commoncrawl.util.JobBuilder.keyValue','org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.commoncrawl.util.JobBuilder.numMappers','org.apache.hadoop.mapred.JobConf.setNumMapTasks'
'org.commoncrawl.util.JobBuilder.numReducers','org.apache.hadoop.mapred.JobConf.setNumReduceTasks'
'org.commoncrawl.util.JobBuilder.compressMapOutput','org.apache.hadoop.mapred.JobConf.setCompressMapOutput'
'org.commoncrawl.util.JobBuilder.compressor','org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.io.SequenceFile.CompressionType.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setClass'
'org.commoncrawl.util.JobBuilder.compressType','org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.io.SequenceFile.CompressionType.toString org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.util.JobBuilder.sort','org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass'
'org.commoncrawl.util.JobBuilder.group','org.apache.hadoop.mapred.JobConf.setOutputValueGroupingComparator'
'org.commoncrawl.util.JobBuilder.partition','org.apache.hadoop.mapred.JobConf.setPartitionerClass'
'org.commoncrawl.util.JobBuilder.speculativeExecution','org.apache.hadoop.mapred.JobConf.setSpeculativeExecution'
'org.commoncrawl.util.JobBuilder.speculativeMapExecution','org.apache.hadoop.mapred.JobConf.setMapSpeculativeExecution'
'org.commoncrawl.util.JobBuilder.speculativeReducerExecution','org.apache.hadoop.mapred.JobConf.setReduceSpeculativeExecution'
'org.commoncrawl.util.JobBuilder.maxMapAttempts','org.apache.hadoop.mapred.JobConf.setMaxMapAttempts'
'org.commoncrawl.util.JobBuilder.maxReduceAttempts','org.apache.hadoop.mapred.JobConf.setMaxReduceAttempts'
'org.commoncrawl.util.JobBuilder.delayReducersUntil','org.apache.hadoop.mapred.JobConf.setFloat'
'org.commoncrawl.util.JobBuilder.maxMapTaskFailures','org.apache.hadoop.mapred.JobConf.setMaxMapTaskFailuresPercent'
'org.commoncrawl.util.JobBuilder.setAffinity','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get'
'org.commoncrawl.util.JobBuilder.reuseJVM','org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm'
'org.commoncrawl.util.JobBuilder.setAffinityNoBalancing','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getInt'
'org.commoncrawl.util.JobBuilder.minSplitSize','org.apache.hadoop.mapred.JobConf.setLong'
'org.commoncrawl.util.JobBuilder.tempDir','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.util.JobBuilder.set','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setLong'
'com.zinnia.nectar.util.hadoop.JobCreatorUtil.createJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass'
'org.springframework.data.hadoop.mapreduce.JobExecutor.executeJobs','org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.mapred.RunningJob.getFailureInfo'
'org.springframework.data.hadoop.mapreduce.JobFactoryBean.getObjectType','org.apache.hadoop.mapreduce.Job.getClass'
'org.springframework.data.hadoop.mapreduce.JobFactoryBean.afterPropertiesSet','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser org.apache.hadoop.security.UserGroupInformation.doAs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapred.JobConf.setJar org.apache.hadoop.mapred.JobConf.setClassLoader org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setWorkingDirectory org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass'
'org.springframework.data.hadoop.mapreduce.JobFactoryBean.run','org.apache.hadoop.mapreduce.Job.<init>'
'org.springframework.data.hadoop.mapreduce.JobFactoryBean.configureMapperTypesIfPossible','org.apache.hadoop.mapreduce.Mapper.equals org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass'
'com.manning.hip.common.JobHelper.addToCache','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified'
'com.manning.hip.common.JobHelper.addJarForJob','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.StringUtils.arrayToString org.apache.hadoop.conf.Configuration.set'
'com.manning.hip.ch6.JobHistoryHelper.getJobInfoFromCliArgs','org.apache.hadoop.conf.Configuration.<init>'
'com.manning.hip.ch6.JobHistoryHelper.accept','org.apache.hadoop.fs.Path.getName'
'com.manning.hip.ch6.JobHistoryHelper.getJobInfoFromHdfsOutputDir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.DefaultJobHistoryParser.parseJobTasks'
'com.manning.hip.ch6.JobHistoryHelper.getJobInfoFromLocalFile','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.DefaultJobHistoryParser.parseJobTasks'
'com.manning.hip.ch6.JobHistoryHelper.extractCounter','org.apache.hadoop.mapred.Counters.fromEscapedCompactString'
'org.apache.hama.bsp.JobImpl.JobImpl','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.bsp.JobImpl.getMemoryRequirements','org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.bsp.JobImpl.startJob','org.apache.hadoop.yarn.util.BuilderUtils.newAllocateRequest org.apache.hadoop.yarn.api.AMRMProtocol.allocate org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse.getAMResponse org.apache.hadoop.yarn.api.records.AMResponse.getResponseId org.apache.hadoop.yarn.api.records.AMResponse.getAllocatedContainers org.apache.hadoop.yarn.api.records.AMResponse.getAvailableResources org.apache.hadoop.yarn.api.records.AMResponse.getResponseId org.apache.hadoop.yarn.api.records.AMResponse.getAllocatedContainers org.apache.hadoop.yarn.api.records.Container.getId org.apache.hadoop.yarn.api.records.Container.getNodeId org.apache.hadoop.yarn.api.records.Container.getNodeId org.apache.hadoop.yarn.api.records.Container.getNodeHttpAddress org.apache.hadoop.yarn.api.records.Container.getState org.apache.hadoop.yarn.api.records.Container.getResource org.apache.hadoop.yarn.api.records.Container.getNodeId org.apache.hadoop.yarn.api.records.Container.getNodeId org.apache.hadoop.net.NetUtils.createSocketAddr org.apache.hadoop.yarn.ipc.YarnRPC.getProxy'
'org.apache.hama.bsp.JobImpl.createBSPTaskRequest','org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.ResourceRequest.setHostName org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.Priority.setPriority org.apache.hadoop.yarn.api.records.ResourceRequest.setPriority org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.Resource.setMemory org.apache.hadoop.yarn.api.records.ResourceRequest.setCapability org.apache.hadoop.yarn.api.records.ResourceRequest.setNumContainers'
'org.apache.hama.bsp.JobInProgress.JobInProgress','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile'
'org.apache.hama.bsp.JobInProgress.initTasks','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.hama.bsp.JobInProgress.updateTaskStatus','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.hama.bsp.JobInProgress.garbageCollect','org.apache.hadoop.fs.LocalFileSystem.delete org.apache.hadoop.fs.LocalFileSystem.delete org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.delete'
'org.fluxua.driver.JobLauncher.run','org.apache.hadoop.util.ToolRunner.run'
'org.goldenorb.JobManager.launchJob','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.InvalidJobConfException.getMessage'
'org.goldenorb.JobManager.heartbeat','org.apache.hadoop.io.LongWritable.get'
'org.goldenorb.JobManager.removeJobsHDFSfiles','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toString'
'edu.duke.starfish.jobopt.optimizer.JobOptimizer.JobOptimizer','org.apache.hadoop.conf.Configuration.<init>'
'edu.duke.starfish.jobopt.optimizer.JobOptimizer.getBestConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'edu.duke.starfish.jobopt.optimizer.JobOptimizer.copyOptimalConfSettings','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.clear org.apache.hadoop.conf.Configuration.set'
'edu.duke.starfish.jobopt.optimizer.JobOptimizer.processJobRecommendationRequest','org.apache.hadoop.conf.Configuration.writeXml'
'edu.duke.starfish.jobopt.optimizer.JobOptimizer.findBestJobConfiguration','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.duke.starfish.jobopt.optimizer.JobOptimizer.getOptimizerOutput','org.apache.hadoop.conf.Configuration.get'
'edu.duke.starfish.jobopt.optimizer.JobOptimizer.loadOptimizationSystemProperties','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'edu.duke.starfish.jobopt.JobOptimizerDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.<init>'
'edu.duke.starfish.whatif.oracle.JobProfileOracle.whatif','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.crunch.impl.mr.plan.JobPrototype.build','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setJobName'
'org.apache.crunch.impl.mr.plan.JobPrototype.serialize','org.apache.hadoop.fs.Path.<init>'
'org.huahinframework.manager.rest.service.JobService.detail','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getJobFile org.apache.hadoop.mapred.RunningJob.getTrackingURL'
'org.huahinframework.manager.rest.service.JobService.killJobId','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getAllJobs org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.killJob'
'org.huahinframework.manager.rest.service.JobService.killJobName','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getAllJobs org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getJobName org.apache.hadoop.mapred.RunningJob.killJob'
'org.huahinframework.manager.rest.service.JobService.getStatus','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getAllJobs org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.Group.iterator org.apache.hadoop.mapred.Counters.Group.getDisplayName org.apache.hadoop.mapred.Counters.Counter.getDisplayName org.apache.hadoop.mapred.Counters.Counter.getValue'
'org.apache.oozie.servlet.JobServlet.checkAuthorizationForApp','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.templeton.tool.JobStateTracker.getTrackingJobs','org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.templeton.tool.JobStateTracker.getTrackingJobs','org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.templeton.tool.JobState.getStorageInstance','org.apache.hadoop.conf.Configuration.get'
'hudson.plugins.hadoop.JobTrackerStartTask.run','org.apache.hadoop.mapred.JobTracker.offerService'
'hudson.plugins.hadoop.JobTrackerStartTask.call','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobTracker.startTracker'
'org.apache.oozie.util.JobUtils.normalizeAppPath','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.huahinframework.manager.util.JobUtils.getJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.huahinframework.manager.util.JobUtils.listJob','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getAllJobs org.apache.hadoop.mapred.JobStatus.getRunState'
'org.huahinframework.manager.util.JobUtils.getJob','org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.JobStatus.getJobPriority org.apache.hadoop.mapred.JobStatus.getUsername org.apache.hadoop.mapred.JobStatus.getStartTime org.apache.hadoop.mapred.RunningJob.getJobName org.apache.hadoop.mapred.JobStatus.getRunState org.apache.hadoop.mapred.JobStatus.getJobRunState org.apache.hadoop.mapred.JobStatus.mapProgress org.apache.hadoop.mapred.JobStatus.reduceProgress org.apache.hadoop.mapred.JobStatus.getSchedulingInfo'
'extramuros.java.jobs.stats.freqdistribution.Job.Job','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.stats.freqdistribution.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.stats.freqdistribution.Job.getOutputFile','org.apache.hadoop.fs.Path.suffix'
'extramuros.java.jobs.clustering.dirichlet.Job.Job','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.dirichlet.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix'
'extramuros.java.jobs.clustering.dirichlet.Job.getOutput','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.Job.Job','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.Job.getOutput','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.file.vectorize.Job.Job','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.file.vectorize.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.file.vectorize.Job.getOutput','org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.file.probabilisticsample.Job.Job','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.file.probabilisticsample.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.file.probabilisticsample.Job.getOutput','org.apache.hadoop.fs.Path.toUri'
'org.apache.mahout.clustering.syntheticcontrol.canopy.Job.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.syntheticcontrol.canopy.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.syntheticcontrol.kmeans.Job.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.syntheticcontrol.kmeans.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.syntheticcontrol.kmeans.Job.finalClusterPath','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.mahout.clustering.syntheticcontrol.dirichlet.Job.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.syntheticcontrol.dirichlet.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.canopy.Job.Job','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.canopy.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix'
'extramuros.java.jobs.clustering.canopy.Job.getOutput','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.kmeans.Job.Job','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.kmeans.Job.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix'
'extramuros.java.jobs.clustering.kmeans.Job.getOutput','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.validation.daviesbouldin.Job.Job','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.validation.daviesbouldin.Job.run','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.clustering.validation.daviesbouldin.Job.computeIndex','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.fs.Path.suffix org.apache.hadoop.io.Text.asSubclass org.apache.hadoop.io.DoubleWritable.asSubclass org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'test.modelgen.table.model.JobflowInstanceLock.setExecutionId','org.apache.hadoop.io.Text.modify'
'com.hadoopilluminated.examples.Join.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'com.hadoopilluminated.examples.Join.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.ClusterStatus.getMaxReduceTasks org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.join.CompositeInputFormat.compose org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'com.hadoopilluminated.examples.Join.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.JoinIPAddressAndCrawlStatsStep.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.JoinIPAddressAndCrawlStatsStep.runStep','org.apache.hadoop.mapred.JobClient.runJob'
'com.manning.hip.ch12.crunch.JoinLogsAndUsers.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.commoncrawl.util.JoinMapper.getParentDirFromPath','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName'
'org.commoncrawl.util.JoinMapper.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getParent'
'org.commoncrawl.util.JoinMapper.setPathToTagMapping','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.util.JoinMapper.readTagMappings','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified'
'org.commoncrawl.util.JoinMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.commoncrawl.util.JoinValue>.collect'
'.JoinRecordMapper.map','org.apache.hadoop.mapred.OutputCollector<UNRESOLVED.TextPair,org.apache.hadoop.io.Text>.collect'
'.JoinStationMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<UNRESOLVED.TextPair,org.apache.hadoop.io.Text>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.JoinSubDomainsAndCrawlStatsStep.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.JoinSubDomainsAndCrawlStatsStep.runStep','org.apache.hadoop.mapred.JobClient.runJob'
'com.asakusafw.dmdl.thundergate.emitter.JoinedModelGeneratorTest.simple','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.chombo.mr.Joiner.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.chombo.mr.Joiner.JoinerMapper.setup','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'org.chombo.mr.Joiner.JoinerMapper.map','org.apache.hadoop.io.Text.toString'
'org.chombo.mr.Joiner.JoinerReducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'org.chombo.mr.Joiner.main','org.apache.hadoop.util.ToolRunner.run'
'hipi.examples.jpegfromhib.JpegFromHibInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileBlockLocations org.apache.hadoop.fs.BlockLocation.getOffset org.apache.hadoop.fs.BlockLocation.getLength org.apache.hadoop.fs.BlockLocation.getHosts org.apache.hadoop.fs.BlockLocation.getHosts org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.fs.BlockLocation.getHosts org.apache.hadoop.mapreduce.lib.input.FileSplit.<init>'
'com.lightboxtechnologies.spectrum.JsonImport.FsEntryMapLoader.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.io.ImmutableBytesWritable.get'
'com.lightboxtechnologies.spectrum.JsonImport.run','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.lightboxtechnologies.spectrum.JsonImport.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'org.apache.pig.builtin.JsonLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'org.apache.pig.builtin.JsonLoader.getInputFormat','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init>'
'org.apache.pig.builtin.JsonLoader.getNext','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentValue org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.acme.io.JsonLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'com.acme.io.JsonLoader.getInputFormat','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init>'
'com.acme.io.JsonLoader.getNext','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentValue org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.acme.io.JsonLoader.getSchema','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.twitter.elephantbird.pig.load.JsonLoader.JsonLoader','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.getName'
'com.twitter.elephantbird.pig.load.JsonLoader.getNext','org.apache.hadoop.io.Text.toString'
'org.apache.giraph.io.JsonLongDoubleFloatDoubleVertexOutputFormat.JsonLongDoubleFloatDoubleVertexWriter.convertVertexToLine','org.apache.hadoop.io.Text.<init>'
'voldemort.store.readonly.mr.serialization.JsonMapper.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set'
'voldemort.store.readonly.mr.serialization.JsonMapper.map','org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.get'
'org.apache.pig.builtin.JsonMetadata.findMetaFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.toString'
'org.apache.pig.builtin.JsonMetadata.getSchema','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.builtin.JsonMetadata.getStatistics','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.builtin.JsonMetadata.storeStatistics','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.builtin.JsonMetadata.storeSchema','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.openx.data.jsonserde.objectinspector.JsonObjectInspectorFactory.getJsonObjectInspectorFromTypeInfo','org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapKeyTypeInfo org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapValueTypeInfo org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldNames org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldTypeInfos'
'org.openx.data.jsonserde.objectinspector.JsonObjectInspectorFactory.getPrimitiveJavaObjectInspector','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory.equals org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector'
'voldemort.store.readonly.mr.serialization.JsonOutputCollector.collect','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect'
'voldemort.store.readonly.mr.serialization.JsonReducer.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set'
'voldemort.store.readonly.mr.serialization.JsonReducer.reduce','org.apache.hadoop.io.BytesWritable.get'
'com.linkedin.json.JsonSequenceFileInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.createRecordReader'
'com.linkedin.json.JsonSequenceFileInputFormat.listStatus','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.util.StringUtils.split org.apache.hadoop.fs.Path.<init>'
'com.linkedin.json.JsonSequenceFileInputFormat.getAllSubFileStatus','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.linkedin.json.JsonSequenceFileInputFormat.JsonObjectRecordReader.initialize','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.initialize'
'com.linkedin.json.JsonSequenceFileInputFormat.JsonObjectRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.nextKeyValue'
'com.linkedin.json.JsonSequenceFileInputFormat.JsonObjectRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.getCurrentKey'
'com.linkedin.json.JsonSequenceFileInputFormat.JsonObjectRecordReader.getCurrentValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.getCurrentValue'
'com.linkedin.json.JsonSequenceFileInputFormat.JsonObjectRecordReader.getProgress','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.getProgress'
'com.linkedin.json.JsonSequenceFileInputFormat.JsonObjectRecordReader.close','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.close'
'azkaban.common.web.JsonSequenceFileViewer.canReadFile','org.apache.hadoop.io.SequenceFile.Reader.getMetadata org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Reader.getMetadata org.apache.hadoop.io.Text.<init>'
'azkaban.common.web.JsonSequenceFileViewer.displaySequenceFile','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getBytes'
'org.openx.data.jsonserde.JsonSerDe.initialize','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString org.apache.hadoop.hive.serde2.SerDeStats.<init> org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo'
'org.openx.data.jsonserde.JsonSerDe.deserialize','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.toString'
'org.openx.data.jsonserde.JsonSerDe.serialize','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes'
'org.openx.data.jsonserde.JsonSerDe.serializeStruct','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldName org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector'
'org.openx.data.jsonserde.JsonSerDe.serializeField','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector.get org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject'
'org.openx.data.jsonserde.JsonSerDe.serializeList','org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListLength org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElement org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElementObjectInspector'
'org.openx.data.jsonserde.JsonSerDe.serializeMap','org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMap org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapKeyObjectInspector org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.getMapValueObjectInspector'
'org.openx.data.jsonserde.JsonSerDe.onMalformedJson','org.apache.hadoop.hive.serde2.SerDeException.<init>'
'org.openx.data.jsonserde.JsonSerDe.getSerDeStats','org.apache.hadoop.hive.serde2.SerDeStats.setRawDataSize org.apache.hadoop.hive.serde2.SerDeStats.setRawDataSize'
'com.proofpoint.hive.serde.JsonSerde.initialize','org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo'
'com.proofpoint.hive.serde.JsonSerde.deserialize','org.apache.hadoop.io.Writable.getClass org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.proofpoint.hive.serde.JsonSerde.doDeserialize','org.apache.hadoop.io.BinaryComparable.getBytes org.apache.hadoop.io.BinaryComparable.getLength org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.proofpoint.hive.serde.JsonSerde.getNodeValue','org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.proofpoint.hive.serde.JsonSerde.getListNodeValue','org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo'
'com.proofpoint.hive.serde.JsonSerde.getMapNodeValue','org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapKeyTypeInfo org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapKeyTypeInfo org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapKeyTypeInfo org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapValueTypeInfo org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.proofpoint.hive.serde.JsonSerde.getPrimitiveNodeValue','org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.proofpoint.hive.serde.JsonSerde.getStructNodeValue','org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldTypeInfos'
'org.apache.pig.builtin.JsonStorage.getOutputFormat','org.apache.hadoop.mapreduce.lib.output.TextOutputFormat<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.<init>'
'org.apache.pig.builtin.JsonStorage.setStoreLocation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.apache.pig.builtin.JsonStorage.putNext','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordWriter.write'
'org.openx.data.jsonserde.objectinspector.JsonStringJavaObjectInspector.getPrimitiveWritableObject','org.apache.hadoop.io.Text.<init>'
'org.openx.data.jsonserde.objectinspector.JsonStringJavaObjectInspector.create','org.apache.hadoop.io.Text.toString'
'org.openx.data.jsonserde.objectinspector.JsonStringJavaObjectInspector.set','org.apache.hadoop.io.Text.toString'
'com.livingsocial.hive.utils.KISSInspector.sameAsTypeIn','org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.getListElementObjectInspector org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory'
'com.livingsocial.hive.utils.KISSInspector.getCategory','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory'
'com.livingsocial.hive.utils.KISSInspector.getAnInspector','org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector'
'com.livingsocial.hive.utils.KISSInspector.get','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject'
'com.livingsocial.hive.utils.KISSInspector.isPrimitive','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory'
'com.livingsocial.hive.utils.KISSInspector.isList','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory'
'mia.clustering.ch09.KMeansClustering.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'tv.floe.caduceus.mahout.clustering.kmeans.KMeansClusteringDemo.printClusterResults','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'tv.floe.caduceus.mahout.clustering.kmeans.KMeansClusteringDemo.runKMeans','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.kmeans.KMeansDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.kmeans.KMeansDriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.kmeans.KMeansDriver.buildClusters','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.kmeans.KMeansDriver.clusterData','org.apache.hadoop.fs.Path.<init>'
'hadoop.KMeansHadoop.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.GenericOptionsParser.getConfiguration org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.util.GenericOptionsParser.getConfiguration org.apache.hadoop.util.ToolRunner.run'
'hadoop.KMeansHadoop.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getTrackingURL'
'skywriting.examples.skyhout.kmeans.KMeansHead.invoke','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClassLoader org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'de.jungblut.clustering.mapreduce.KMeansMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.kmeans.KMeansMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'skywriting.examples.skyhout.kmeans.KMeansReduceTask.invoke','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClassLoader org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.io.serializer.WritableSerialization.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.Text,org.apache.mahout.clustering.kmeans.KMeansInfo>.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'de.jungblut.clustering.mapreduce.KMeansReducer.cleanup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init>'
'skywriting.examples.skyhout.kmeans.KMeansReducerCombiner.combineFinal','org.apache.hadoop.io.Text.toString'
'skywriting.examples.skyhout.kmeans.KMeansSeedGenerator.invoke','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClassLoader org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.io.serializer.WritableSerialization.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.kmeans.KMeansUtil.configureWithClusterInfo','org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.Writable.getClass'
'org.apache.mahout.clustering.kmeans.KMeansUtil.configureWithClusterInfo','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.io.Writable.getClass'
'mia.clustering.ch09.KMeansWithCanopyClustering.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.oozie.action.hadoop.KerberosAuthHelper.set','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.JobClient.getDelegationToken org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.io.Text.<init>'
'org.apache.oozie.action.hadoop.KerberosDoAs.call','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.oozie.service.KerberosHadoopAccessorService.init','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.security.UserGroupInformation.setConfiguration org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.security.UserGroupInformation.setConfiguration org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.KerberosHadoopAccessorService.getUGI','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser'
'org.apache.oozie.service.KerberosHadoopAccessorService.createJobClient','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.security.UserGroupInformation.doAs org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.JobClient.getDelegationToken org.apache.hadoop.mapred.JobConf.getCredentials org.apache.hadoop.io.Text.<init>'
'org.apache.oozie.service.KerberosHadoopAccessorService.run','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.filecache.DistributedCache.addFileToClassPath'
'org.apache.oozie.service.KerberosHadoopAccessorService.createFileSystem','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.security.UserGroupInformation.doAs org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.oozie.service.KerberosHadoopAccessorService.addFileToClassPath','org.apache.hadoop.security.UserGroupInformation.doAs'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToBooleanWritableConverter.canConvert','org.apache.hadoop.io.BooleanWritable.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToBooleanWritableConverter.convert','org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.set org.apache.hadoop.io.BooleanWritable.getSimpleName'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToBooleanWritableConverterTest.convert','org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.<init>'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToBytesWritableConverter.canConvert','org.apache.hadoop.io.BytesWritable.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToBytesWritableConverter.convert','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.getSimpleName'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToBytesWritableConverterTest.convert','org.apache.hadoop.io.BytesWritable.<init>'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToIntWritableConverterTest.convert','org.apache.hadoop.io.IntWritable.<init>'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToLongWritableConverter.canConvert','org.apache.hadoop.io.LongWritable.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToLongWritableConverter.convert','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.getSimpleName'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToTextConverter.canConvert','org.apache.hadoop.io.Text.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.KettleTypeToTextConverter.convert','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.getSimpleName'
'org.apache.accumulo.core.data.Key.Key','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.Key.getRow','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.Key.compareRow','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.accumulo.core.data.Key.getColumnFamily','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.Key.compareColumnFamily','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.accumulo.core.data.Key.getColumnQualifier','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.Key.compareColumnQualifier','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.accumulo.core.data.Key.getColumnVisibility','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'org.apache.accumulo.core.data.Key.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVLong'
'org.apache.accumulo.core.data.Key.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVLong'
'org.apache.accumulo.core.data.Key.compareTo','org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.accumulo.core.data.Key.hashCode','org.apache.hadoop.io.WritableComparator.hashBytes org.apache.hadoop.io.WritableComparator.hashBytes org.apache.hadoop.io.WritableComparator.hashBytes org.apache.hadoop.io.WritableComparator.hashBytes'
'com.rapleaf.hank.hadoop.KeyAndPartitionWritable.KeyAndPartitionWritable','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.IntWritable.<init>'
'com.rapleaf.hank.hadoop.KeyAndPartitionWritable.getKey','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'com.rapleaf.hank.hadoop.KeyAndPartitionWritable.getPartition','org.apache.hadoop.io.IntWritable.get'
'com.rapleaf.hank.hadoop.KeyAndPartitionWritable.readFields','org.apache.hadoop.io.BytesWritable.readFields org.apache.hadoop.io.IntWritable.readFields'
'com.rapleaf.hank.hadoop.KeyAndPartitionWritable.write','org.apache.hadoop.io.BytesWritable.write org.apache.hadoop.io.IntWritable.write'
'com.rapleaf.hank.hadoop.KeyAndPartitionWritable.toString','org.apache.hadoop.io.BytesWritable.toString org.apache.hadoop.io.IntWritable.toString'
'org.apache.mahout.fpm.pfpgrowth.dataset.KeyBasedStringTupleGrouper.startJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.fpm.pfpgrowth.dataset.KeyBasedStringTupleReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.KeyExtent.dedupeTableId','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.KeyExtent.check','org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.core.data.KeyExtent.KeyExtent','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.KeyExtent.getMetadataEntry','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.data.KeyExtent.setEndRow','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.KeyExtent.setPrevEndRow','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.KeyExtent.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields'
'org.apache.accumulo.core.data.KeyExtent.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write'
'org.apache.accumulo.core.data.KeyExtent.getKeyExtentsForRange','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength'
'org.apache.accumulo.core.data.KeyExtent.decodePrevEndRow','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'org.apache.accumulo.core.data.KeyExtent.encodePrevEndRow','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.accumulo.core.data.KeyExtent.compareTo','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.core.data.KeyExtent.hashCode','org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.Text.hashCode'
'org.apache.accumulo.core.data.KeyExtent.equals','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.core.data.KeyExtent.toString','org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.data.KeyExtent.decodeMetadataRow','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.set'
'org.apache.accumulo.core.data.KeyExtent.contains','org.apache.hadoop.io.BinaryComparable.<init> org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.core.data.KeyExtent.toMetadataRange','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.data.KeyExtent.rowAfterPrevRow','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.data.KeyExtent.findOverlapping','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.KeyExtent.isMeta','org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.data.KeyExtentTest.nke','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.KeyExtentTest.testDecodingMetadataRow','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'.KeyFieldBasedComparatorTest.check','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setKeyFieldComparatorOptions org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.<init> org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.configure org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.compare'
'.KeyFieldBasedComparatorTest.serialize','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Writable.write'
'org.apache.accumulo.examples.simple.filedata.KeyUtil.buildNullSepText','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'org.apache.accumulo.examples.simple.filedata.KeyUtil.splitNullSepText','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength'
'org.apache.accumulo.examples.simple.filedata.KeyUtilTest.checkSeps','org.apache.hadoop.io.Text.getLength'
'com.hphoto.util.KeyUtil.getKey','org.apache.hadoop.io.Text.<init>'
'com.hphoto.util.KeyUtil.getAuthKey','org.apache.hadoop.io.Text.<init>'
'com.lightboxtechnologies.spectrum.KeyUtils.makeEntryKey','org.apache.hadoop.hbase.util.Bytes.putBytes org.apache.hadoop.hbase.util.Bytes.putBytes'
'com.lightboxtechnologies.spectrum.KeyUtils.getHash','org.apache.hadoop.hbase.util.Bytes.putBytes'
'com.lightboxtechnologies.spectrum.KeyUtils.getFsEntryID','org.apache.hadoop.hbase.util.Bytes.putBytes'
'elephantdb.cascading.KeyValTailAssembly.MakeSortableKey.operate','org.apache.hadoop.io.BytesWritable.<init>'
'com.rapleaf.hank.hadoop.KeyValuePair.KeyValuePair','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init>'
'com.rapleaf.hank.hadoop.KeyValuePair.toString','org.apache.hadoop.io.BytesWritable.toString org.apache.hadoop.io.BytesWritable.toString'
'shark.KeyWrapperFactory.getKeyWrapper','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfoFromObjectInspector org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfoFromObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.<init> org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.<init>'
'shark.KeyWrapperFactory.ListKeyWrapper.equals','org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.areEqual'
'shark.KeyWrapperFactory.ListKeyWrapper.getNewKey','org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate'
'shark.KeyWrapperFactory.ListKeyWrapper.deepCopyElements','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject'
'shark.KeyWrapperFactory.TextKeyWrapper.equals','org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveWritableObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveWritableObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveWritableObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveWritableObject org.apache.hadoop.io.Text.equals'
'shark.KeyWrapperFactory.TextKeyWrapper.getNewKey','org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate'
'shark.KeyWrapperFactory.TextKeyWrapper.copyKey','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject'
'org.apache.hama.bsp.KillJobAction.write','org.apache.hadoop.io.Text.writeString'
'org.apache.hama.bsp.KillJobAction.readFields','org.apache.hadoop.io.Text.readString'
'com.cloudera.kitten.client.KittenClient.handle','org.apache.hadoop.yarn.api.records.ApplicationReport.getYarnApplicationState org.apache.hadoop.yarn.api.records.ApplicationReport.getTrackingUrl org.apache.hadoop.yarn.api.records.ApplicationReport.getFinalApplicationStatus'
'com.cloudera.kitten.client.KittenClient.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'mapreduce.KmeansDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue'
'mapreduce.KmeansMapper.map','org.apache.hadoop.io.LongWritable.set'
'mapreduce.KmeansMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.LongWritable.<init>'
'cascading.kryo.KryoFactory.getSkipMissing','org.apache.hadoop.conf.Configuration.getBoolean'
'cascading.kryo.KryoFactory.setSkipMissing','org.apache.hadoop.conf.Configuration.setBoolean'
'cascading.kryo.KryoFactory.getAcceptAll','org.apache.hadoop.conf.Configuration.getBoolean'
'cascading.kryo.KryoFactory.setAcceptAll','org.apache.hadoop.conf.Configuration.setBoolean'
'cascading.kryo.KryoFactory.getRegistrations','org.apache.hadoop.conf.Configuration.get'
'cascading.kryo.KryoFactory.setRegistrations','org.apache.hadoop.conf.Configuration.set'
'cascading.kryo.KryoFactory.getHierarchyRegistrations','org.apache.hadoop.conf.Configuration.get'
'cascading.kryo.KryoFactory.setHierarchyRegistrations','org.apache.hadoop.conf.Configuration.set'
'cascading.kryo.KryoSerialization.KryoSerialization','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.lda.LDADriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.lda.LDADriver.createState','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.clustering.lda.LDADriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.lda.LDADriver.writeInitialState','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.DoubleWritable.set'
'org.apache.mahout.clustering.lda.LDADriver.findLL','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.clustering.lda.LDADriver.runIteration','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.lda.LDAReducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.vectorizer.collocations.llr.LLRReducer.reduce','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.vectorizer.collocations.llr.LLRReducer.setup','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest.testReduce','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest.testReduce','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.vectorizer.collocations.llr.LLRReducer.reduce','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.vectorizer.collocations.llr.LLRReducer.setup','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData','org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.pig.newplan.logical.relational.LOLoad.storeScriptSchema','org.apache.hadoop.conf.Configuration.set'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.LOOKUPHandler.doHandle','org.apache.hadoop.fs.Path.<init>'
'upenn.junto.algorithm.parallel.LP_ZGL_Hadoop.LP_ZGL_Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'upenn.junto.algorithm.parallel.LP_ZGL_Hadoop.LP_ZGL_Reduce.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'upenn.junto.algorithm.parallel.LP_ZGL_Hadoop.LP_ZGL_Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'upenn.junto.algorithm.parallel.LP_ZGL_Hadoop.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'lsh.hadoop.LSHDriver.removeOutputDir','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.get'
'lsh.hadoop.LSHDriver.runJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.twitter.elephanttwin.lzo.retrieval.LZOBlockOffsetMapper.setup','org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toString'
'com.twitter.elephanttwin.lzo.retrieval.LZOBlockOffsetMapper.map','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.<init>'
'com.twitter.elephanttwin.lzo.retrieval.LZOBlockOffsetMapper.cleanup','org.apache.hadoop.io.Text.<init>'
'gov.llnl.ontology.mapreduce.ingest.LabelNounPairsMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.ingest.LabelNounPairsMR.setupConfiguration','org.apache.hadoop.conf.Configuration.set'
'gov.llnl.ontology.mapreduce.ingest.LabelNounPairsMR.LabelNounPairsMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'fr.insarennes.fafdti.builder.LabeledExample.LabeledExample','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'fr.insarennes.fafdti.builder.LabeledExample.readFields','org.apache.hadoop.io.Text.readFields'
'fr.insarennes.fafdti.builder.LabeledExample.write','org.apache.hadoop.io.Text.write'
'fr.insarennes.fafdti.builder.LabeledExample.getLabel','org.apache.hadoop.io.Text.toString'
'fr.insarennes.fafdti.builder.LabeledExample.fromString','org.apache.hadoop.io.Text.set'
'fr.insarennes.fafdti.builder.LabeledExample.toString','org.apache.hadoop.io.Text.toString'
'babel.prep.langid.LangIdMapper.configure','org.apache.hadoop.mapred.JobConf.get'
'babel.prep.langid.LangIdMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,babel.content.pages.Page>.collect'
'babel.prep.langid.LangIdentifier.createJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.set'
'babel.prep.langid.LangIdentifier.main','org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.FileOutputFormat.getOutputPath'
'com.digitalpebble.behemoth.languageidentification.LanguageIDProcessorTest.testLanguage','org.apache.hadoop.io.Writable.toString'
'com.digitalpebble.behemoth.languageidentification.LanguageIdDriver.main','org.apache.hadoop.util.ToolRunner.run'
'com.digitalpebble.behemoth.languageidentification.LanguageIdDriver.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.analysis.lang.LanguageIdentifier.LanguageIdentifier','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.analysis.lang.LanguageIdentifier.getUrlContent','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.analysis.lang.LanguageQueryFilter.setConf','org.apache.hadoop.conf.Configuration.getFloat'
'org.apache.sqoop.lib.LargeObjectLoader.LargeObjectLoader','org.apache.hadoop.fs.FileSystem.get'
'org.apache.sqoop.lib.LargeObjectLoader.getNextLobFilePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.sqoop.lib.LargeObjectLoader.getRelativePath','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.sqoop.lib.LargeObjectLoader.copyAll','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.sqoop.lib.LargeObjectLoader.readBlobRef','org.apache.hadoop.conf.Configuration.getLong'
'org.apache.sqoop.lib.LargeObjectLoader.readClobRef','org.apache.hadoop.conf.Configuration.getLong'
'org.apache.accumulo.server.test.functional.LargeRowTest.getTablesToCreate','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.LargeRowTest.basicTest','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.LargeRowTest.verify','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.minhash.LastfmDataConverter.writeToSequenceFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'org.apache.mahout.clustering.minhash.LastfmDataConverter.main','org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.latedata.LateDataHandler.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.ivory.latedata.LateDataHandler.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.ivory.latedata.LateDataHandler.detectChanges','org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.ivory.latedata.LateDataHandler.usage','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.getContentSummary'
'org.apache.hcatalog.templeton.LauncherDelegator.run','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hcatalog.templeton.LauncherDelegator.queueAsUser','org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.hcatalog.templeton.LauncherDelegator.makeOverrideClasspath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.util.StringUtils.join'
'org.apache.oozie.action.hadoop.LauncherMapper.setRecoveryId','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.action.hadoop.LauncherMapper.getRecoveryId','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.action.hadoop.LauncherMapper.setupMainClass','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.LauncherMapper.setupMainArguments','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.LauncherMapper.setupMaxOutputData','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.oozie.action.hadoop.LauncherMapper.setupLauncherInfo','org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.action.hadoop.LauncherMapper.isMainDone','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.LauncherMapper.isMainSuccessful','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup'
'org.apache.oozie.action.hadoop.LauncherMapper.hasOutputData','org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup'
'org.apache.oozie.action.hadoop.LauncherMapper.hasIdSwap','org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.LauncherMapper.getOutputDataPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.LauncherMapper.getErrorPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.LauncherMapper.getIdSwapPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.LauncherMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.LauncherMapper.map','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.oozie.action.hadoop.LauncherMapper.setupMainConfiguration','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.LauncherMapper.getMainArguments','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.LauncherMapper.run','org.apache.hadoop.mapred.Reporter.progress'
'org.apache.oozie.action.hadoop.LauncherMapper.failLauncher','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.hadoop.LauncherMapper.setRecoveryId','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.action.hadoop.LauncherMapper.getRecoveryId','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.action.hadoop.LauncherMapper.setupMainClass','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.LauncherMapper.setupMainArguments','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.LauncherMapper.setupMaxOutputData','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.oozie.action.hadoop.LauncherMapper.setupLauncherInfo','org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.action.hadoop.LauncherMapper.isMainDone','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.LauncherMapper.isMainSuccessful','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup'
'org.apache.oozie.action.hadoop.LauncherMapper.hasOutputData','org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup'
'org.apache.oozie.action.hadoop.LauncherMapper.hasIdSwap','org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.LauncherMapper.getOutputDataPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.LauncherMapper.getErrorPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.LauncherMapper.getIdSwapPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.LauncherMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.LauncherMapper.map','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.oozie.action.hadoop.LauncherMapper.setupMainConfiguration','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.LauncherMapper.getMainArguments','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.LauncherMapper.run','org.apache.hadoop.mapred.Reporter.progress'
'org.apache.oozie.action.hadoop.LauncherMapper.failLauncher','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'fr.insarennes.fafdti.bagging.Launcher.Launcher','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'fr.insarennes.fafdti.bagging.Launcher.splitData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.hcatalog.data.LazyHCatRecord.get','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector'
'org.apache.hcatalog.data.LazyHCatRecord.size','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs'
'org.apache.hcatalog.data.LazyHCatRecord.LazyHCatRecord','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.SerDeException.<init>'
'ivory.core.data.document.LazyIntDocVector.writeRawBytes','org.apache.hadoop.io.WritableUtils.writeVInt'
'ivory.core.data.document.LazyIntDocVector.writeTermPositionsMap','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt'
'ivory.core.data.document.LazyIntDocVector.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'com.cloudera.recordbreaker.learnstructure.LearnStructure.inferRecordFormat','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open'
'com.cloudera.recordbreaker.learnstructure.LearnStructure.main','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'ivory.ptc.sampling.LengthCountCriterion.initialize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'cascading.tap.hadoop.Lfs.getFileSystem','org.apache.hadoop.fs.FileSystem.getLocal'
'edu.jhu.thrax.hadoop.features.mapred.LhsGivenSourcePhraseFeature.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.NullWritable.get'
'org.apache.expreval.expr.stringpattern.LikeStmt.getValue','org.apache.hadoop.hbase.hbql.client.HBqlException.<init> org.apache.hadoop.hbase.hbql.client.HBqlException.<init>'
'edu.isi.mavuno.score.LikelihoodScorer.setup','org.apache.hadoop.conf.Configuration.get'
'org.lilyproject.util.hbase.LilyHBaseSchema.Table.Table','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.util.hbase.LilyHBaseSchema.RecordCf.RecordCf','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.util.hbase.LilyHBaseSchema.RecordColumn.RecordColumn','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.util.hbase.LilyHBaseSchema.TypeCf.TypeCf','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.util.hbase.LilyHBaseSchema.TypeColumn.TypeColumn','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.archive.wayback.hadoop.LineDereferencingInputFormat.getSplits','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.getSplits'
'org.archive.hadoop.mapreduce.LineDereferencingInputFormat.getSplits','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.getSplits'
'org.archive.wayback.hadoop.LineDereferencingRecordReader.forceCompressed','org.apache.hadoop.conf.Configuration.setBoolean'
'org.archive.wayback.hadoop.LineDereferencingRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize'
'org.archive.wayback.hadoop.LineDereferencingRecordReader.nextKeyValue','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.archive.wayback.hadoop.LineDereferencingRecordReader.close','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'org.archive.hadoop.mapreduce.LineDereferencingRecordReader.forceCompressed','org.apache.hadoop.conf.Configuration.setBoolean'
'org.archive.hadoop.mapreduce.LineDereferencingRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize'
'org.archive.hadoop.mapreduce.LineDereferencingRecordReader.nextKeyValue','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.archive.hadoop.mapreduce.LineDereferencingRecordReader.close','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'org.apache.hama.bsp.LineRecordReader.LineRecordReader','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.Text.<init> org.apache.hadoop.conf.Configuration.getInt'
'org.apache.hama.bsp.LineRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.hama.bsp.LineRecordReader.createValue','org.apache.hadoop.io.Text.<init>'
'org.apache.hama.bsp.LineRecordReader.next','org.apache.hadoop.io.LongWritable.set'
'com.datasalt.pangool.flow.LinearFlow.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.scoring.link.LinkAnalysisScoringFilter.setConf','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkCollectorJob.mergeSegmentEC2','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.JobClient.runJob'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkCollectorJob.mergeSegmentInternal','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkCollectorJob.scanForValidSegments','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkCollectorJob.scanForMergedSegments','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkCollectorJob.LinkCollectorJob','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getUri'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkCollectorJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.dateHeadersFromJsonObject','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.map','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.emitLinksFromFeedContent','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.safeEmitLinkTypeInLink','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.emitLinksFromAtomContent','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.safeEmitRSSLinkTypeInLink','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.emitLinksFromRSSContent','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.emitLinksFromHTMLContent','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkDataResharder.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.nutch.crawl.LinkDb.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.crawl.LinkDb.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.Inlinks>.collect'
'org.apache.nutch.crawl.LinkDb.invert','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.LinkDb.createJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.apache.nutch.crawl.LinkDb.install','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getFs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.crawl.LinkDb.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.crawl.LinkDb.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.crawl.LinkDbFilter.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.get'
'org.apache.nutch.crawl.LinkDbFilter.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks>.collect'
'org.apache.nutch.searcher.LinkDbInlinks.getAnchors','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.searcher.LinkDbInlinks.getInlinks','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.crawl.LinkDbMerger.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks>.collect'
'org.apache.nutch.crawl.LinkDbMerger.configure','org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.nutch.crawl.LinkDbMerger.merge','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.apache.nutch.crawl.LinkDbMerger.createMergeJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setBoolean'
'org.apache.nutch.crawl.LinkDbMerger.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.crawl.LinkDbMerger.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.crawl.LinkDbReader.init','org.apache.hadoop.fs.FileSystem.get'
'org.apache.nutch.crawl.LinkDbReader.getInlinks','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getReaders org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getEntry'
'org.apache.nutch.crawl.LinkDbReader.processDumpJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.crawl.LinkDbReader.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.crawl.LinkDbReader.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.crawl.LinkDb.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.crawl.LinkDb.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.Inlinks>.collect'
'org.apache.nutch.crawl.LinkDb.invert','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.LinkDb.createJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.apache.nutch.crawl.LinkDb.install','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getFs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.crawl.LinkDb.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.crawl.LinkDb.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.scoring.webgraph.LinkDumper.Reader.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getReaders org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.lib.HashPartitioner<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDumper.LinkNodes>.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getEntry'
'org.apache.nutch.scoring.webgraph.LinkDumper.Inverter.map','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.ObjectWritable>.collect'
'org.apache.nutch.scoring.webgraph.LinkDumper.Inverter.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.ObjectWritable.get org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDumper.LinkNode>.collect'
'org.apache.nutch.scoring.webgraph.LinkDumper.Merger.reduce','org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDumper.LinkNodes>.collect'
'org.apache.nutch.scoring.webgraph.LinkDumper.dumpLinks','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.scoring.webgraph.LinkDumper.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.scoring.webgraph.LinkDumper.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.lilyproject.linkindex.test.LinkIndexTest.setUpBeforeClass','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkKey.main','org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.digest'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.findLatestMergeDBTimestamp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.filterMergeCandidtes','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.emitRedirectRecord','org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.updateLinkStatsFromLinkJSONObject','org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.updateCrawlStatsFromJSONObject','org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.updateLinkStatsFromHTMLContent','org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.reduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.mapred.Reporter.progress org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.postprocess.linkCollector.LinkMergerJob.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.FileOutputFormat.getWorkOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.hcatalog.templeton.ListDelegator.run','org.apache.hadoop.security.UserGroupInformation.createRemoteUser org.apache.hadoop.mapred.JobTracker.getAddress org.apache.hadoop.mapred.TempletonJobTracker.<init> org.apache.hadoop.mapred.TempletonJobTracker.getAllJobs org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.TempletonJobTracker.close'
'org.childtv.hadoop.hbase.mapred.ListTableInputFormat.configure','org.apache.hadoop.mapred.JobConf.get'
'org.childtv.hadoop.hbase.mapred.ListTableInputFormat.formatRowResult','org.apache.hadoop.hbase.io.RowResult.values org.apache.hadoop.hbase.io.Cell.getValue'
'admin.ListTablesExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor'
'org.apache.accumulo.server.monitor.servlets.trace.ListType.pageBody','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.service.listcrawler.ListUploadServlet.doPost','org.apache.hadoop.util.Shell.execCommand'
'org.apache.oozie.service.LiteWorkflowAppService.parseDef','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.LiteWorkflowAppService.parseDef','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.workflow.lite.LiteWorkflowInstance.newInstance','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.oozie.workflow.lite.LiteWorkflowInstance.refreshLog','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.workflow.lite.LiteWorkflowInstance.write','org.apache.hadoop.conf.Configuration.writeXml'
'org.apache.oozie.workflow.lite.LiteWorkflowInstance.newInstance','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.oozie.workflow.lite.LiteWorkflowInstance.refreshLog','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.workflow.lite.LiteWorkflowInstance.write','org.apache.hadoop.conf.Configuration.writeXml'
'org.apache.oozie.service.LiteWorkflowStoreService.getUserRetryInterval','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.LiteWorkflowStoreService.getUserRetryMax','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.LiteWorkflowStoreService.getUserRetryErrorCode','org.apache.hadoop.conf.Configuration.getStringCollection org.apache.hadoop.conf.Configuration.getStringCollection'
'org.apache.oozie.service.LiteWorkflowStoreService.getNodeDefDefaultVersion','org.apache.hadoop.conf.Configuration.get'
'.LoadColumnMap.ColumnFamilyMapper.map','org.apache.hadoop.io.Text.toString'
'.LoadColumnMap.ColumnFamilyMapper.setup','org.apache.hadoop.conf.Configuration.get'
'.LoadColumnMap.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.waitForCompletion'
'.LoadColumnMap.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'nl.vu.datalayer.hbase.coprocessor.LoadCoprocessor.runCoprocessors','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.coprocessor.Batch.forMethod org.apache.hadoop.hbase.client.HTableInterface.coprocessorExec'
'nl.vu.datalayer.hbase.coprocessor.LoadCoprocessor.main','org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.HTableDescriptor.addCoprocessor org.apache.hadoop.hbase.client.HBaseAdmin.modifyTable org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.client.HBaseAdmin.close'
'org.apache.hcatalog.pig.drivers.LoadFuncBasedInputFormat.LoadFuncBasedInputFormat','org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.hcatalog.pig.drivers.LoadFuncBasedInputFormat.getSplits','org.apache.hadoop.mapreduce.InputFormat<org.apache.hadoop.io.BytesWritable,org.apache.pig.data.Tuple>.getSplits'
'org.apache.hcatalog.pig.drivers.LoadFuncBasedInputFormat.LoadFuncBasedRecordReader.close','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable,org.apache.pig.data.Tuple>.close'
'org.apache.hcatalog.pig.drivers.LoadFuncBasedInputFormat.LoadFuncBasedRecordReader.initialize','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable,org.apache.pig.data.Tuple>.initialize'
'org.apache.pig.piggybank.storage.allloader.LoadFuncHelper.LoadFuncHelper','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.piggybank.storage.allloader.LoadFuncHelper.determineFirstFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir'
'org.apache.pig.piggybank.storage.allloader.LoadFuncHelper.determineFunction','org.apache.hadoop.fs.Path.getName'
'org.apache.pig.piggybank.storage.allloader.LoadFuncHelper.getFuncSpecFromContent','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.pig.piggybank.storage.allloader.LoadFuncHelper.getApplicableTag','org.apache.hadoop.fs.Path.toUri'
'org.apache.pig.piggybank.storage.allloader.LoadFuncHelper.getFirstFile','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.isDir'
'.LoadTable.ColumnFamilyMapper.map','org.apache.hadoop.io.Text.toString'
'.LoadTable.ColumnFamilyMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'.LoadTable.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.waitForCompletion'
'.LoadTable.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'fr.eurecom.dsg.hbase.LoadUser.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'ch.sentric.hbase.coprocessor.LoadWithTableDescriptorExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.fs.Path.toString org.apache.hadoop.hbase.HTableDescriptor.setValue org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor'
'com.cloudera.sqoop.testutil.LobAvroImportTestCase.read','org.apache.hadoop.conf.Configuration.set'
'com.cloudera.sqoop.testutil.LobAvroImportTestCase.testBlobAvroImportInline','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.testutil.LobAvroImportTestCase.testBlobAvroImportExternal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.testutil.LobAvroImportTestCase.testBlobCompressedAvroImportInline','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.testutil.LobAvroImportTestCase.testBlobCompressedAvroImportExternal','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.testutil.LobAvroImportTestCase.testBlobAvroImportMultiCols','org.apache.hadoop.fs.Path.<init>'
'org.apache.sqoop.io.LobReaderCache.qualify','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.makeQualified'
'org.apache.sqoop.lib.LobRef.getDataStream','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.equals'
'org.apache.sqoop.lib.LobRef.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.apache.sqoop.lib.LobRef.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'org.commoncrawl.hadoop.io.LocalARCSource.getInputs','org.apache.hadoop.mapred.JobConf.get'
'org.commoncrawl.hadoop.io.LocalARCSource.setInputs','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.hadoop.io.LocalARCSource.setInputs','org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.hadoop.io.LocalARCSource.getInputs','org.apache.hadoop.mapred.JobConf.get'
'org.apache.giraph.examples.LocalClusteringCoefficientVertex.compute','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.toString org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.LocalClusteringCoefficientVertex.LocalClusteringCoefficientVertexReader.getCurrentVertex','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.examples.LocalClusteringCoefficientVertex.LocalClusteringCoefficientVertexWriter.writeVertex','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.examples.LocalClusteringCoefficientVertex.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.apache.giraph.examples.LocalClusteringCoefficientVertex.main','org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.kitten.util.LocalDataHelper.copyConfiguration','org.apache.hadoop.conf.Configuration.writeXml'
'com.cloudera.kitten.util.LocalDataHelper.copyToHdfs','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.toUri'
'com.cloudera.kitten.util.LocalDataHelper.getPath','org.apache.hadoop.yarn.api.records.ApplicationId.getId org.apache.hadoop.fs.Path.<init>'
'com.cloudera.kitten.util.LocalDataHelper.getAppPath','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init>'
'elephantdb.hadoop.LocalElephantManager.setTmpDirs','org.apache.hadoop.conf.Configuration.setStrings'
'elephantdb.hadoop.LocalElephantManager.getTmpDirs','org.apache.hadoop.conf.Configuration.getStrings'
'elephantdb.hadoop.LocalElephantManager.downloadRemoteShard','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile'
'elephantdb.hadoop.LocalElephantManager.cleanup','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.example.LocalExample.main','org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.example.LocalExample.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMaxMapTaskFailuresPercent org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'org.commoncrawl.example.LocalExample.MimeCounterMapper.map','org.apache.hadoop.mapred.Reporter.getCounter'
'com.ning.metrics.action.hdfs.writer.LocalFileSystemAccessProvider.LocalFileSystemAccessProvider','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.lilyproject.util.hbase.LocalHTable.LocalHTable','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.util.hbase.LocalHTable.initialValue','org.apache.hadoop.hbase.client.HTable.<init>'
'org.lilyproject.util.hbase.LocalHTable.getTableName','org.apache.hadoop.hbase.client.Result[].getTableName'
'org.lilyproject.util.hbase.LocalHTable.getConfiguration','org.apache.hadoop.hbase.client.Result[].getConfiguration'
'org.lilyproject.util.hbase.LocalHTable.getTableDescriptor','org.apache.hadoop.hbase.client.Result[].getTableDescriptor'
'org.lilyproject.util.hbase.LocalHTable.exists','org.apache.hadoop.hbase.client.Result[].exists'
'org.lilyproject.util.hbase.LocalHTable.get','org.apache.hadoop.hbase.client.Result[].get org.apache.hadoop.hbase.client.Result[].get'
'org.lilyproject.util.hbase.LocalHTable.getRowOrBefore','org.apache.hadoop.hbase.client.Result[].getRowOrBefore'
'org.lilyproject.util.hbase.LocalHTable.getScanner','org.apache.hadoop.hbase.client.Result[].getScanner org.apache.hadoop.hbase.client.Result[].getScanner org.apache.hadoop.hbase.client.Result[].getScanner'
'org.lilyproject.util.hbase.LocalHTable.put','org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.getWriteBuffer org.apache.hadoop.hbase.client.Result[].put'
'org.lilyproject.util.hbase.LocalHTable.checkAndPut','org.apache.hadoop.hbase.client.Result[].checkAndPut'
'org.lilyproject.util.hbase.LocalHTable.delete','org.apache.hadoop.hbase.client.Result[].delete org.apache.hadoop.hbase.client.Result[].delete'
'org.lilyproject.util.hbase.LocalHTable.checkAndDelete','org.apache.hadoop.hbase.client.Result[].checkAndDelete'
'org.lilyproject.util.hbase.LocalHTable.incrementColumnValue','org.apache.hadoop.hbase.client.Result[].incrementColumnValue org.apache.hadoop.hbase.client.Result[].incrementColumnValue'
'org.lilyproject.util.hbase.LocalHTable.isAutoFlush','org.apache.hadoop.hbase.client.Result[].isAutoFlush'
'org.lilyproject.util.hbase.LocalHTable.flushCommits','org.apache.hadoop.hbase.client.Result[].flushCommits'
'org.lilyproject.util.hbase.LocalHTable.close','org.apache.hadoop.hbase.client.Result[].close'
'org.lilyproject.util.hbase.LocalHTable.lockRow','org.apache.hadoop.hbase.client.Result[].lockRow'
'org.lilyproject.util.hbase.LocalHTable.unlockRow','org.apache.hadoop.hbase.client.Result[].unlockRow'
'org.lilyproject.util.hbase.LocalHTable.batch','org.apache.hadoop.hbase.client.Result[].batch org.apache.hadoop.hbase.client.Result[].batch'
'org.lilyproject.util.hbase.LocalHTable.increment','org.apache.hadoop.hbase.client.Result[].increment'
'com.nearinfinity.blur.manager.indexserver.LocalIndexServer.openIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.pen.LocalMapReduceSimulator.launchPig','org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.Job.getJobConf org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.Text,org.apache.pig.data.Tuple,org.apache.pig.impl.io.PigNullableWritable,org.apache.hadoop.io.Writable>.run org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.Text,org.apache.pig.data.Tuple,org.apache.pig.impl.io.PigNullableWritable,org.apache.hadoop.io.Writable>.run org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCADenseTest.runSSVDSolver','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.io.compress.DefaultCodec.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest.testSSVDSolver','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.io.compress.DefaultCodec.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest.runSSVDSolver','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.io.compress.DefaultCodec.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.local.LocalStreamService.LocalStreamService','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.local.LocalStreamService.cleanUpTmp','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'com.inmobi.databus.local.LocalStreamService.execute','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'com.inmobi.databus.local.LocalStreamService.prepareForCommit','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.local.LocalStreamService.populateTrashCommitPaths','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.local.LocalStreamService.commit','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename'
'com.inmobi.databus.local.LocalStreamService.createMRInput','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'com.inmobi.databus.local.LocalStreamService.CollectorPathFilter.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'com.inmobi.databus.local.LocalStreamService.createListing','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.local.LocalStreamService.processFile','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete'
'com.inmobi.databus.local.LocalStreamService.isEmptyFile','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.local.LocalStreamService.FileTimeStampComparator.compare','org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getModificationTime'
'com.inmobi.databus.local.LocalStreamService.getCurrentFile','org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.local.LocalStreamService.getCategoryFromSrcPath','org.apache.hadoop.fs.Path.getParent'
'com.inmobi.databus.local.LocalStreamService.getCategoryFromDestPath','org.apache.hadoop.fs.Path.getParent'
'com.inmobi.databus.local.LocalStreamService.getCategoryJobOutTmpPath','org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.local.LocalStreamService.createJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString'
'com.inmobi.databus.local.LocalStreamServiceTest.createMockForFileSystem','org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.local.LocalStreamServiceTest.testCreateListing','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.close'
'com.inmobi.databus.local.LocalStreamServiceTest.createTestData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init>'
'com.inmobi.databus.local.LocalStreamServiceTest.buildTestDatabusConfig','org.apache.hadoop.mapred.JobConf.get'
'com.inmobi.databus.local.LocalStreamServiceTest.testPopulateTrashPaths','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init>'
'com.inmobi.databus.local.LocalStreamServiceTest.testMapReduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.etsy.cascading.tap.local.LocalTap.setup','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set'
'org.honu.datacollection.writer.localfs.LocalToRemoteHdfsMover.init','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.chukwa.util.DaemonWatcher.bailout org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.getUri'
'org.honu.datacollection.writer.localfs.LocalToRemoteHdfsMover.moveFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.delete'
'org.honu.datacollection.writer.localfs.LocalToRemoteHdfsMover.cleanup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getModificationTime'
'org.apache.accumulo.server.util.LocalityCheck.run','org.apache.hadoop.fs.FileSystem.get'
'org.apache.accumulo.server.util.LocalityCheck.addBlocks','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.getFileBlockLocations org.apache.hadoop.fs.BlockLocation.getHosts'
'org.apache.accumulo.core.util.LocalityGroupUtil.encodeColumnFamilies','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.accumulo.core.util.LocalityGroupUtilTest.testEncoding','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.gora.tutorial.log.LogAnalytics.LogAnalyticsMapper.setup','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.gora.tutorial.log.LogAnalytics.LogAnalyticsReducer.reduce','org.apache.hadoop.io.LongWritable.get'
'org.apache.gora.tutorial.log.LogAnalytics.createJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass'
'org.apache.gora.tutorial.log.LogAnalytics.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.gora.tutorial.log.LogAnalytics.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.ivory.cleanup.LogCleanupServiceTest.tearDown','org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'org.apache.ivory.cleanup.LogCleanupServiceTest.setup','org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init>'
'org.apache.ivory.cleanup.LogCleanupServiceTest.testProcessLogs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.ivory.cleanup.LogCleanupServiceTest.testFeedLogs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'massive.logs.simple.LogEntryMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'org.apache.accumulo.server.logger.LogFileTest.readWrite','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.read'
'org.apache.accumulo.server.logger.LogFileTest.testReadFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.manning.hip.ch11.LogMapReduce.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch11.LogMapReduce.Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch11.LogMapReduce.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.ivory.logging.LogMover.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.ivory.logging.LogMover.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.ivory.logging.LogMover.copyOozieLog','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes'
'org.apache.ivory.logging.LogMover.copyTTlogs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes'
'org.apache.ivory.logging.LogMover.getTTlogURL','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getTaskCompletionEvents org.apache.hadoop.mapred.TaskCompletionEvent.getTaskTrackerHttp org.apache.hadoop.mapred.TaskCompletionEvent.getTaskAttemptId'
'org.apache.ivory.logging.LogProviderTest.setup','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile'
'org.apache.accumulo.server.logger.LogReader.main','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open'
'org.apache.accumulo.server.logger.LogReader.printLogEvent','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.server.tabletserver.log.LogSorter.LogProcessor.process','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init>'
'org.apache.accumulo.server.tabletserver.log.LogSorter.LogProcessor.sort','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.accumulo.server.tabletserver.log.LogSorter.LogProcessor.writeBuffer','org.apache.hadoop.fs.FileSystem.getConf'
'org.apache.accumulo.server.tabletserver.log.LogSorter.LogProcessor.close','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.accumulo.server.tabletserver.log.LogSorter.LogProcessor.getBytesCopied','org.apache.hadoop.fs.FSDataInputStream.getPos'
'com.bah.culvert.constraints.Logic.readFields','org.apache.hadoop.io.ObjectWritable.get'
'com.bah.culvert.constraints.Logic.write','org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.io.ObjectWritable.write'
'org.apache.accumulo.server.test.functional.LogicalTimeTest.runMergeTest','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.pig.LogisticRegression.LogisticRegression','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.visitante.mr.logit.LogitEstimator.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.visitante.mr.logit.LogitEstimator.main','org.apache.hadoop.util.ToolRunner.run'
'fr.eurecom.dsg.mapreduce.pagerank.utils.LongArrayWritable.toString','org.apache.hadoop.io.LongWritable.get'
'org.apache.accumulo.core.iterators.LongCombiner.VarLenEncoder.encode','org.apache.hadoop.io.WritableUtils.writeVLong'
'org.apache.accumulo.core.iterators.LongCombiner.VarLenEncoder.decode','org.apache.hadoop.io.WritableUtils.readVLong'
'com.nearinfinity.hbase.dsl.types.LongConverter.fromBytes','org.apache.hadoop.hbase.util.Bytes.toLong'
'com.nearinfinity.hbase.dsl.types.LongConverter.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.giraph.graph.LongDoubleFloatDoubleEdgeListVertex.initialize','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.get'
'org.apache.giraph.graph.LongDoubleFloatDoubleEdgeListVertex.getId','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.giraph.graph.LongDoubleFloatDoubleEdgeListVertex.getValue','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.graph.LongDoubleFloatDoubleEdgeListVertex.setValue','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.giraph.graph.LongDoubleFloatDoubleEdgeListVertex.getEdgeValue','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.graph.LongDoubleFloatDoubleEdgeListVertex.hasEdge','org.apache.hadoop.io.LongWritable.get'
'org.apache.giraph.graph.LongDoubleFloatDoubleEdgeListVertex.sendMessageToAllEdges','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.giraph.examples.LongDoubleFloatDoubleTextInputFormat.LongDoubleFloatDoubleVertexReader.getCurrentVertex','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.graph.LongDoubleNullDoubleVertex.initialize','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.LongWritable.get'
'org.apache.giraph.graph.LongDoubleNullDoubleVertex.getId','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.giraph.graph.LongDoubleNullDoubleVertex.getValue','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.graph.LongDoubleNullDoubleVertex.setValue','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.giraph.graph.LongDoubleNullDoubleVertex.getEdgeValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.graph.LongDoubleNullDoubleVertex.hasEdge','org.apache.hadoop.io.LongWritable.get'
'org.apache.giraph.graph.LongDoubleNullDoubleVertex.sendMessageToAllEdges','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.giraph.aggregators.LongMaxAggregator.aggregate','org.apache.hadoop.io.LongWritable.get'
'org.apache.giraph.aggregators.LongMaxAggregator.createInitialValue','org.apache.hadoop.io.LongWritable.<init>'
'com.manning.hip.ch6.LongSleepJob.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.giraph.aggregators.LongSumAggregator.aggregate','org.apache.hadoop.io.LongWritable.get'
'org.apache.giraph.aggregators.LongSumAggregator.createInitialValue','org.apache.hadoop.io.LongWritable.<init>'
'com.github.srec.hadoop.LongSumReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set'
'com.twitter.elephantbird.pig.util.LongWritableConverter.LongWritableConverter','org.apache.hadoop.io.LongWritable.<init>'
'com.twitter.elephantbird.pig.util.LongWritableConverter.toCharArray','org.apache.hadoop.io.LongWritable.get'
'com.twitter.elephantbird.pig.util.LongWritableConverter.toInteger','org.apache.hadoop.io.LongWritable.get'
'com.twitter.elephantbird.pig.util.LongWritableConverter.toLong','org.apache.hadoop.io.LongWritable.get'
'com.twitter.elephantbird.pig.util.LongWritableConverter.toFloat','org.apache.hadoop.io.LongWritable.get'
'com.twitter.elephantbird.pig.util.LongWritableConverter.toDouble','org.apache.hadoop.io.LongWritable.get'
'org.pentaho.hadoop.mapreduce.converter.converters.LongWritableToLongConverter.canConvert','org.apache.hadoop.io.LongWritable.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.LongWritableToLongConverter.convert','org.apache.hadoop.io.LongWritable.get'
'org.pentaho.hadoop.mapreduce.converter.converters.LongWritableToLongConverterTest.convert','org.apache.hadoop.io.LongWritable.<init>'
'edu.umd.cloud9.example.ir.LookupPostings.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.fs.FSDataInputStream.close'
'.LookupRecordByTemperature.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getReaders org.apache.hadoop.mapred.lib.HashPartitioner<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text>.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getEntry org.apache.hadoop.io.Text.toString'
'.LookupRecordByTemperature.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.scoring.webgraph.LoopReader.dumpUrl','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getReaders org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.lib.HashPartitioner<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Loops.LoopSet>.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getEntry'
'org.apache.nutch.scoring.webgraph.LoopReader.main','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.kitten.appmaster.params.lua.LuaApplicationMasterParameters.loadApplicationAttemptId','org.apache.hadoop.yarn.util.ConverterUtils.toContainerId org.apache.hadoop.yarn.api.records.ContainerId.getApplicationAttemptId'
'com.cloudera.kitten.lua.LuaContainerLaunchParameters.getUser','org.apache.hadoop.security.UserGroupInformation.getCurrentUser'
'com.cloudera.kitten.lua.LuaContainerLaunchParameters.getContainerResource','org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.Resource.getMemory org.apache.hadoop.yarn.api.records.Resource.getMemory org.apache.hadoop.yarn.api.records.Resource.setMemory'
'com.cloudera.kitten.lua.LuaContainerLaunchParameters.constructExtraResource','org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.LocalResource.setType org.apache.hadoop.yarn.api.records.LocalResource.setVisibility org.apache.hadoop.fs.Path.<init>'
'com.cloudera.kitten.lua.LuaContainerLaunchParameters.constructResource','org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.records.LocalResource.setType org.apache.hadoop.yarn.api.records.LocalResourceType.valueOf org.apache.hadoop.yarn.api.records.LocalResource.setType org.apache.hadoop.yarn.api.records.LocalResource.setVisibility org.apache.hadoop.yarn.api.records.LocalResourceVisibility.valueOf org.apache.hadoop.yarn.api.records.LocalResource.setVisibility org.apache.hadoop.yarn.util.ConverterUtils.getYarnUrlFromURI org.apache.hadoop.yarn.api.records.LocalResource.setResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'com.cloudera.kitten.lua.LuaContainerLaunchParameters.configureLocalResourceForPath','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.yarn.api.records.LocalResource.setSize org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.yarn.api.records.LocalResource.setTimestamp org.apache.hadoop.yarn.util.ConverterUtils.getYarnUrlFromPath org.apache.hadoop.yarn.api.records.LocalResource.setResource'
'com.cloudera.kitten.client.params.lua.LuaYarnClientParameters.initConf','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.digitalpebble.behemoth.mahout.LuceneTokenizerMapper.map','org.apache.hadoop.io.Text.toString'
'com.twitter.elephantbird.pig.load.LzoBaseLoadFunc.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.twitter.elephantbird.pig.load.LzoBaseLoadFunc.getNextBinaryValue','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentValue'
'com.twitter.elephantbird.mapreduce.input.LzoBinaryB64LineRecordReader.close','org.apache.hadoop.util.LineReader.close'
'com.twitter.elephantbird.mapreduce.input.LzoBinaryB64LineRecordReader.createInputReader','org.apache.hadoop.util.LineReader.<init>'
'com.twitter.elephantbird.mapreduce.input.LzoBinaryB64LineRecordReader.skipToNextSyncPoint','org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'com.twitter.elephantbird.mapreduce.input.LzoBinaryB64LineRecordReader.nextKeyValue','org.apache.hadoop.io.LongWritable.set org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.charAt org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.mapreduce.Counter.increment'
'com.twitter.elephantbird.mapreduce.input.LzoGenericProtobufBlockRecordReader.LzoGenericProtobufBlockRecordReader','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.BytesWritable.<init>'
'com.twitter.elephantbird.mapreduce.input.LzoGenericProtobufBlockRecordReader.nextKeyValue','org.apache.hadoop.io.LongWritable.set'
'com.twitter.elephantbird.mapreduce.input.LzoInputFormat.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'com.twitter.elephantbird.mapreduce.input.LzoInputFormat.listStatus','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.twitter.elephantbird.mapreduce.input.LzoInputFormat.addInputPath','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.PathFilter.accept'
'com.twitter.elephantbird.mapreduce.input.LzoInputFormat.isSplitable','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists'
'com.twitter.elephantbird.mapreduce.input.LzoInputFormat.getSplits','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.equals org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getLocations org.apache.hadoop.mapreduce.lib.input.FileSplit.<init>'
'com.twitter.elephantbird.mapreduce.input.LzoJsonRecordReader.close','org.apache.hadoop.util.LineReader.close'
'com.twitter.elephantbird.mapreduce.input.LzoJsonRecordReader.createInputReader','org.apache.hadoop.util.LineReader.<init>'
'com.twitter.elephantbird.mapreduce.input.LzoJsonRecordReader.skipToNextSyncPoint','org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'com.twitter.elephantbird.mapreduce.input.LzoJsonRecordReader.nextKeyValue','org.apache.hadoop.io.MapWritable.clear org.apache.hadoop.io.LongWritable.set org.apache.hadoop.util.LineReader.readLine'
'com.twitter.elephantbird.mapreduce.input.LzoJsonRecordReader.decodeLineToJson','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.MapWritable.put'
'com.twitter.elephantbird.pig.store.LzoJsonStorage.putNext','org.apache.hadoop.io.Text.<init>'
'com.twitter.elephantbird.examples.LzoJsonWordCount.LzoJsonWordCountMapper.map','org.apache.hadoop.io.MapWritable.entrySet org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.set'
'com.twitter.elephantbird.examples.LzoJsonWordCount.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.twitter.elephantbird.examples.LzoJsonWordCount.main','org.apache.hadoop.util.ToolRunner.run'
'com.twitter.elephantbird.mapreduce.output.LzoOutputFormat.getOutputStream','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.twitter.elephantbird.pig.store.LzoPigStorage.TupleOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.NullWritable,org.apache.pig.data.Tuple>.<init>'
'com.twitter.elephantbird.mapreduce.output.LzoProtobufBlockOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.twitter.elephantbird.cascading2.scheme.LzoTextDelimited.sourceConfInit','org.apache.hadoop.mapred.JobConf.setInputFormat'
'com.twitter.elephantbird.cascading2.scheme.LzoTextDelimited.sinkConfInit','org.apache.hadoop.mapred.JobConf.setOutputFormat'
'com.twitter.elephantbird.cascading2.scheme.LzoTextLine.sourceConfInit','org.apache.hadoop.mapred.JobConf.setInputFormat'
'com.twitter.elephantbird.cascading2.scheme.LzoTextLine.sinkConfInit','org.apache.hadoop.mapred.JobConf.setOutputFormat'
'com.twitter.elephantbird.mapreduce.output.LzoThriftBlockOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.twitter.elephantbird.mapreduce.input.LzoW3CLogInputFormat.createRecordReader','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.MapWritable>.initialize'
'com.twitter.elephantbird.pig.load.LzoW3CLogLoader.getNext','org.apache.hadoop.io.MapWritable.keySet org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'com.twitter.elephantbird.pig.load.LzoW3CLogLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'com.twitter.elephantbird.mapreduce.input.LzoW3CLogRecordReader.close','org.apache.hadoop.util.LineReader.close'
'com.twitter.elephantbird.mapreduce.input.LzoW3CLogRecordReader.createInputReader','org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'com.twitter.elephantbird.mapreduce.input.LzoW3CLogRecordReader.skipToNextSyncPoint','org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'com.twitter.elephantbird.mapreduce.input.LzoW3CLogRecordReader.nextKeyValue','org.apache.hadoop.io.MapWritable.clear org.apache.hadoop.io.LongWritable.set org.apache.hadoop.util.LineReader.readLine'
'com.twitter.elephantbird.mapreduce.input.LzoW3CLogRecordReader.decodeLine','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put'
'com.manning.hip.ch5.LzopFileReadWrite.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.manning.hip.ch5.LzopFileReadWrite.compress','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch5.LzopFileReadWrite.decompress','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'org.megalon.MArrayWritable.toStrings','org.apache.hadoop.io.Writable.toString'
'org.megalon.MArrayWritable.readFields','org.apache.hadoop.io.WritableFactories.newInstance org.apache.hadoop.io.Writable.readFields'
'org.megalon.MArrayWritable.write','org.apache.hadoop.io.Writable.write'
'cascading.memcached.MCSinkTap.getPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.crawl.MD5Signature.calculate','org.apache.hadoop.io.MD5Hash.digest'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.aggregateScalarsFiles','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.visitFRJoin','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.hasTooManyInputFiles','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.InputFormat.getSplits'
'.MRExample1Query.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion'
'.MRExample1Query.MRExample1Mapper.map','org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'.MRExample1Query.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.crunch.impl.mr.exec.MRExecutor.MRExecutor','org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchJobControl.<init>'
'org.apache.crunch.impl.mr.exec.MRExecutor.addJob','org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchJobControl.addJob'
'org.apache.crunch.impl.mr.exec.MRExecutor.execute','org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchJobControl.allFinished org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchJobControl.stop org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchJobControl.getFailedJobList org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchControlledJob.getJobName org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchControlledJob.getJobID org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchControlledJob.getMessage org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchJobControl.getSuccessfulJobList org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchControlledJob.getJobName org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchControlledJob.getJob org.apache.hadoop.mapreduce.lib.jobcontrol.CrunchControlledJob.getJobName'
'org.apache.oozie.action.hadoop.MRStats.toJSON','org.apache.hadoop.mapred.Counters.getGroupNames org.apache.hadoop.mapred.Counters.getGroup org.apache.hadoop.mapred.Counters.Counter.getName org.apache.hadoop.mapred.Counters.Counter.getValue'
'.MRToHBaseMapperTest.testJob','org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.getDFSCluster org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getValue'
'org.pentaho.hadoop.mapreduce.test.MRUtilIntegrationTest.createTrans_normalEngine','org.apache.hadoop.conf.Configuration.<init>'
'org.pentaho.hadoop.mapreduce.test.MRUtilIntegrationTest.createTrans_singleThreaded','org.apache.hadoop.conf.Configuration.<init>'
'com.hadoopilluminated.ch01.MRWordCount21.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.hadoopilluminated.ch01.MRWordCount21.main','org.apache.hadoop.util.ToolRunner.run'
'com.hadoopilluminated.ch01.MRWordCountFeatures.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.hadoopilluminated.ch01.MRWordCountFeatures.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'com.tdunning.plume.local.lazy.MSCRMapper.map','org.apache.hadoop.mapreduce.lib.input.FileInputSplitWrapper.getFileInputSplit org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'com.tdunning.plume.local.lazy.MSCRReducer.setup','org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.<init>'
'com.tdunning.plume.local.lazy.MSCRReducer.cleanup','org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.close'
'com.tdunning.plume.local.lazy.MSCRReducer.reduce','org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write'
'com.tdunning.plume.local.lazy.MSCRReducer.emit','org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write'
'org.hackreduce.models.MSD.MSD','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.common.distance.MahalanobisDistanceMeasure.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.common.distance.MahalanobisDistanceMeasure.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'com.digitalpebble.behemoth.mahout.util.Mahout2LibSVM.main','org.apache.hadoop.util.ToolRunner.run'
'com.digitalpebble.behemoth.mahout.util.Mahout2LibSVM.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.FileSystem.delete'
'com.digitalpebble.behemoth.mahout.util.Mahout2LibSVM.vectorToString','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.isSuccessful'
'com.digitalpebble.behemoth.mahout.util.Mahout2LibSVM.convert','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.isSuccessful'
'com.digitalpebble.behemoth.mahout.util.Mahout2LibSVM.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'com.digitalpebble.behemoth.mahout.util.Mahout2LibSVM.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.apache.mahout.driver.MahoutDriver.main','org.apache.hadoop.util.ProgramDriver.<init> org.apache.hadoop.util.ProgramDriver.driver org.apache.hadoop.util.ProgramDriver.driver'
'org.apache.mahout.driver.MahoutDriver.addClass','org.apache.hadoop.util.ProgramDriver.addClass'
'org.apache.mahout.driver.MahoutDriver.main','org.apache.hadoop.util.ProgramDriver.<init> org.apache.hadoop.util.ProgramDriver.driver org.apache.hadoop.util.ProgramDriver.driver'
'org.apache.mahout.driver.MahoutDriver.addClass','org.apache.hadoop.util.ProgramDriver.addClass'
'org.apache.mahout.ga.watchmaker.MahoutEvaluator.evaluate','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.ga.watchmaker.MahoutEvaluator.configureJob','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.ga.watchmaker.MahoutEvaluator.storePopulation','org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.common.MahoutTestCase.tearDown','org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.common.MahoutTestCase.getTestTempDirPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.deleteOnExit'
'org.apache.mahout.common.MahoutTestCase.getTestTempFileOrDirPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.mahout.cf.taste.example.email.MailToDictionaryReducer.reduce','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.example.email.MailToRecReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.pig.Main.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getConfiguration org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'com.manning.hip.ch4.joins.semijoin.Main.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.manning.hip.ch4.joins.semijoin.Main.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'nl.waredingen.graphs.Main.main','org.apache.hadoop.util.ToolRunner.run'
'com.manning.hip.ch7.shortestpath.Main.iterate','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.manning.hip.ch7.shortestpath.Main.createInputFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch7.shortestpath.Main.findShortestPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.v2.api.records.Counter.getValue org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.v2.api.records.CounterGroup.iterator org.apache.hadoop.mapreduce.v2.api.records.Counter.getValue'
'com.manning.hip.ch7.friendsofafriend.Main.runCalcJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch7.friendsofafriend.Main.runSortJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch7.pagerank.Main.iterate','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.manning.hip.ch7.pagerank.Main.createInputFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch7.pagerank.Main.getNumNodes','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch7.pagerank.Main.calcPageRank','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'org.apache.hcatalog.templeton.Main.loadConfig','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'org.apache.hcatalog.templeton.Main.runServer','org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab'
'org.apache.hcatalog.templeton.Main.makeAuthFilter','org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled'
'utils.MakeIntial.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.LongWritable.<init>'
'com.cloudera.sqoop.testutil.ManagerCompatTestCase.getBlobSeqOutput','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.toString'
'com.cloudera.sqoop.testutil.ManagerCompatTestCase.getVarBinarySeqOutput','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.toString'
'org.apache.hcatalog.hbase.ManyMiniCluster.start','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.hcatalog.hbase.ManyMiniCluster.stop','org.apache.hadoop.hbase.client.HConnectionManager.deleteAllConnections org.apache.hadoop.hbase.MiniHBaseCluster.shutdown org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown org.apache.hadoop.mapred.MiniMRCluster.shutdown org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.hdfs.MiniDFSCluster.shutdown org.apache.hadoop.fs.FileSystem.closeAll'
'org.apache.hcatalog.hbase.ManyMiniCluster.getHBaseConf','org.apache.hadoop.hbase.HBaseConfiguration.create'
'org.apache.hcatalog.hbase.ManyMiniCluster.getJobConf','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.hbase.ManyMiniCluster.getHiveConf','org.apache.hadoop.hive.conf.HiveConf.<init>'
'org.apache.hcatalog.hbase.ManyMiniCluster.getFileSystem','org.apache.hadoop.fs.FileSystem.get'
'org.apache.hcatalog.hbase.ManyMiniCluster.setupMRCluster','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.mapred.MiniMRCluster.createJobConf'
'org.apache.hcatalog.hbase.ManyMiniCluster.setupZookeeper','org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.<init> org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setDefaultClientPort org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup'
'org.apache.hcatalog.hbase.ManyMiniCluster.setupHBaseCluster','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.hbase.MiniHBaseCluster.<init> org.apache.hadoop.hbase.MiniHBaseCluster.getMaster org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.HTable.<init>'
'org.apache.hcatalog.hbase.ManyMiniCluster.setUpMetastore','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>'
'step2.Map2.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init>'
'step5.Map5.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init>'
'com.hadoopilluminated.ch01.MapFeatures.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.util.StringUtils.stringifyException'
'com.hadoopilluminated.ch01.MapFeatures.parseSkipFile','org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.StringUtils.stringifyException'
'com.hadoopilluminated.ch01.MapFeatures.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.setStatus'
'.MapFileFixer.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.MapFile.fix'
'org.apache.accumulo.core.iterators.system.MapFileIterator.next','org.apache.hadoop.io.MapFile.Reader.next'
'org.apache.accumulo.core.iterators.system.MapFileIterator.seek','org.apache.hadoop.io.MapFile.Reader.seek'
'org.apache.accumulo.core.iterators.system.MapFileIterator.getMetaStore','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'org.apache.accumulo.core.iterators.system.MapFileIterator.close','org.apache.hadoop.io.MapFile.Reader.close'
'.MapFileSeekTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance'
'.MapFileSeekTest.tearDown','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'.MapFileSeekTest.get','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.toString'
'.MapFileSeekTest.seek','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.toString'
'com.datasalt.pangool.tuplemr.MapOnlyJobBuilder.createJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass'
'org.apache.mahout.knn.legacy.MapRKMeansQueryTest.QueryScorer.configure','org.apache.hadoop.mapred.JobConf.get'
'org.apache.mahout.knn.legacy.MapRKMeansQueryTest.QueryScorer.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.apache.mahout.knn.legacy.MapRKMeansQueryTest.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.mahout.knn.legacy.MapRKMeansQueryTest.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.tdunning.plume.local.lazy.MapRedBypassTest.MapRedBypassWorkflow.process','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.tdunning.plume.local.lazy.MapRedBypassTest.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.datasalt.utils.mapred.counter.MapRedCounter.MapRedCounterMapper.emit','org.apache.hadoop.io.LongWritable.set'
'com.datasalt.utils.mapred.counter.MapRedCounter.MapRedCountCombiner.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set'
'com.datasalt.utils.mapred.counter.MapRedCounter.MapRedCountReducer.reduce','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.compareTo org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get'
'com.datasalt.utils.mapred.counter.MapRedCounter.buildMapRedCounterJob','org.apache.hadoop.mapreduce.Job.setCombinerClass'
'com.datasalt.utils.mapred.counter.MapRedCounter.buildMapRedCounterJobWithoutCombiner','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass'
'com.datasalt.utils.mapred.counter.MapRedCounter.addInput','org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath org.apache.hadoop.mapreduce.Job.setJarByClass'
'com.tdunning.plume.local.lazy.MapRedMultipleGroupsTest.MultipleGroupsWorkflow.build','org.apache.hadoop.io.IntWritable.<init>'
'com.tdunning.plume.local.lazy.MapRedMultipleGroupsTest.MultipleGroupsWorkflow.process','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init>'
'com.tdunning.plume.local.lazy.MapRedMultipleGroupsTest.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.tdunning.plume.local.lazy.MapRedOnlyFlattensTest.MapRedOnlyFlattensTestWorkflow.process','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'com.tdunning.plume.local.lazy.MapRedOnlyFlattensTest.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.tdunning.plume.local.lazy.MapRedSequenceFileTest.OtherWorkflow.process','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.tdunning.plume.local.lazy.MapRedSequenceFileTest.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.loadPartitionFileFromLocalCache','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.getAllFileRecursively','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.addInputPathRecursively','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.ComparableSplit.compareTo','org.apache.hadoop.mapreduce.InputSplit.getLength'
'org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.getCombinePigSplits','org.apache.hadoop.mapreduce.InputSplit.getLength org.apache.hadoop.mapreduce.InputSplit.getLength org.apache.hadoop.mapreduce.InputSplit.getLength org.apache.hadoop.mapreduce.InputSplit.getLocations'
'org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.inputSplitToString','org.apache.hadoop.mapreduce.InputSplit.getLength org.apache.hadoop.mapreduce.InputSplit.getLength org.apache.hadoop.mapreduce.InputSplit.getLocations'
'org.apache.accumulo.server.test.randomwalk.sequential.MapRedVerify.visit','org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.test.randomwalk.sequential.MapRedVerifyTool.SeqMapClass.map','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.apache.accumulo.server.test.randomwalk.sequential.MapRedVerifyTool.SeqReduceClass.writeMutation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.randomwalk.sequential.MapRedVerifyTool.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getJar org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'com.tdunning.plume.local.lazy.MapRedWordCountTest.WordCountWorkflow.process','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.tdunning.plume.local.lazy.MapRedWordCountTest.testWordCount','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.hadoop.MapReduceActionExecutor.getLauncherMain','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.MapReduceActionExecutor.setupLauncherConf','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.oozie.action.hadoop.MapReduceActionExecutor.end','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getJobName org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.JobClient.close'
'org.apache.oozie.action.hadoop.MapReduceActionExecutor.counterstoJson','org.apache.hadoop.mapred.Counters.getGroupNames org.apache.hadoop.mapred.Counters.getGroup'
'edu.jhu.thrax.hadoop.features.mapred.MapReduceFeature.getJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'cascading.flow.hadoop.MapReduceFlow.MapReduceFlow','org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobConf.getJobName'
'cascading.flow.hadoop.MapReduceFlow.createSources','org.apache.hadoop.mapred.FileInputFormat.getInputPaths org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'cascading.flow.hadoop.MapReduceFlow.createSinks','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath'
'cascading.flow.hadoop.MapReduceFlowStep.getInitializedConfig','org.apache.hadoop.mapred.JobConf.<init>'
'hbase_mapreduce.MapReduceHbaseDB.Mapper1.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.io.Text.<init>'
'hbase_mapreduce.MapReduceHbaseDB.Reducer1.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'hbase_mapreduce.MapReduceHbaseDB.Mapper2.map','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'hbase_mapreduce.MapReduceHbaseDB.Reducer2.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'hbase_mapreduce.MapReduceHbaseDB.main','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.HBaseConfiguration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumns org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumns org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.senseidb.indexing.hadoop.job.MapReduceJob.createJob','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.StringUtils.split org.apache.hadoop.util.StringUtils.unEscapeString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Trash.<init> org.apache.hadoop.fs.Trash.expunge org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.FileInputFormat.getInputPaths org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.JobConf.getNumMapTasks org.apache.hadoop.mapred.JobConf.getNumReduceTasks org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.getInputFormat org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setReduceSpeculativeExecution'
'com.senseidb.indexing.hadoop.job.MapReduceJob.getFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get'
'com.senseidb.indexing.hadoop.job.MapReduceJob.createShards','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'com.senseidb.indexing.hadoop.job.MapReduceJob.setShardGeneration','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.jena.tdbloader4.io.MapReduceLabelToNode.MapReduceAllocator.MapReduceAllocator','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.jena.tdbloader4.io.MapReduceLabelToNode.MapReduceAllocator.create','org.apache.hadoop.fs.Path.hashCode'
'org.apache.oozie.action.hadoop.MapReduceMain.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.mapred.RunningJob.getID'
'org.apache.oozie.action.hadoop.MapReduceMain.addActionConf','org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.action.hadoop.MapReduceMain.submitJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.setJar org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.JobClient.close'
'org.apache.oozie.action.hadoop.MapReduceMain.createJobClient','org.apache.hadoop.mapred.JobClient.<init>'
'org.apache.oozie.action.hadoop.MapReduceMain.setStrings','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.MapReduceMain.getStrings','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReducePOStoreImpl.MapReducePOStoreImpl','org.apache.hadoop.mapreduce.TaskInputOutputContext.getConfiguration org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.TaskInputOutputContext.getTaskAttemptID'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReducePOStoreImpl.createStoreFunc','org.apache.hadoop.mapreduce.OutputFormat.getRecordWriter'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReducePOStoreImpl.tearDown','org.apache.hadoop.mapreduce.RecordWriter.close'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReducePOStoreImpl.cleanUp','org.apache.hadoop.mapreduce.RecordWriter.close'
'org.springframework.data.hadoop.mapreduce.MapReducePropertyEditorRegistrar.BaseJobEditor.setAsText','org.apache.hadoop.mapreduce.Job.getName'
'org.springframework.data.hadoop.mapreduce.MapReducePropertyEditorRegistrar.BaseJobEditor.setValue','org.apache.hadoop.mapreduce.Job.getName'
'de.tuberlin.dima.aim.exercises.two.MapSideInMemoryBookAndAuthorJoin.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'step2.MapTest.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init>'
'org.apache.nutch.crawl.MapWritable.MapWritable','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.crawl.MapWritable.write','org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.crawl.MapWritable.readFields','org.apache.hadoop.io.Text.readString'
'org.apache.nutch.crawl.MapWritable.KeyValueEntry.toString','org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'.MapWritableTest.mapWritable','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.WritableUtils.cloneInto org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.LongWritable.<init>'
'.MapWritableTest.setWritableEmulation','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.WritableUtils.cloneInto org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.MapWritable.containsKey'
'org.apache.nutch.crawl.MapWritable.MapWritable','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.crawl.MapWritable.write','org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.crawl.MapWritable.readFields','org.apache.hadoop.io.Text.readString'
'org.apache.nutch.crawl.MapWritable.KeyValueEntry.toString','org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'com.manning.hip.ch7.shortestpath.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'com.hadoopilluminated.ch01.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'massive.logs.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'WordCount.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'neuralnet.mapred.Map.Map','org.apache.hadoop.io.Text.<init>'
'neuralnet.mapred.Map.readRunParams','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'neuralnet.mapred.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'tap.core.MapperBridge.configure','org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapred.JobConf.getNumReduceTasks org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'tap.core.MapperBridge.determineInputFormat','org.apache.hadoop.mapred.JobConf.getInputFormat org.apache.hadoop.mapred.JobConf.getInputFormat org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapred.JobConf.getInputFormat'
'tap.core.MapperBridge.determineOutputFormat','org.apache.hadoop.mapred.JobConf.getClass'
'tap.core.MapperBridge.Collector.collect','org.apache.hadoop.mapred.OutputCollector<UNRESOLVED.KO,UNRESOLVED.VO>.collect org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<UNRESOLVED.KO,UNRESOLVED.VO>.collect org.apache.hadoop.mapred.OutputCollector<UNRESOLVED.KO,UNRESOLVED.VO>.collect'
'tap.core.MapperBridge.invokeMapper','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter'
'tap.core.MapperBridge.sniffMapInFormat','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'tap.core.MapperBridge.readHeader','org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.oozie.action.hadoop.MapperReducerCredentialsForTest.configure','org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.MapperReducerCredentialsForTest.map','org.apache.hadoop.mapred.OutputCollector.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.oozie.action.hadoop.MapperReducerCredentialsForTest.reduce','org.apache.hadoop.mapred.OutputCollector.collect'
'org.apache.oozie.action.hadoop.MapperReducerCredentialsForTest.hasCredentials','org.apache.hadoop.mapred.RunningJob.getCounters org.apache.hadoop.mapred.Counters.getGroup'
'org.apache.oozie.action.hadoop.MapperReducerForTest.map','org.apache.hadoop.mapred.OutputCollector.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.oozie.action.hadoop.MapperReducerForTest.reduce','org.apache.hadoop.mapred.OutputCollector.collect'
'org.apache.oozie.action.hadoop.MapperReducerUberJarForTest.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector.collect org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.oozie.action.hadoop.MapperReducerUberJarForTest.reduce','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector.collect'
'extramuros.java.jobs.timeseries.stationality.Mapper.map','org.apache.hadoop.io.LongWritable.<init>'
'extramuros.java.jobs.timeseries.stationality.Mapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get'
'org.apache.crunch.lib.join.MapsideJoin.join','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.toString'
'org.apache.crunch.lib.join.MapsideJoin.MapsideJoinDoFn.getCacheFilePath','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.makeQualified'
'com.cloudera.flume.handlers.hive.MarkerStore.MarkerStore','org.apache.hadoop.hive.service.HiveClient.<init> org.apache.hadoop.hive.service.HiveClient.<init>'
'com.cloudera.flume.handlers.hive.MarkerStore.runElasticSearchMarkerQueries','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.cloudera.flume.handlers.hive.MarkerStore.writeElasticSearchToMarkerFolder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'com.cloudera.flume.handlers.hive.MarkerStore.runHiveMarkerQueries','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.close'
'com.cloudera.flume.handlers.hive.MarkerStore.cleanMarkerFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete'
'com.cloudera.flume.handlers.hive.MarkerStore.runHiveQuery','org.apache.hadoop.hive.service.HiveClient.<init> org.apache.hadoop.hive.service.HiveClient.execute org.apache.hadoop.hive.service.HiveServerException.getMessage'
'com.cloudera.flume.handlers.hive.MarkerStore.writeHiveMarker','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'com.cloudera.flume.handlers.hive.MarkerStore.mergeFiles','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete'
'com.cloudera.flume.handlers.hive.MarkerStore.checkIfPartitionExists','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists'
'org.hackreduce.examples.stockexchange.MarketCapitalization.MarketCapitalizationMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.hackreduce.examples.stockexchange.MarketCapitalization.MarketCapitalizationReducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.Text.<init>'
'org.hackreduce.examples.stockexchange.MarketCapitalization.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.hackreduce.examples.stockexchange.MarketCapitalization.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.master.Master.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.accumulo.server.master.Master.nonMetaDataTabletsAssignedOrHosted','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.Master.displayUnassigned','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.server.master.Master.MasterClientServiceHandler.waitForFlush','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.server.master.Master.MasterClientServiceHandler.executeTableOperation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.server.master.Master.getMergeInfo','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.accumulo.server.master.Master.setMergeState','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData'
'org.apache.accumulo.server.master.Master.clearMergeState','org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.server.master.Master.TabletGroupWatcher.sendSplitRequest','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.server.master.Master.TabletGroupWatcher.deleteTablets','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.Master.TabletGroupWatcher.mergeMetadataRecords','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.Master.recoverLogs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.accumulo.server.master.Master.merges','org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.graph.MasterAggregatorHandler.prepareSuperstep','org.apache.hadoop.io.Writable.readFields'
'org.apache.giraph.graph.MasterAggregatorHandler.readFields','org.apache.hadoop.io.Writable.readFields'
'org.hbase.tdg.MasterObserverExample.postCreateTable','org.apache.hadoop.hbase.HRegionInfo.getTableNameAsString org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>.getEnvironment org.apache.hadoop.hbase.master.MasterServices.getMasterFileSystem org.apache.hadoop.hbase.master.MasterFileSystem.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'coprocessor.MasterObserverExample.postCreateTable','org.apache.hadoop.hbase.HRegionInfo.getTableDesc org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>.getEnvironment org.apache.hadoop.hbase.master.MasterServices.getMasterFileSystem org.apache.hadoop.hbase.master.MasterFileSystem.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists'
'org.commoncrawl.service.queryserver.master.MasterServer.queueClientQueryRequest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.commoncrawl.service.queryserver.master.MasterServer.initServer','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.queryserver.master.MasterServer.parseArguements','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.queryserver.master.MasterServer.locateQueryDBPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.service.queryserver.master.MasterServer.getBaseConfigForSlave','org.apache.hadoop.fs.Path.toString'
'org.apache.giraph.graph.MasterThread.MasterThread','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration'
'org.apache.giraph.graph.MasterThread.run','org.apache.hadoop.mapreduce.Mapper.Context.getCounter org.apache.hadoop.mapreduce.Mapper.Context.getCounter org.apache.hadoop.mapreduce.Mapper.Context.getCounter org.apache.hadoop.mapreduce.Mapper.Context.progress org.apache.hadoop.mapreduce.Mapper.Context.getCounter'
'org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob.MatrixDiagonalizeMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob.MatrixDiagonalizeMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.NullWritable.get'
'org.cloudata.util.matrix.MatrixInputFormat.MatrixRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'org.cloudata.util.matrix.MatrixInputFormat.MatrixRecordReader.next','org.apache.hadoop.io.Text.set'
'org.cloudata.util.matrix.MatrixInputFormat.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.util.matrix.MatrixInputFormat.validateInput','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.apache.mahout.math.hadoop.MatrixMultiplicationJob.createMatrixMultiplyJobConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.join.CompositeInputFormat.compose org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.apache.mahout.math.hadoop.MatrixMultiplicationJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.hadoop.MatrixMultiplicationJob.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.MatrixMultiplicationJob.MatrixMultiplyMapper.configure','org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.mahout.math.hadoop.MatrixMultiplicationJob.MatrixMultiplyMapper.map','org.apache.hadoop.mapred.join.TupleWritable.get org.apache.hadoop.mapred.join.TupleWritable.get org.apache.hadoop.mapred.join.TupleWritable.get org.apache.hadoop.mapred.join.TupleWritable.get org.apache.hadoop.mapred.join.TupleWritable.get org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.mahout.math.VectorWritable>.collect'
'org.apache.mahout.math.hadoop.MatrixMultiplicationJob.MatrixMultiplicationReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.mahout.math.VectorWritable>.collect'
'.MatrixMultiplicationOneDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.LocalFileSystem.<init> org.apache.hadoop.fs.LocalFileSystem.initialize org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileUtil.copy org.apache.hadoop.mapred.lib.MultipleOutputs.addMultiNamedOutput org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputValueGroupingComparator org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'.MatrixMultiplicationOneDriver.main','org.apache.hadoop.util.ToolRunner.run'
'.MatrixMultiplicationOneOutputKeyComparator.compare','org.apache.hadoop.io.WritableComparable.toString org.apache.hadoop.io.WritableComparable.toString'
'.MatrixMultiplicationOneReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'com.zinnia.nectar.regression.hadoop.primitive.jobs.MatrixMultiplyJob.call','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.mycompany.hiaex.MatrixMultiplyVector.Mapper1.setup','org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.StringUtils.getStrings'
'com.mycompany.hiaex.MatrixMultiplyVector.Mapper1.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.util.StringUtils.getStrings org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'com.mycompany.hiaex.MatrixMultiplyVector.Reducer1.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'com.mycompany.hiaex.MatrixMultiplyVector.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.mycompany.hiaex.MatrixMultiplyVector.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.cloudata.util.matrix.MatrixMutiplyMap.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.cloudata.util.matrix.MatrixItem,org.apache.hadoop.io.Text>.collect'
'org.cloudata.util.matrix.MatrixMutiplyMap.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.util.matrix.MatrixMutiplyReduce.reduce','org.apache.hadoop.io.Text.toString'
'org.cloudata.util.matrix.MatrixMutiplyReduce.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.MatrixTransposeMapper.setup','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.MatrixTransposeMapper.map','org.apache.hadoop.io.LongWritable.<init>'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.MatrixTransposeReducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.math.MatrixUtils.write','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'org.apache.mahout.math.MatrixWritableTest.writeAndRead','org.apache.hadoop.io.Writable.write org.apache.hadoop.io.Writable.readFields'
'org.apache.hama.graph.MaxAggregator.aggregate','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'org.apache.hama.graph.MaxAggregator.getValue','org.apache.hadoop.io.IntWritable.<init>'
'com.livingsocial.hive.udf.MaxDate.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init>'
'edu.ucsc.srl.damasc.netcdf.reduce.MaxReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'.MaxTemperature.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'.MaxTemperatureByStationNameUsingDistributedCacheFile.StationTemperatureMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'.MaxTemperatureByStationNameUsingDistributedCacheFile.MaxTemperatureReducerWithStationLookup.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'.MaxTemperatureByStationNameUsingDistributedCacheFile.run','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'.MaxTemperatureByStationNameUsingDistributedCacheFile.main','org.apache.hadoop.util.ToolRunner.run'
'v4.MaxTemperatureDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'v4.MaxTemperatureDriver.main','org.apache.hadoop.util.ToolRunner.run'
'v3.MaxTemperatureDriverTest.OutputLogFilter.accept','org.apache.hadoop.fs.Path.getName'
'v3.MaxTemperatureDriverTest.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'v3.MaxTemperatureDriverTest.checkOutput','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.open'
'v7.MaxTemperatureDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'v7.MaxTemperatureDriver.main','org.apache.hadoop.util.ToolRunner.run'
'v6.MaxTemperatureDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setProfileEnabled org.apache.hadoop.mapred.JobConf.setProfileParams org.apache.hadoop.mapred.JobConf.setProfileTaskRange org.apache.hadoop.mapred.JobClient.runJob'
'v6.MaxTemperatureDriver.main','org.apache.hadoop.util.ToolRunner.run'
'v5.MaxTemperatureDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'v5.MaxTemperatureDriver.main','org.apache.hadoop.util.ToolRunner.run'
'v4.MaxTemperatureDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'v4.MaxTemperatureDriver.main','org.apache.hadoop.util.ToolRunner.run'
'v2.MaxTemperatureDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'v2.MaxTemperatureDriver.main','org.apache.hadoop.util.ToolRunner.run'
'v4.MaxTemperatureMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'v3.MaxTemperatureMapperTest.processesValidRecord','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withMapper org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'v3.MaxTemperatureMapperTest.processesPositiveTemperatureRecord','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withMapper org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'v3.MaxTemperatureMapperTest.ignoresMissingTemperatureRecord','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withMapper'
'v3.MaxTemperatureMapperTest.ignoresSuspectQualityRecord','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withMapper'
'org.hadoop.tdg.MaxTemperatureMapperTest.processValidRecord','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.hadoop.tdg.MaxTemperatureMapperTest.ignoresMissingTemperatureRecord','org.apache.hadoop.io.Text.<init>'
'v1.MaxTemperatureMapperTest.processesValidRecord','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'v1.MaxTemperatureMapperTest.ignoresMissingTemperatureRecord','org.apache.hadoop.io.Text.<init>'
'v1.MaxTemperatureMapperTest.processesMalformedTemperatureRecord','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'v3.MaxTemperatureMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'hadoop.MaxTemperatureMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'v5.MaxTemperatureMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.hadoop.tdg.MaxTemperatureMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'.MaxTemperatureMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'v4.MaxTemperatureMapper.map','org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'v3.MaxTemperatureMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'v1.MaxTemperatureReducerTest.returnsMaximumIntegerInValues','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withReducer org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'.MaxTemperatureReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'v1.MaxTemperatureReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'jocl.MaxTemperatureReducer.reduce','org.apache.hadoop.io.IntWritable.<init>'
'.MaxTemperatureWithCompression.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'com.hadoopbook.hive.Maximum.MaximumIntUDAFEvaluator.iterate','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'org.goldenorb.algorithms.maximumValue.MaximumValueVertex.compute','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.goldenorb.algorithms.maximumValue.MaximumValueVertexWriter.vertexWrite','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.hadoop.MaybePruneRowsMapperTest.testPruning','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.mapreduce.Counter.increment'
'org.lilyproject.tools.mboximport.MboxMetrics.MboxMetrics','org.apache.hadoop.metrics.MetricsUtil.getContext org.apache.hadoop.metrics.MetricsUtil.createRecord org.apache.hadoop.metrics.MetricsContext.registerUpdater'
'org.lilyproject.tools.mboximport.MboxMetrics.shutdown','org.apache.hadoop.metrics.MetricsContext.unregisterUpdater'
'org.lilyproject.tools.mboximport.MboxMetrics.doUpdates','org.apache.hadoop.metrics.util.MetricsRegistry.getMetricsList org.apache.hadoop.metrics.util.MetricsBase.pushMetric org.apache.hadoop.metrics.MetricsRecord.update'
'com.hadoopbook.hive.Mean.MeanDoubleUDAFEvaluator.iterate','org.apache.hadoop.hive.serde2.io.DoubleWritable.get'
'com.hadoopbook.hive.Mean.MeanDoubleUDAFEvaluator.terminate','org.apache.hadoop.hive.serde2.io.DoubleWritable.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyClusterMapper.map','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyClusterMapper.getCanopies','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyClusterer.MeanShiftCanopyClusterer','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyCreatorMapper.map','org.apache.hadoop.io.WritableComparable<?>.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyCreatorMapper.map','org.apache.hadoop.io.WritableComparable<?>.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.createCanopyFromVectorsSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.createCanopyFromVectorsMR','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.buildClustersSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.buildClustersMR','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.runIterationMR','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.clusterDataSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.clusterDataMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.createCanopyFromVectorsSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.createCanopyFromVectorsMR','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.buildClustersSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.buildClustersMR','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.runIterationMR','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.clusterDataSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver.clusterDataMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyMapper.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.meanshift.MeanShiftCanopyMapper.cleanup','org.apache.hadoop.io.Text.<init>'
'cascading.tap.hadoop.util.MeasuredOutputCollector.collect','org.apache.hadoop.mapred.OutputCollector.collect'
'edu.ucsc.srl.damasc.netcdf.reduce.MedianReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.collection.medline.MedlineCitationInputFormat.MedlineCitationRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'extramuros.java.jobs.clustering.proclus.algorithm.Medoid.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'extramuros.java.jobs.clustering.proclus.algorithm.Medoid.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'org.apache.crunch.impl.mem.collect.MemGroupedTable.createMapFor','org.apache.hadoop.util.ReflectionUtils.newInstance'
'edu.umd.cloud9.example.hits.MergeFormattedRecords.MergeReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.example.hits.HITSNode>.collect'
'edu.umd.cloud9.example.hits.MergeFormattedRecords.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.hits.MergeFormattedRecords.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.example.hits.MergeFormattedRecords.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.master.state.MergeInfoTest.readWrite','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.accumulo.server.master.state.MergeInfoTest.ke','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.sqoop.mapreduce.MergeJob.runMergeJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.getClassByName org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass'
'org.lilyproject.hbaseindex.test.MergeJoinTest.testConjunction','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.hbaseindex.test.MergeJoinTest.testDisjunction','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.hbaseindex.test.MergeJoinTest.buildQueryResult','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.sqoop.mapreduce.MergeMapperBase.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.mapreduce.MergeMapperBase.processRecord','org.apache.hadoop.io.Text.<init>'
'org.apache.sqoop.mapreduce.MergeReducer.reduce','org.apache.hadoop.io.NullWritable.get'
'org.apache.accumulo.server.master.state.MergeStats.verifyMergeConsistency','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.state.MergeStats.main','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'com.inmobi.databus.utils.MergeStreamDataConsistency.listingValidation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem'
'com.inmobi.databus.utils.MergeStreamDataConsistency.doRecursiveListing','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.utils.MergeStreamDataConsistency.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.accumulo.server.test.functional.MergeTest.runMergeTest','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.sqoop.mapreduce.MergeTextMapper.setup','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.sqoop.tool.MergeTool.run','org.apache.hadoop.util.StringUtils.stringifyException'
'co.nubetech.hiho.merge.MergeValueMapper.setup','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.pig.MessageLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'org.apache.mahout.pig.MessageLoader.createRecordReader','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'org.apache.mahout.pig.MessageLoader.getInputFormat','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init>'
'org.apache.mahout.pig.MessageLoader.getNext','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.getLength org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue'
'org.msgpack.hadoop.mapreduce.output.MessagePackOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create'
'org.msgpack.hadoop.mapred.MessagePackOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create'
'org.msgpack.hadoop.mapreduce.input.MessagePackRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'org.msgpack.hadoop.mapreduce.input.MessagePackRecordReader.nextKeyValue','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.io.LongWritable.set'
'org.apache.ivory.messaging.MessageProducer.main','org.apache.hadoop.util.ToolRunner.run'
'.MetOfficeRecordParser.parse','org.apache.hadoop.io.Text.toString'
'org.apache.cassandra.hadoop.hive.metastore.MetaStorePersister.buildEntityColumnName','org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Index.getOrigTableName org.apache.hadoop.hive.metastore.api.Index.getIndexName org.apache.hadoop.hive.metastore.api.Partition.getTableName org.apache.hadoop.hive.metastore.api.Partition.getValues org.apache.hadoop.hive.metastore.api.Type.getName'
'org.apache.cassandra.hadoop.hive.metastore.MetaStorePersisterTest.testBasicPersistMetaStoreEntity','org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName org.apache.hadoop.hive.metastore.api.Database.setDescription org.apache.hadoop.hive.metastore.api.Database.setLocationUri org.apache.hadoop.hive.metastore.api.Database.setParameters org.apache.hadoop.hive.metastore.api.Database.getName'
'org.apache.cassandra.hadoop.hive.metastore.MetaStorePersisterTest.testEntityNotFound','org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName'
'org.apache.cassandra.hadoop.hive.metastore.MetaStorePersisterTest.testBasicLoadMetaStoreEntity','org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName org.apache.hadoop.hive.metastore.api.Database.setDescription org.apache.hadoop.hive.metastore.api.Database.setLocationUri org.apache.hadoop.hive.metastore.api.Database.setParameters org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName'
'org.apache.cassandra.hadoop.hive.metastore.MetaStorePersisterTest.testFindMetaStoreEntities','org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName org.apache.hadoop.hive.metastore.api.Database.setDescription org.apache.hadoop.hive.metastore.api.Database.setLocationUri org.apache.hadoop.hive.metastore.api.Database.setParameters org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Table.<init> org.apache.hadoop.hive.metastore.api.Table.setDbName org.apache.hadoop.hive.metastore.api.Table.setTableName org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.setTableName org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.setTableName org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.setTableName org.apache.hadoop.hive.metastore.api.Table.getDbName'
'org.apache.cassandra.hadoop.hive.metastore.MetaStorePersisterTest.testEntityDeletion','org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName org.apache.hadoop.hive.metastore.api.Database.setDescription org.apache.hadoop.hive.metastore.api.Database.setLocationUri org.apache.hadoop.hive.metastore.api.Database.setParameters org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Table.<init> org.apache.hadoop.hive.metastore.api.Table.setDbName org.apache.hadoop.hive.metastore.api.Table.setTableName org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.api.Database.setName org.apache.hadoop.hive.metastore.api.Table.<init> org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.setDbName org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Table.setTableName'
'org.apache.cassandra.hadoop.hive.metastore.MetaStoreTestBase.buildConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.cassandra.hadoop.hive.metastore.MetaStoreTestBase.setupOtherKeyspace','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.parse.MetaTagsParser.setConf','org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.server.constraints.MetadataConstraints.isValidColumn','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.constraints.MetadataConstraints.check','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo'
'org.apache.nutch.indexer.metadata.MetadataIndexer.filter','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.nutch.indexer.metadata.MetadataIndexer.setConf','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.accumulo.server.util.MetadataTable.updateTabletDataFile','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.getMetadataDirectoryEntries','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.getDataFileSizes','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.addNewTablet','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.finishSplit','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.replaceDatafiles','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.createDeleteMutation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.removeScanFiles','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.fixSplit','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.splitDatafiles','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.server.util.MetadataTable.deleteTable','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.LogEntry.toBytes','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeLong org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.apache.accumulo.server.util.MetadataTable.LogEntry.fromBytes','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.DataInputBuffer.readUTF org.apache.hadoop.io.DataInputBuffer.readUTF org.apache.hadoop.io.DataInputBuffer.read org.apache.hadoop.io.DataInputBuffer.read org.apache.hadoop.io.DataInputBuffer.readUTF'
'org.apache.accumulo.server.util.MetadataTable.addLogEntry','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.getFileAndLogEntries','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.accumulo.server.util.MetadataTable.getLogEntries','org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.server.util.MetadataTable.removeUnusedWALEntries','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.createCloneMutation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.createCloneScanner','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.initializeClone','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.compareEndRows','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.checkClone','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.cloneTable','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.util.MetadataTable.removeBulkLoadEntries','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.util.MetadataTable.getMetadataLocationEntries','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.util.MetadataTable.getTabletAndPrevTabletKeyValues','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.util.MetadataTable.getEntries','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.core.util.MetadataTable.isContiguousRange','org.apache.hadoop.io.Text.compareTo'
'org.apache.nutch.metadata.Metadata.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.metadata.Metadata.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.lilyproject.server.modules.general.Metrics.Metrics','org.apache.hadoop.metrics.ContextFactory.getFactory org.apache.hadoop.metrics.ContextFactory.setAttribute org.apache.hadoop.metrics.jvm.JvmMetrics.init'
'org.lilyproject.util.hbase.metrics.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged','org.apache.hadoop.metrics.util.MetricsRegistry.size'
'org.lilyproject.util.hbase.metrics.MetricsDynamicMBeanBase.createMBeanInfo','org.apache.hadoop.metrics.util.MetricsRegistry.size org.apache.hadoop.metrics.util.MetricsRegistry.getMetricsList org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.isInstance org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getDescription org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getDescription org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getDescription org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getDescription org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsIntValue.isInstance org.apache.hadoop.metrics.util.MetricsTimeVaryingInt.isInstance org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getDescription org.apache.hadoop.metrics.util.MetricsLongValue.isInstance org.apache.hadoop.metrics.util.MetricsTimeVaryingLong.isInstance org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getDescription org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getDescription org.apache.hadoop.hbase.metrics.MetricsRate.isInstance org.apache.hadoop.metrics.util.MetricsBase.getName org.apache.hadoop.metrics.util.MetricsBase.getDescription org.apache.hadoop.metrics.util.MetricsBase.getClass'
'org.lilyproject.util.hbase.metrics.MetricsDynamicMBeanBase.getAttribute','org.apache.hadoop.metrics.util.MetricsRegistry.get org.apache.hadoop.metrics.util.MetricsIntValue.get org.apache.hadoop.metrics.util.MetricsLongValue.get org.apache.hadoop.metrics.util.MetricsTimeVaryingInt.getPreviousIntervalValue org.apache.hadoop.metrics.util.MetricsTimeVaryingLong.getPreviousIntervalValue org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.getPreviousIntervalNumOps org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.getPreviousIntervalAverageTime org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.getMinTime org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.getMaxTime org.apache.hadoop.hbase.metrics.MetricsRate.getPreviousIntervalValue'
'org.lilyproject.util.hbase.metrics.MetricsDynamicMBeanBase.invoke','org.apache.hadoop.metrics.util.MetricsRegistry.getMetricsList org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.isInstance org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.cast'
'com.acme.marketing.MetroResolver.exec','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.nutch.crawl.MimeAdaptiveFetchSchedule.setConf','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getConfResourceAsReader org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.crawl.MimeAdaptiveFetchSchedule.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.util.MimeUtil.MimeUtil','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getConfResourceAsInputStream org.apache.hadoop.conf.Configuration.getBoolean'
'com.livingsocial.hive.udf.MinDate.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init>'
'org.apache.mahout.clustering.minhash.MinHashDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.minhash.MinHashDriver.runJob','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.minhash.MinHashReducer.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.clustering.minhash.MinHashReducer.reduce','org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.minhash.MinHashReducer.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.clustering.minhash.MinHashReducer.reduce','org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Text.<init>'
'org.apache.hama.examples.MindistSearch.MindistSearchVertex.compute','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo'
'org.apache.hama.examples.MindistSearch.MinTextCombiner.combine','org.apache.hadoop.io.Text.compareTo'
'org.apache.hama.examples.MindistSearch.MindistSearchCountReader.parseVertex','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.hama.examples.MindistSearch.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'ivory.lsh.MinhashSignatureTest.testReadWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'ivory.lsh.MinhashSignatureTest.testSignatureSizeOnDisk','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init>'
'com.nearinfinity.blur.MiniCluster.getFileSystemUri','org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem'
'com.nearinfinity.blur.MiniCluster.startDfs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.waitActive'
'com.nearinfinity.blur.MiniCluster.shutdownDfs','org.apache.hadoop.hdfs.MiniDFSCluster.shutdown org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.closeAll'
'org.apache.pig.test.MiniGenericCluster.shutdownMiniDfsClusters','org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'org.apache.pig.test.MiniGenericCluster.getConfiguration','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.pig.test.MiniGenericCluster.setProperty','org.apache.hadoop.conf.Configuration.set'
'com.wibidata.maven.plugins.hbase.MiniHBaseCluster.MiniHBaseCluster','org.apache.hadoop.hbase.HBaseTestingUtility.<init>'
'com.wibidata.maven.plugins.hbase.MiniHBaseCluster.getConfiguration','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration'
'com.wibidata.maven.plugins.hbase.MiniHBaseCluster.startup','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.HBaseTestingUtility.startMiniMapReduceCluster org.apache.hadoop.hbase.HBaseTestingUtility.getDataTestDir org.apache.hadoop.conf.Configuration.set'
'com.wibidata.maven.plugins.hbase.MiniHBaseCluster.shutdown','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniMapReduceCluster org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'com.wibidata.maven.plugins.hbase.MiniHBaseCluster.configure','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'org.apache.giraph.examples.MinimumDoubleCombiner.combine','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.MinimumIntCombiner.combine','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.apache.giraph.examples.MinimumIntCombinerTest.testCombiner','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.accumulo.server.tabletserver.MinorCompactor.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.utils.MirrorStreamDataConsistencyValidation.MirrorStreamDataConsistencyValidation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.utils.MirrorStreamDataConsistencyValidation.mergedStreamListing','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem'
'com.inmobi.databus.utils.MirrorStreamDataConsistencyValidation.mirrorStreamListing','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toString'
'com.inmobi.databus.utils.MirrorStreamDataConsistencyValidation.doRecursiveListing','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.utils.MirrorStreamDataConsistencyValidation.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.distcp.MirrorStreamService.execute','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.tools.DistCpOptions.setPreserveSrcPath'
'com.inmobi.databus.distcp.MirrorStreamService.prepareForCommit','org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.distcp.MirrorStreamService.createCommitPaths','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.distcp.MirrorStreamService.createListing','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'client.MissingRegionExample.printTableRegions','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getStartEndKeys org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getFirst org.apache.hadoop.hbase.util.Pair<byte[],byte[]>.getSecond org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toStringBinary'
'client.MissingRegionExample.Getter.run','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get'
'client.MissingRegionExample.main','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HTable.getRegionLocation org.apache.hadoop.hbase.HRegionLocation.getRegionInfo org.apache.hadoop.hbase.HRegionLocation.getRegionInfo org.apache.hadoop.hbase.client.HBaseAdmin.closeRegion org.apache.hadoop.hbase.client.HTable.getRegionLocations org.apache.hadoop.hbase.HRegionLocation.getRegionInfo org.apache.hadoop.hbase.HRegionLocation.getRegionInfo org.apache.hadoop.hbase.client.HBaseAdmin.assign'
'.MissingTemperatureFields.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.TaskTracker.RunningJob.isComplete org.apache.hadoop.mapred.TaskTracker.RunningJob.getCounters org.apache.hadoop.mapreduce.Counters.getCounter org.apache.hadoop.mapreduce.Counters.findCounter'
'.MissingTemperatureFields.main','org.apache.hadoop.util.ToolRunner.run'
'com.senseidb.indexing.hadoop.reduce.MixedDirectory.MixedDirectory','org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.servlet.MockCoordinatorEngineService.createDummyCoordinatorJob','org.apache.hadoop.conf.Configuration.toString'
'org.apache.oozie.servlet.MockCoordinatorEngineService.createDummyCoordinatorJob','org.apache.hadoop.conf.Configuration.toString'
'org.apache.gora.mock.store.MockDataStore.get','org.apache.hadoop.conf.Configuration.<init>'
'com.ripariandata.timberwolf.writer.hbase.MockHTableTest.put','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.ripariandata.timberwolf.writer.hbase.MockHTableTest.get','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.ripariandata.timberwolf.writer.hbase.MockHTableTest.testResetValue','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.bah.culvert.mock.MockIndex.MockIndex','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.accumulo.core.client.mock.MockInstance.getDefaultFileSystem','org.apache.hadoop.fs.FileSystem.get'
'org.apache.accumulo.core.client.mock.MockInstance.getConnector','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.compiler.flow.mock.MockOutput.MockOutput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'com.asakusafw.compiler.flow.mock.MockOutput.MockStatusReporter.getCounter','org.apache.hadoop.mapreduce.Counter.<init>'
'org.goldenorb.util.MockPartitionThread.run','org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.ipc.RPC.Server.start org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.ipc.RPC.Server.start'
'org.goldenorb.util.MockPartitionThread.HeartbeatGenerator.run','org.apache.hadoop.io.LongWritable.<init>'
'com.asakusafw.testdriver.directio.MockStreamFormat.readTo','org.apache.hadoop.io.Text.set'
'com.asakusafw.testdriver.directio.MockStreamFormat.write','org.apache.hadoop.io.Text.toString'
'com.bah.culvert.mock.MockTableAdapter.MockTableAdapter','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.accumulo.core.client.mock.MockTableOperationsTest.prepareTestFiles','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getParent'
'org.apache.accumulo.core.client.mock.MockTableOperationsTest.testFailsWithNonEmptyFailureDirectory','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.accumulo.core.client.mock.MockTabletLocator.binRanges','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.testdriver.directio.MockTextDefinition.toReflection','org.apache.hadoop.io.Text.toString'
'com.asakusafw.testdriver.directio.MockTextDefinition.toObject','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'com.asakusafw.testdriver.file.MockTextDefinition.toReflection','org.apache.hadoop.io.Text.toString'
'com.asakusafw.testdriver.file.MockTextDefinition.toObject','org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.utils.MockUtils.answer','org.apache.hadoop.io.IntWritable.get'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.ModeHandler.get','org.apache.hadoop.fs.FileStatus.getPermission'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.ModeHandler.set','org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.setPermission'
'edu.umd.hooka.alignment.model1.Model1.processTrainingInstance','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.progress'
'edu.umd.hooka.alignment.model1.Model1_InitUniform.processTrainingInstance','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.progress'
'org.apache.camel.component.hbase.filters.ModelAwareFilterList.apply','org.apache.hadoop.hbase.filter.Filter.getClass'
'org.apache.camel.component.hbase.filters.ModelAwareFilterList.wrap','org.apache.hadoop.hbase.filter.FilterList.getOperator org.apache.hadoop.hbase.filter.FilterList.getFilters'
'org.apache.camel.component.hbase.filters.ModelAwareWhileMatchFilter.wrap','org.apache.hadoop.hbase.filter.WhileMatchFilter.getFilter'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.ModifyTimeHandler.get','org.apache.hadoop.fs.FileStatus.getModificationTime'
'com.mongodb.hadoop.MongoConfigUnitTests.testConstructor','org.apache.hadoop.conf.Configuration.<init>'
'com.mongodb.hadoop.MongoInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.mongodb.hadoop.mapred.MongoInputFormat.getRecordReader','org.apache.hadoop.mapred.JobTracker.IllegalStateException.<init>'
'org.yong3.hive.mongo.MongoInputFormat.getRecordReader','org.apache.hadoop.hive.serde2.ColumnProjectionUtils.getReadColumnIDs org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.hive.ql.exec.Utilities.deserializeExpression'
'org.yong3.hive.mongo.MongoInputFormat.dumpFilterExpr','org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getCols org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getExprString org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getChildren'
'com.mongodb.hadoop.streaming.io.MongoInputWriter.initialize','org.apache.hadoop.streaming.PipeMapRed.getClientOutput'
'com.mongodb.hadoop.pig.MongoLoader.setLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.mongodb.hadoop.pig.MongoLoader.prepareToRead','org.apache.hadoop.conf.Configuration.get'
'com.mongodb.hadoop.MongoOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.mongodb.hadoop.streaming.io.MongoOutputReader.initialize','org.apache.hadoop.streaming.PipeMapRed.getClientInput'
'org.yong3.hive.mongo.MongoSplit.getSplits','org.apache.hadoop.mapred.FileInputFormat.getInputPaths'
'com.mongodb.hadoop.streaming.MongoStreamJob.run','org.apache.hadoop.conf.Configuration.setClass'
'com.mongodb.hadoop.streaming.MongoStreamJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.indexer.mongodb.MongodbWriter.open','org.apache.hadoop.mapred.JobConf.get'
'ivory.lsh.bitext.MoreGenericModelReader.MoreGenericModelReader','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.memonews.hbase.MoveColumnFamilyData.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.MoveSegmentsStep.findNextSegmentId','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.MoveSegmentsStep.runStep','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent'
'com.dappit.Dapper.parser.MozillaParser.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.DataOutputBuffer.getData'
'org.apache.mahout.knn.legacy.MrBrute.Map.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.mahout.knn.legacy.MrBrute.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.knn.legacy.MrBrute.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.knn.legacy.MrBrute.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.example.email.MsgIdToDictionaryMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileMergePartitioner.getPartition','org.apache.hadoop.io.IntWritable.get'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileMergeInputFormat.getRecordReader','org.apache.hadoop.mapred.FileInputFormat.getInputPaths org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text>.<init>'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileMergeInputFormat.next','org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.IntWritable.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.set'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileMergeInputFormat.createKey','org.apache.hadoop.io.IntWritable.<init>'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileMergeInputFormat.createValue','org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileMergeInputFormat.createRecordReader','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getInputPaths org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileMergeInputFormat.nextKeyValue','org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.IntWritable.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.set'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.setKeyClass','org.apache.hadoop.conf.Configuration.setClass'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.setComparatorClass','org.apache.hadoop.conf.Configuration.setClass'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.MultiFileInputReader','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.RawValueIterator.hasNext','org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.getNextItemIterator','org.apache.hadoop.io.DataInputBuffer.reset'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.readNextItem','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.compare','org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.InputSource.InputSource','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.InputSource.next','org.apache.hadoop.io.SequenceFile.ValueBytes.writeUncompressedBytes org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.Writable.readFields'
'org.commoncrawl.util.MultiFileMergeUtils.MultiFileInputReader.InputSource.detachKeyData','org.apache.hadoop.io.DataOutputBuffer.<init>'
'org.commoncrawl.util.MultiFileMergeUtils.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.util.MultiFileMergeUtils.scanToItemThenDisplayNext','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'org.commoncrawl.util.MultiFileMergeUtils.addFirstNFPItemsToSet','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.reset'
'org.apache.accumulo.core.iterators.system.MultiIteratorTest.nr','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.system.MultiIteratorTest.test7','org.apache.hadoop.io.Text.<init>'
'com.datasalt.utils.mapred.joiner.MultiJoinChanneledMapper.emit','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'com.datasalt.utils.mapred.joiner.MultiJoinChanneledMapper.emitBytes','org.apache.hadoop.mapreduce.lib.input.GetInputFileFromTaggedInputSplit.get'
'com.datasalt.utils.mapred.joiner.MultiJoinPair.setMultiJoinGroup','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.set'
'com.datasalt.utils.mapred.joiner.MultiJoinPair.write','org.apache.hadoop.io.BytesWritable.write org.apache.hadoop.io.WritableUtils.writeVInt'
'com.datasalt.utils.mapred.joiner.MultiJoinPair.readFields','org.apache.hadoop.io.BytesWritable.readFields org.apache.hadoop.io.WritableUtils.readVInt'
'com.datasalt.utils.mapred.joiner.MultiJoinPair.equals','org.apache.hadoop.io.BytesWritable.equals'
'com.datasalt.utils.mapred.joiner.MultiJoinPair.compareTo','org.apache.hadoop.io.BytesWritable.compareTo'
'com.datasalt.utils.mapred.joiner.MultiJoinPair.hashCode','org.apache.hadoop.io.BytesWritable.hashCode org.apache.hadoop.io.BytesWritable.hashCode'
'com.datasalt.utils.mapred.joiner.MultiJoinPair.Comparator.compare','org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableComparator.get org.apache.hadoop.io.WritableComparator.compare'
'com.datasalt.utils.mapred.joiner.MultiJoinPair.GroupComparator.compare','org.apache.hadoop.io.WritableComparator.compareBytes'
'com.datasalt.utils.mapred.joiner.MultiJoiner.getJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'com.datasalt.utils.mapred.joiner.MultiJoiner.readStringListFromConfig','org.apache.hadoop.conf.Configuration.get'
'com.datasalt.utils.mapred.joiner.MultiJoiner.addInOrder','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.datasalt.utils.mapred.joiner.MultiJoiner.addConf','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.datasalt.utils.mapred.joiner.MultiJoiner.associatedConf','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.datasalt.utils.mapred.joiner.MultiJoiner.setMultiJoinPairClass','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass'
'com.datasalt.utils.mapred.joiner.MultiJoiner.addInput','org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath'
'com.datasalt.utils.mapred.joiner.MultiJoiner.setChannelDatumClass','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.datasalt.utils.mapred.joiner.MultiJoiner.accept','org.apache.hadoop.fs.Path.getName'
'com.datasalt.utils.mapred.joiner.MultiJoiner.addChanneledInput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.datasalt.utils.mapred.joiner.MultiJoiner.addChanneledInputInner','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.getJobContext','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.getTaskAttemptContext','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.TaskInputOutputContext.write'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.OutputFormat<?,?>.checkOutputSpecs org.apache.hadoop.mapreduce.JobContext.getCredentials org.apache.hadoop.mapreduce.JobContext.getCredentials'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.getOutputFormatInstance','org.apache.hadoop.mapreduce.JobContext.getOutputFormatClass org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.getOutputFormatAliases','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.setAliasConf','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.getRaw org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.addToConfig','org.apache.hadoop.conf.Configuration.set'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.JobConfigurer.addOutputFormat','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.JobConfigurer.configure','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getCredentials org.apache.hadoop.conf.Configuration.set'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.MultiRecordWriter.MultiRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.mapreduce.OutputFormat<?,?>.getRecordWriter'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.MultiRecordWriter.write','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.commitTask','org.apache.hadoop.mapreduce.OutputCommitter.needsTaskCommit org.apache.hadoop.mapreduce.OutputCommitter.commitTask'
'com.mozilla.hadoop.hbase.mapreduce.MultiScanTableMapReduceUtil.initMultiScanTableMapperJob','org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.mozilla.hadoop.hbase.mapreduce.MultiScanTableMapReduceUtil.convertStringToScanArray','org.apache.hadoop.hbase.util.Base64.decode org.apache.hadoop.io.ArrayWritable.<init> org.apache.hadoop.io.ArrayWritable.readFields org.apache.hadoop.io.ArrayWritable.get'
'com.mozilla.hadoop.hbase.mapreduce.MultiScanTableMapReduceUtil.convertScanArrayToString','org.apache.hadoop.io.ArrayWritable.<init> org.apache.hadoop.io.ArrayWritable.write org.apache.hadoop.hbase.util.Base64.encodeBytes'
'com.mozilla.hadoop.hbase.mapreduce.MultiScanTableMapReduceUtil.generateHexPrefixScans','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.setStopRow'
'com.mozilla.hadoop.hbase.mapreduce.MultiScanTableMapReduceUtil.generateBytePrefixScans','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.Scan.setBatch org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Scan.setStopRow'
'com.datasalt.pangool.examples.solr.MultiShakespeareIndexer.CategoryMapper.map','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.toString'
'com.datasalt.pangool.examples.solr.MultiShakespeareIndexer.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.datasalt.pangool.examples.solr.MultiShakespeareIndexer.reduce','org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.examples.solr.MultiShakespeareIndexer.main','org.apache.hadoop.util.ToolRunner.run'
'org.beymani.dist.MultiVariateDistribution.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.beymani.dist.MultiVariateDistribution.HistogramMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.beymani.dist.MultiVariateDistribution.HistogramMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'org.beymani.dist.MultiVariateDistribution.HistogramReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.beymani.dist.MultiVariateDistribution.HistogramReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'org.beymani.dist.MultiVariateDistribution.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.expreval.expr.MultipleExpressionContext.getMapping','org.apache.hadoop.hbase.hbql.mapping.MappingContext.getMapping'
'org.apache.expreval.expr.MultipleExpressionContext.getTableMapping','org.apache.hadoop.hbase.hbql.mapping.MappingContext.getTableMapping'
'org.apache.expreval.expr.MultipleExpressionContext.getResultAccessor','org.apache.hadoop.hbase.hbql.mapping.MappingContext.getResultAccessor'
'org.apache.expreval.expr.MultipleExpressionContext.setMappingContext','org.apache.hadoop.hbase.hbql.mapping.MappingContext.getResultAccessor org.apache.hadoop.hbase.hbql.mapping.HRecordResultAccessor.<init> org.apache.hadoop.hbase.hbql.mapping.MappingContext.setResultAccessor'
'org.apache.expreval.expr.MultipleExpressionContext.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init> org.apache.hadoop.hbase.hbql.util.Lists.newArrayList org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init> org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init> org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'org.apache.expreval.expr.MultipleExpressionContext.addNamedParameter','org.apache.hadoop.hbase.hbql.util.Lists.newArrayList'
'org.apache.crunch.impl.mr.emit.MultipleOutputEmitter.emit','org.apache.hadoop.mapreduce.lib.output.CrunchMultipleOutputs<org.apache.crunch.impl.mr.emit.K,org.apache.crunch.impl.mr.emit.V>.write'
'fm.last.feathers.output.MultipleSequenceFiles.generateActualKey','org.apache.hadoop.typedbytes.TypedBytesWritable.getValue org.apache.hadoop.typedbytes.TypedBytesWritable.setValue'
'fm.last.feathers.output.MultipleSequenceFiles.generateFileNameForKeyValue','org.apache.hadoop.typedbytes.TypedBytesWritable.getValue org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.core.data.Mutation.SimpleReader.readVLong','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.isNegativeVInt'
'org.apache.accumulo.core.data.Mutation.Mutation','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.Mutation.put','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.Mutation.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.accumulo.core.data.Mutation.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt'
'com.nearinfinity.blur.thrift.MutationHelper.getKey','org.apache.hadoop.io.BytesWritable.<init>'
'org.apache.accumulo.server.tabletserver.MutationLog.MutationLog','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.create'
'org.apache.accumulo.server.tabletserver.MutationLog.log','org.apache.hadoop.fs.FSDataOutputStream.writeByte org.apache.hadoop.fs.FSDataOutputStream.flush'
'org.apache.accumulo.server.tabletserver.MutationLog.close','org.apache.hadoop.fs.FSDataOutputStream.writeByte org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.accumulo.server.tabletserver.MutationLog.replay','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readByte'
'org.apache.accumulo.server.tabletserver.MutationLog.next','org.apache.hadoop.fs.FSDataInputStream.readByte'
'org.apache.accumulo.core.data.MutationTest.test1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.MutationTest.test2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.MutationTest.test3','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.MutationTest.nt','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.MutationTest.testPuts','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.MutationTest.testPutsString','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.MutationTest.testPutsStringString','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.MutationTest.test4','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.MutationTest.testNewSerialization','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength'
'com.cloudera.sqoop.manager.MySQLAuthTest.testAuthAccess','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.IOUtils.closeStream'
'com.cloudera.sqoop.manager.MySQLAuthTest.doZeroTimestampTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.sqoop.mapreduce.MySQLDumpImportJob.configureInputFormat','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.mapreduce.Job.setInputFormatClass'
'org.apache.sqoop.mapreduce.MySQLDumpImportJob.configureMapper','org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass'
'org.apache.sqoop.mapreduce.MySQLDumpInputFormat.MySQLDumpRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.sqoop.mapreduce.MySQLExportJob.configureInputFormat','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.manager.MySQLManager.execAndPrint','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.sqoop.manager.MySQLUtils.outputDelimsAreMySQL','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.sqoop.manager.MySQLUtils.writePasswordFile','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'voldemort.contrib.batchindexer.performance.MysqlBuildPerformanceTest.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init> org.apache.hadoop.mapred.SequenceFileRecordReader<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.<init>'
'voldemort.contrib.batchindexer.performance.MysqlBuildPerformanceTest.doOperation','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapred.SequenceFileRecordReader<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.next org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize'
'com.taobao.adfs.database.MysqlServerController.getData','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.database.MysqlServerController.setData','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.database.MysqlServerController.backupData','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getLong'
'com.taobao.adfs.database.MysqlServerController.formatData','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.getBoolean'
'com.taobao.adfs.database.MysqlServerController.createDatabase','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.database.MysqlServerController.startServer','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.database.MysqlServerController.saveMysqlConf','org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.database.MysqlServerController.getMysqlConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.database.MysqlServerController.setMysqlConf','org.apache.hadoop.conf.Configuration.set'
'com.taobao.adfs.database.MysqlServerController.setMysqlDefaultConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.database.MysqlServerController.setMysqlBinPermission','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.database.MysqlServerController.getServerPid','org.apache.hadoop.conf.Configuration.get'
'com.taobao.adfs.database.MysqlServerControllerTest.testFormatAndGetAndSetData','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.ucsc.srl.damasc.netcdf.NCTool.main','org.apache.hadoop.util.ProgramDriver.<init> org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.driver'
'com.cloudera.hadoop.hdfs.nfs.NFSUtils.getDomain','org.apache.hadoop.conf.Configuration.get'
'com.mozilla.grouperfish.transforms.coclustering.lucene.analysis.en.NGramEnglishAnalyzer.main','org.apache.hadoop.fs.Path.<init>'
'com.mozilla.grouperfish.lucene.analysis.en.NGramEnglishAnalyzer.main','org.apache.hadoop.fs.Path.<init>'
'co.nubetech.hiho.similarity.ngram.NGramMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.similarity.ngram.NGramMapper.getNGrams','org.apache.hadoop.io.Text.toString'
'co.nubetech.hiho.similarity.ngram.NGramReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'hadoop.NIMapperReducer.NIMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'hadoop.NIMapperReducer.NIMapper.map','org.apache.hadoop.io.FloatWritable.<init>'
'hadoop.NIMapperReducer.NIReducer.reduce','org.apache.hadoop.io.FloatWritable.toString org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.<init>'
'hadoop.NIMapperReducerCL.NIMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'hadoop.NIMapperReducerCL.NIMapper.map','org.apache.hadoop.io.FloatWritable.<init>'
'hadoop.NIMapperReducerCL.NIReducer.reduce','org.apache.hadoop.io.FloatWritable.toString org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.<init>'
'hadoop.NIMapperReducerCLNamed.NIMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'hadoop.NIMapperReducerCLNamed.NIReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'hadoop.NIMapperReducerCLNamed.NIReducer.reduce','org.apache.hadoop.io.FloatWritable.<init>'
'org.commoncrawl.io.NIODNSSimpleResolverImpl.doTCPClientSendRecv','org.apache.hadoop.util.StringUtils.stringifyException'
'edu.isi.mavuno.nlp.NLProcTools.extractMainChunk','org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.nlp.NLProcTools.extractGeneralizedChunk','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.getLength'
'eu.scape_project.pt.mapred.NLineInputFormat.createRecordReader','org.apache.hadoop.mapreduce.InputSplit.toString org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.mapreduce.lib.input.LineRecordReader.<init>'
'eu.scape_project.pt.mapred.NLineInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'eu.scape_project.pt.mapred.NLineInputFormat.getSplitsForFile','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.util.LineReader.close'
'eu.scape_project.pt.mapred.NLineInputFormat.setNumLinesPerSplit','org.apache.hadoop.mapreduce.Job.getConfiguration'
'eu.scape_project.pt.mapred.NLineInputFormat.getNumLinesPerSplit','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'gov.llnl.ontology.mapreduce.stats.NYTOnelineSectionMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.NYTOnelineSectionMR.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'gov.llnl.ontology.mapreduce.stats.NYTOnelineSectionMR.NYTOnelineSectionMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.datasalt.pangool.examples.naivebayes.NaiveBayesGenerate.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.examples.naivebayes.NaiveBayesGenerate.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.datasalt.pangool.examples.naivebayes.NaiveBayesGenerate.reduce','org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.examples.naivebayes.NaiveBayesGenerate.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.classifier.naivebayes.NaiveBayesModel.materialize','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readFloat'
'org.apache.mahout.classifier.naivebayes.NaiveBayesModel.serialize','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeFloat'
'org.apache.mahout.classifier.naivebayes.NaiveBayesTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.naivebayes.NaiveBayesTest.toyData','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.naivebayes.NaiveBayesTest.toyDataComplementary','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.naivebayes.trainer.NaiveBayesThetaComplementaryMapper.map','org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.classifier.naivebayes.trainer.NaiveBayesThetaComplementaryMapper.setup','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.naivebayes.trainer.NaiveBayesThetaComplementaryMapper.cleanup','org.apache.hadoop.io.Text.<init>'
'com.bah.culvert.data.index.NaiveIndexTest.setup','org.apache.hadoop.conf.Configuration.<init>'
'org.fusesource.fabric.hadoop.hdfs.NameNodeFactory.doCreate','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs org.apache.hadoop.hdfs.server.namenode.NameNode.format org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode'
'org.fusesource.fabric.hadoop.hdfs.NameNodeFactory.doDelete','org.apache.hadoop.hdfs.server.namenode.NameNode.stop'
'org.apache.sqoop.io.NamedFifo.create','org.apache.hadoop.util.Shell.execCommand org.apache.hadoop.util.Shell.execCommand'
'org.apache.expreval.expr.var.NamedParameter.validateTypes','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init> org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init> org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init> org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init> org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'org.apache.expreval.expr.var.NamedParameter.setParameter','org.apache.hadoop.hbase.hbql.util.Lists.newArrayList'
'org.apache.expreval.expr.var.NamedParameter.getValueExpr','org.apache.hadoop.hbase.hbql.impl.InvalidTypeException.<init>'
'org.apache.expreval.expr.var.NamedParameter.getFilter','org.apache.hadoop.hbase.hbql.impl.InvalidServerFilterException.<init>'
'org.pingles.cascading.cassandra.NarrowRowScheme.sink','org.apache.hadoop.mapred.OutputCollector.collect'
'nl.vu.datalayer.hbase.connection.NativeJavaConnection.NativeJavaConnection','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTablePool.<init> org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'nl.vu.datalayer.hbase.connection.NativeJavaConnection.initTables','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTable.getRegionsInfo org.apache.hadoop.hbase.client.HTable.prewarmRegionCache org.apache.hadoop.hbase.client.HTable.close'
'nl.vu.datalayer.hbase.connection.NativeJavaConnection.getTable','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTablePool.getTable'
'nl.vu.datalayer.hbase.connection.NativeJavaConnection.close','org.apache.hadoop.hbase.client.HBaseAdmin.close org.apache.hadoop.hbase.client.HTablePool.close'
'org.apache.accumulo.server.test.NativeMapConcurrencyTest.nm','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.NativeMapConcurrencyTest.pc','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.NativeMapStressTest.put','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.NativeMapStressTest.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'v3.NcdcRecordParser.parse','org.apache.hadoop.io.Text.toString'
'v5.NcdcRecordParser.parse','org.apache.hadoop.io.Text.toString'
'mia.clustering.ch10.NewsKMeansClustering.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'cc.bf.NgramsToBloomFilter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'cc.bf.NgramsToBloomFilter.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'cc.bf.NgramsToBloomFilter.BFBase.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.util.bloom.BloomFilter.<init>'
'cc.bf.NgramsToBloomFilter.BFBase.close','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.NullWritable,org.apache.hadoop.util.bloom.BloomFilter>.collect org.apache.hadoop.util.bloom.Key.<init> org.apache.hadoop.util.bloom.BloomFilter.membershipTest'
'cc.bf.NgramsToBloomFilter.BFMapper.map','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.util.bloom.Key.<init>'
'co.nubetech.hiho.mapreduce.lib.output.NoKeyOnlyValueOutputFormat.NoKeyRecordWriter.writeObject','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'co.nubetech.hiho.mapreduce.lib.output.NoKeyOnlyValueOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'tv.floe.caduceus.hadoop.movingaverage.NoShuffleSort_MovingAverageJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setCompressMapOutput org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'tv.floe.caduceus.hadoop.movingaverage.NoShuffleSort_MovingAverageJob.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'tv.floe.caduceus.hadoop.movingaverage.NoShuffleSort_MovingAverageJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'tv.floe.caduceus.hadoop.movingaverage.NoShuffleSort_MovingAverageReducer.reduce','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.NodeBuilderCommonFurious','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.computeInitialDistribution','org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.readParentDistribution','org.apache.hadoop.fs.Path.<init>'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.setupJob0','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.readCandidatesQuestionsFile','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.util.LineReader.close'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.mapReduceSelectBestQuestion','org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.readBestQuestion','org.apache.hadoop.fs.Path.<init>'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.setupJob2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.getSplitPath','org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.mapred.RunningJob.getJobName org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'fr.insarennes.fafdti.builder.nodebuilder.NodeBuilderCommonFurious.setupJob3','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'org.apache.nutch.scoring.webgraph.NodeDumper.Sorter.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getLong'
'org.apache.nutch.scoring.webgraph.NodeDumper.Sorter.map','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.FloatWritable,org.apache.hadoop.io.Text>.collect'
'org.apache.nutch.scoring.webgraph.NodeDumper.Sorter.reduce','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect'
'org.apache.nutch.scoring.webgraph.NodeDumper.Dumper.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.scoring.webgraph.NodeDumper.Dumper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect'
'org.apache.nutch.scoring.webgraph.NodeDumper.Dumper.reduce','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.FloatWritable>.collect'
'org.apache.nutch.scoring.webgraph.NodeDumper.dumpNodes','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.scoring.webgraph.NodeDumper.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.scoring.webgraph.NodeDumper.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'fr.eurecom.dsg.mapreduce.pagerank.utils.NodeInputFormat.AdjacencyListRecordReader.AdjacencyListRecordReader','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.<init>'
'fr.eurecom.dsg.mapreduce.pagerank.utils.NodeInputFormat.AdjacencyListRecordReader.close','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'fr.eurecom.dsg.mapreduce.pagerank.utils.NodeInputFormat.AdjacencyListRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue org.apache.hadoop.io.LongWritable.<init>'
'fr.eurecom.dsg.mapreduce.pagerank.utils.NodeInputFormat.AdjacencyListRecordReader.getCurrentValue','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue'
'fr.eurecom.dsg.mapreduce.pagerank.utils.NodeInputFormat.AdjacencyListRecordReader.getProgress','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress'
'fr.eurecom.dsg.mapreduce.pagerank.utils.NodeInputFormat.AdjacencyListRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize'
'fr.eurecom.dsg.mapreduce.pagerank.utils.NodeInputFormat.AdjacencyListRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue'
'org.apache.nutch.scoring.webgraph.NodeReader.dumpUrl','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getReaders org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.lib.HashPartitioner<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node>.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getEntry'
'org.apache.nutch.scoring.webgraph.NodeReader.main','org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.scoring.webgraph.NodeReader.dumpUrl','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getReaders org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.lib.HashPartitioner<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node>.<init> org.apache.hadoop.mapred.MapFileOutputFormat.getEntry'
'org.apache.nutch.scoring.webgraph.NodeReader.main','org.apache.hadoop.fs.Path.<init>'
'nl.erdf.datalayer.hbase.NodeSerializer.fromBytes','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readByte org.apache.hadoop.io.DataInputBuffer.readUTF org.apache.hadoop.io.DataInputBuffer.readUTF org.apache.hadoop.io.DataInputBuffer.readUTF org.apache.hadoop.io.DataInputBuffer.close'
'nl.erdf.datalayer.hbase.NodeSerializer.toBytes','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.writeUTF org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'hadooptree.job.NodeSplitsJob.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'hadooptree.job.NodeSplitsJob.Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.apache.giraph.examples.NormalizingLongDoubleFloatDoubleTextInputFormat.NormalizingLongDoubleFloatDoubleVertexReader.getCurrentVertex','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.NormalizingLongDoubleFloatDoubleTextInputFormat.NormalizingLongDoubleFloatDoubleVertexReader.parse','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.examples.NormalizingLongDoubleFloatDoubleTextInputFormat.NormalizingLongDoubleFloatDoubleVertexReader.normalize','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.mapred.NullInputFormat.NullRecordReader.createKey','org.apache.hadoop.io.NullWritable.get'
'edu.umd.cloud9.mapred.NullInputFormat.NullRecordReader.createValue','org.apache.hadoop.io.NullWritable.get'
'edu.umd.cloud9.mapred.NullMapper.HeartbeatRunnable.run','org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.hcatalog.templeton.tool.NullRecordReader.getCurrentKey','org.apache.hadoop.io.NullWritable.get'
'org.apache.hcatalog.templeton.tool.NullRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'edu.ucsc.srl.damasc.netcdf.reduce.NullReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'backtype.cascading.tap.NullTapTest.testNullTap','org.apache.hadoop.mapred.JobConf.<init>'
'org.pentaho.hadoop.mapreduce.converter.converters.NullWritableConverter.canConvert','org.apache.hadoop.io.NullWritable.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.NullWritableConverter.convert','org.apache.hadoop.io.NullWritable.get'
'org.apache.pig.impl.io.NullableBooleanWritable.NullableBooleanWritable','org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.<init>'
'org.apache.pig.impl.io.NullableBooleanWritable.getValueAsPigType','org.apache.hadoop.io.BooleanWritable.get'
'org.apache.pig.impl.io.NullableLongWritable.NullableLongWritable','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.pig.impl.io.NullableLongWritable.getValueAsPigType','org.apache.hadoop.io.LongWritable.get'
'org.chombo.mr.NumericSorter.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.chombo.mr.NumericSorter.NumericSorterMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.set'
'org.chombo.mr.NumericSorter.NumericSorterReducer.reduce','org.apache.hadoop.io.NullWritable.get'
'org.chombo.mr.NumericSorter.main','org.apache.hadoop.util.ToolRunner.run'
'hadoop.NumericalIntegration.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.GenericOptionsParser.getConfiguration org.apache.hadoop.util.ToolRunner.run'
'hadoop.NumericalIntegration.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getTrackingURL'
'hadoop.NumericalIntegrationNamed.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.GenericOptionsParser.getConfiguration org.apache.hadoop.util.ToolRunner.run'
'hadoop.NumericalIntegrationNamed.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getTrackingURL'
'org.apache.nutch.searcher.NutchBean.NutchBean','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.searcher.NutchBean.readConfig','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open'
'org.apache.nutch.searcher.NutchBean.search','org.apache.hadoop.conf.Configuration.getFloat'
'org.apache.nutch.searcher.NutchBean.close','org.apache.hadoop.fs.FileSystem.close'
'org.apache.nutch.searcher.NutchBean.NutchBeanConstructor.contextDestroyed','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.searcher.NutchBean.NutchBeanConstructor.contextInitialized','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.util.NutchConfiguration.create','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.nutch.util.NutchConfiguration.createCrawlConfiguration','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.nutch.util.NutchConfiguration.get','org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.util.NutchConfiguration.addNutchResources','org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource'
'org.apache.nutch.indexer.NutchDocument.readFields','org.apache.hadoop.io.VersionMismatchException.<init> org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Text.readString org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Text.readString'
'org.apache.nutch.indexer.NutchDocument.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.indexer.NutchIndexWriterFactory.getNutchIndexWriters','org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.nutch.indexer.NutchIndexWriterFactory.addClassToConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'babel.prep.extract.NutchPageExtractor.main','org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.FileOutputFormat.getOutputPath'
'babel.prep.extract.NutchPageExtractor.createJobConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'org.wyona.yanel.impl.resources.NutchResource.getNutchConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addDefaultResource org.apache.hadoop.conf.Configuration.addFinalResource'
'org.wyona.yanel.impl.resources.NutchResource.getSearchPageAsDOM','org.apache.hadoop.conf.Configuration.get'
'org.wyona.yanel.impl.resources.NutchResource.loadOntology','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.digitalpebble.behemoth.io.nutch.NutchSegmentConverterJob.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,com.digitalpebble.behemoth.BehemothDocument>.collect'
'com.digitalpebble.behemoth.io.nutch.NutchSegmentConverterJob.convert','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'com.digitalpebble.behemoth.io.nutch.NutchSegmentConverterJob.main','org.apache.hadoop.util.ToolRunner.run'
'com.digitalpebble.behemoth.io.nutch.NutchSegmentConverterJob.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.scoring.opic.OPICScoringFilter.setConf','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.scoring.opic.OPICScoringFilter.distributeScoreToOutlinks','org.apache.hadoop.io.Text.toString'
'com.asakusafw.dmdl.java.emitter.driver.ObjectDriverTest.simple_record','org.apache.hadoop.io.Text.<init>'
'org.apache.yaoql.impl.ObjectQueryImpl.addListener','org.apache.hadoop.hbase.hbql.util.Lists.newArrayList'
'org.apache.yaoql.impl.ObjectQueryImpl.getExpressionTree','org.apache.hadoop.hbase.hbql.mapping.MappingContext.<init> org.apache.hadoop.hbase.hbql.parser.ParserUtil.parseWhereExpression'
'org.apache.yaoql.impl.ObjectQueryImpl.getResultList','org.apache.hadoop.hbase.hbql.util.Lists.newArrayList'
'org.apache.yaoql.client.ObjectQueryPredicate.apply','org.apache.hadoop.hbase.hbql.mapping.MappingContext.<init> org.apache.hadoop.hbase.hbql.parser.ParserUtil.parseWhereExpression org.apache.hadoop.hbase.hbql.client.HBqlException.printStackTrace'
'org.pentaho.hadoop.mapreduce.converter.converters.ObjectToStringConverterTest.convert','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'com.taobao.adfs.distributed.rpc.ObjectWritable.writeObject','org.apache.hadoop.io.Writable.isAssignableFrom org.apache.hadoop.io.Writable.write'
'com.taobao.adfs.distributed.rpc.ObjectWritable.readObject','org.apache.hadoop.io.Writable.isAssignableFrom org.apache.hadoop.io.Writable.readFields'
'com.taobao.adfs.distributed.rpc.ObjectWritable.isWritable','org.apache.hadoop.io.Writable.isAssignableFrom'
'org.apache.accumulo.core.client.impl.OfflineIterator.OfflineIterator','org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.client.impl.OfflineIterator.nextTablet','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.impl.OfflineScanner.OfflineScanner','org.apache.hadoop.io.Text.<init>'
'org.okuyama.hadoop.format.OkuyamaRecordReader.getCurrentValue','org.apache.hadoop.io.Text.<init>'
'org.okuyama.hadoop.format.OkuyamaRecordReader.initialize','org.apache.hadoop.mapreduce.InputSplit.getLocations org.apache.hadoop.mapreduce.InputSplit.getTag'
'org.apache.nutch.fetcher.OldFetcher.InputFormat.getSplits','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init>'
'org.apache.nutch.fetcher.OldFetcher.FetcherThread.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.next org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.fetcher.OldFetcher.FetcherThread.handleRedirect','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.fetcher.OldFetcher.FetcherThread.output','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.io.Text.equals org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect'
'org.apache.nutch.fetcher.OldFetcher.reportStatus','org.apache.hadoop.mapred.Reporter.setStatus'
'org.apache.nutch.fetcher.OldFetcher.configure','org.apache.hadoop.mapred.JobConf.get'
'org.apache.nutch.fetcher.OldFetcher.isParsing','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.fetcher.OldFetcher.isStoringContent','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.nutch.fetcher.OldFetcher.fetch','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.fetcher.OldFetcher.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.fetcher.OldFetcher.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.accumulo.core.data.OldMutation.OldMutation','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.data.OldMutation.put','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'fr.insarennes.fafdti.hadoop.OldStep2Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'thiswillbereplaced.OldWordCount.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'thiswillbereplaced.OldWordCount.Reduce.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'thiswillbereplaced.OldWordCount.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'com.cloudera.hbase.OldWordCountTest.setUp','org.apache.hadoop.mrunit.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init>'
'com.cloudera.hbase.OldWordCountTest.testMapper','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.cloudera.hbase.OldWordCountTest.testReducer','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.cloudera.hbase.OldWordCountTest.testMapReduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.yahoo.omid.OmidTestBase.setupOmid','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.LocalHBaseCluster.<init> org.apache.hadoop.hbase.LocalHBaseCluster.startup org.apache.hadoop.hbase.LocalHBaseCluster.getActiveMaster org.apache.hadoop.hbase.LocalHBaseCluster.getActiveMaster'
'com.yahoo.omid.OmidTestBase.teardownOmid','org.apache.hadoop.hbase.LocalHBaseCluster.shutdown org.apache.hadoop.hbase.LocalHBaseCluster.join'
'com.yahoo.omid.OmidTestBase.setUp','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.isTableDisabled org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.client.HBaseAdmin.listTables org.apache.hadoop.hbase.HTableDescriptor.getNameAsString'
'com.yahoo.omid.OmidTestBase.tearDown','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable'
'com.yahoo.omid.OmidTestBase.dumpTable','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.next'
'com.yahoo.omid.OmidTestBase.verifyValue','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getColumnLatest org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'com.tgam.hadoop.mapreduce.OmnitureDataFileInputFormat.isSplitable','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'com.tgam.hadoop.mapred.OmnitureDataFileRecordReader.OmnitureDataFileRecordReader','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.Text.<init> org.apache.hadoop.conf.Configuration.getInt'
'com.tgam.hadoop.mapred.OmnitureDataFileRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'com.tgam.hadoop.mapred.OmnitureDataFileRecordReader.createValue','org.apache.hadoop.io.Text.<init>'
'com.tgam.hadoop.mapred.OmnitureDataFileRecordReader.next','org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'.OmnitureTextLoaderTester.MockRecordReader.getCurrentValue','org.apache.hadoop.io.Text.<init>'
'.OnTimeArrivalDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setOutputValueGroupingComparator org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'.OnTimeArrivalDriver.main','org.apache.hadoop.util.ToolRunner.run'
'.OnTimeArrivalMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'.OnTimeArrivalPartitioner.getPartition','org.apache.hadoop.io.Text.toString'
'com.sematext.hbase.wd.OneByteSimpleHashTest.testDistribution','org.apache.hadoop.hbase.util.Bytes.toBytes'
'gov.llnl.ontology.mapreduce.ingest.OneLinePerDocExtractorMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.ingest.OneLinePerDocExtractorMR.setupConfiguration','org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.conf.Configuration.set'
'gov.llnl.ontology.mapreduce.ingest.OneLinePerDocExtractorMR.setupReducer','org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath'
'gov.llnl.ontology.mapreduce.ingest.OneLinePerDocExtractorMR.OneLinePerDocExtractorMRMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'gov.llnl.ontology.mapreduce.ingest.OneLinePerDocExtractorMR.OneLinePerDocExtractorMRMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.inadco.hbl.math.aggregators.OnlineIrregularSumCombiner.reduce','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.nutch.ontology.jena.OntologyImpl.main','org.apache.hadoop.conf.Configuration.get'
'org.apache.ivory.converter.OozieFeedMapper.getRetentionWorkflowAction','org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.converter.OozieFeedMapper.createAndGetCoord','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.converter.OozieFeedMapper.getReplicationWorkflowAction','org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.workflow.engine.OozieHouseKeepingService.afterDelete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.ivory.converter.OozieProcessMapperLateProcessTest.setUpDFS','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.conf.Configuration.get'
'org.apache.ivory.converter.OozieProcessMapperLateProcessTest.tearDown','org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'org.apache.ivory.converter.OozieProcessMapperTest.setUpDFS','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.conf.Configuration.get'
'org.apache.ivory.converter.OozieProcessMapperTest.setUp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.ivory.converter.OozieProcessMapperTest.testBundle','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.converter.OozieProcessMapperTest.testBundle1','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.converter.OozieProcessMapperTest.getParentWorkflow','org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.converter.OozieProcessMapperTest.getBundle','org.apache.hadoop.fs.Path.<init>'
'org.apache.ivory.converter.OozieProcessMapperTest.readFile','org.apache.hadoop.fs.FileSystem.open'
'org.apache.ivory.workflow.OozieWorkflowBuilder.createAppProperties','org.apache.hadoop.fs.Path.toString'
'ivory.core.tokenize.OpenNLPTokenizer.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'ivory.core.tokenize.OpenNLPTokenizer.setTokenizer','org.apache.hadoop.fs.FileSystem.open'
'info.chodakowski.opentsdb.OpenTSDBUtils.purgeAllData','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.close'
'info.chodakowski.opentsdb.OpenTSDBUtils.purgeTable','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'com.asakusafw.testdriver.OperatorTestEnvironment.before','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.asakusafw.testdriver.OperatorTestEnvironment.createConfig','org.apache.hadoop.conf.Configuration.addResource'
'org.apache.accumulo.core.util.shell.commands.OptUtil.getStartRow','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.util.shell.commands.OptUtil.getEndRow','org.apache.hadoop.io.Text.<init>'
'edu.duke.starfish.jobopt.OptimizedJobManager.OptimizedJobManager','org.apache.hadoop.conf.Configuration.<init>'
'org.commoncrawl.hadoop.mergeutils.OptimizedKeyGeneratorAndComparator.OptimizedKey.getBufferKeyValueStream','org.apache.hadoop.io.DataOutputBuffer.reset'
'org.commoncrawl.hadoop.mergeutils.OptimizedKeyGeneratorAndComparator.OptimizedKey.initFromKeyValuePair','org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.accumulo.core.iterators.OrIterator.TermSource.TermSource','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.sqoop.mapreduce.db.OracleDBRecordReader.setSessionTimeZone','org.apache.hadoop.conf.Configuration.get'
'com.cloudera.sqoop.manager.OracleExportTest.setUp','org.apache.hadoop.util.StringUtils.stringifyException'
'org.goldenorb.conf.OrbConfiguration.addOrbResources','org.apache.hadoop.conf.Configuration.addDefaultResource org.apache.hadoop.conf.Configuration.addDefaultResource'
'org.goldenorb.conf.OrbConfiguration.getHDFSdistributedFiles','org.apache.hadoop.fs.Path.<init>'
'org.goldenorb.jet.OrbPartitionMember.initProxy','org.apache.hadoop.ipc.RPC.waitForProxy'
'org.goldenorb.jet.OrbPartitionMember.readFields','org.apache.hadoop.io.Text.readString'
'org.goldenorb.jet.OrbPartitionMember.write','org.apache.hadoop.io.Text.writeString'
'org.goldenorb.OrbPartitionProcess.getLocalFilesPath','org.apache.hadoop.fs.Path.toString'
'org.goldenorb.OrbTracker.run','org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.ipc.RPC.Server.start'
'org.goldenorb.OrbTracker.getRequiredFiles','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.Path.toString'
'fr.eurecom.dsg.mapreduce.OrderInversion.GroupComparator.compare','org.apache.hadoop.io.WritableComparator.get'
'fr.eurecom.dsg.mapreduce.OrderInversion.PairMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'fr.eurecom.dsg.mapreduce.OrderInversion.PairReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'fr.eurecom.dsg.mapreduce.OrderInversion.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.eurecom.dsg.mapreduce.OrderInversion.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'fr.eurecom.dsg.mapreduce.OrderInversion.run','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.eurecom.dsg.mapreduce.OrderInversion.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.inmobi.databus.utils.OrderlyCreationOfDirs.doRecursiveListing','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.utils.OrderlyCreationOfDirs.listingAndValidation','org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.inmobi.databus.utils.OrderlyCreationOfDirs.getStreamNames','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.inmobi.databus.utils.OrderlyCreationOfDirs.pathConstruction','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus'
'com.mozilla.grouperfish.mahout.clustering.display.lda.OriginalText.getDocIds','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'com.mozilla.grouperfish.mahout.clustering.display.lda.OriginalText.main','org.apache.hadoop.fs.Path.<init>'
'org.goldenorb.queue.OutboundVertexQueueTest.testOutBoundVertexQueue','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.nutch.parse.Outlink.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.apache.nutch.parse.Outlink.skip','org.apache.hadoop.io.Text.skip org.apache.hadoop.io.Text.skip'
'org.apache.nutch.parse.Outlink.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'com.rapleaf.hank.hadoop.OutputCollectorWriter.write','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapred.OutputCollector<com.rapleaf.hank.hadoop.KeyAndPartitionWritable,com.rapleaf.hank.hadoop.ValueWritable>.collect org.apache.hadoop.mapred.Reporter.progress'
'org.apache.pig.impl.streaming.OutputHandler.bindTo','org.apache.hadoop.util.LineReader.<init>'
'org.apache.pig.impl.streaming.OutputHandler.getNext','org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'edu.jhu.thrax.hadoop.jobs.OutputJob.getJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'com.sap.hadoop.metadata.OutputKeyComparator.configure','org.apache.hadoop.io.WritableComparator.get'
'com.sap.hadoop.metadata.OutputKeyComparator.compare','org.apache.hadoop.io.WritableComparator.compare'
'edu.jhu.thrax.hadoop.output.OutputReducer.setup','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get'
'edu.jhu.thrax.hadoop.output.OutputReducer.reduce','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.equals'
'edu.jhu.thrax.hadoop.output.OutputReducer.cleanup','org.apache.hadoop.io.NullWritable.get'
'org.archive.hadoop.mapreduce.OvercrawlZipNumRecordWriter.write','org.apache.hadoop.io.Text.toString'
'p3.ip.analyzer.P3CoralProgram.P3CoralProgram','org.apache.hadoop.mapred.JobConf.<init>'
'p3.ip.analyzer.P3CoralProgram.Map_CountUp.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt'
'p3.ip.analyzer.P3CoralProgram.Map_CountUp.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'p3.ip.analyzer.P3CoralProgram.Reduce_CountUp.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'p3.ip.analyzer.P3CoralProgram.getCountUpJobConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'p3.ip.analyzer.P3CoralProgram.startCount','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob'
'p3.ip.analyzer.P3CoralProgram.Map_Stats1.configure','org.apache.hadoop.mapred.JobConf.getInt'
'p3.ip.analyzer.P3CoralProgram.Map_Stats1.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect'
'p3.ip.analyzer.P3CoralProgram.Reduce_Stats1.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect'
'p3.ip.analyzer.P3CoralProgram.Map_Stats2.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect'
'p3.ip.analyzer.P3CoralProgram.Reduce_Stats2.reduce','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect'
'p3.ip.analyzer.P3CoralProgram.getStatsGenJobConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'p3.ip.analyzer.P3CoralProgram.getStatsReduceJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'p3.ip.analyzer.P3CoralProgram.startStats','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob'
'p3.ip.analyzer.P3CoralProgram.Map_Rate.configure','org.apache.hadoop.mapred.JobConf.getInt'
'p3.ip.analyzer.P3CoralProgram.Map_Rate.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'p3.ip.analyzer.P3CoralProgram.Reduce_Rate.configure','org.apache.hadoop.mapred.JobConf.getInt'
'p3.ip.analyzer.P3CoralProgram.Reduce_Rate.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'p3.ip.analyzer.P3CoralProgram.getRateGenJobConf','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'p3.ip.analyzer.P3CoralProgram.startRate','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob'
'p3.ip.analyzer.P3CoralProgram.Map_FlowGen.configure','org.apache.hadoop.mapred.JobConf.getInt'
'p3.ip.analyzer.P3CoralProgram.Map_FlowGen.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect'
'p3.ip.analyzer.P3CoralProgram.Reduce_FlowGen.reduce','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect'
'p3.ip.analyzer.P3CoralProgram.getFlowGenJobConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'p3.ip.analyzer.P3CoralProgram.Map_FlowRate.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.collect'
'p3.ip.analyzer.P3CoralProgram.Reduce_FlowRate.configure','org.apache.hadoop.mapred.JobConf.getInt'
'p3.ip.analyzer.P3CoralProgram.Reduce_FlowRate.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.collect'
'p3.ip.analyzer.P3CoralProgram.getFlowStatsJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'p3.ip.analyzer.P3CoralProgram.startFlowStats','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob'
'p3.ip.analyzer.P3CoralProgram.Map_TopN.configure','org.apache.hadoop.mapred.JobConf.getInt'
'p3.ip.analyzer.P3CoralProgram.Map_TopN.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'p3.ip.analyzer.P3CoralProgram.Reduce_TopN.configure','org.apache.hadoop.mapred.JobConf.getLong'
'p3.ip.analyzer.P3CoralProgram.Reduce_TopN.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'p3.ip.analyzer.P3CoralProgram.getTopNJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'p3.ip.analyzer.P3CoralProgram.PcapMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.BytesWritable>.collect'
'p3.ip.analyzer.P3CoralProgram.getTestConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'p3.ip.analyzer.P3CoralProgram.startTest','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob'
'p3.ip.analyzer.P3CoralProgram.getRealTestConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass'
'p3.ip.analyzer.P3CoralProgram.startRealTest','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.mahout.fpm.pfpgrowth.PFPGrowthRetailDataTest2.testRetailDataMinSup100InSteps','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.fpm.pfpgrowth.PFPGrowthTest.testStartParallelFPGrowthInSteps','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.fpm.pfpgrowth.PFPGrowthTest2.testStartParallelFPGrowthInSteps','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.fpm.pfpgrowth.PFPGrowthTest.testStartParallelFPGrowth','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.sqoop.mapreduce.PGBulkloadExportJob.configureInputFormat','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.sqoop.mapreduce.PGBulkloadExportJob.setDelimiter','org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.mapreduce.PGBulkloadExportJob.propagateOptionsToJob','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setIfUnset org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setIfUnset org.apache.hadoop.conf.Configuration.setIfUnset org.apache.hadoop.conf.Configuration.setIfUnset org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setIfUnset org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.sqoop.mapreduce.PGBulkloadExportJob.runExport','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass'
'org.apache.crunch.impl.mr.collect.PGroupedTableImpl.configureShuffle','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'edu.isi.mavuno.score.PMIScorer.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.seekInRightStream','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNext','org.apache.hadoop.mapreduce.Counter.increment'
'edu.umd.hooka.PServerClient.get','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set'
'org.apache.crunch.util.PTypes.ProtoInputMapFn.initialize','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.crunch.util.PTypes.ThriftInputMapFn.initialize','org.apache.hadoop.util.ReflectionUtils.newInstance'
'edu.umd.cloud9.io.PackTextFile.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.set'
'p3.pcap.examples.PacketCount.Map.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.PacketCount.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.PacketCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.waitForCompletion'
'p3.pcap.examples.PacketMixer.Map.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.PacketMixer.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.PacketMixer.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'p3.tcphttp.analyzer.lib.PacketStatsWritable.compareTo','org.apache.hadoop.record.Buffer.hashCode org.apache.hadoop.record.Buffer.hashCode'
'p3.tcphttp.analyzer.lib.PacketStatsWritable.deserialize','org.apache.hadoop.record.meta.Utils.skip'
'p3.tcphttp.analyzer.lib.PacketStatsWritable.PacketStatsWritable','org.apache.hadoop.record.Buffer.<init> org.apache.hadoop.record.Buffer.<init> org.apache.hadoop.record.Buffer.<init>'
'p3.tcphttp.analyzer.lib.PacketStatsWritable.parse','org.apache.hadoop.record.Buffer.set org.apache.hadoop.record.Buffer.set'
'babel.content.pages.Page.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.WritableUtils.readVInt'
'babel.content.pages.Page.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.WritableUtils.writeVInt'
'babel.prep.extract.PageExtMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'babel.prep.extract.PageExtMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,babel.prep.extract.NutchChunk>.collect'
'babel.prep.extract.PageExtReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,babel.content.pages.Page>.collect'
'filters.PageFilterExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.filter.PageFilter.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.ResultScanner.close'
'babel.prep.merge.PageMergeReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,babel.content.pages.Page>.collect'
'org.apache.giraph.benchmark.PageRankComputation.computePageRank','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'skywriting.examples.skyhout.pagerank.PageRankInitMergeTask.PageRankAdjacencyListReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,skywriting.examples.skyhout.common.IntArrayWritable>.collect'
'skywriting.examples.skyhout.pagerank.PageRankInitTask.PageRankInitMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.collect'
'skywriting.examples.skyhout.pagerank.PageRankInitTask.invoke','org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.<init>'
'skywriting.examples.skyhout.pagerank.PageRankInitialScoreTask.PageRankInitialScoreMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.collect'
'skywriting.examples.skyhout.pagerank.PageRankSortMapTask.InverseMapper.map','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.IntWritable>.collect'
'skywriting.examples.skyhout.pagerank.PageRankSortMapTask.SetCombiner.combine','org.apache.hadoop.io.IntWritable.get'
'skywriting.examples.skyhout.pagerank.PageRankSortMapTask.SetCombiner.combineInit','org.apache.hadoop.io.IntWritable.get'
'skywriting.examples.skyhout.pagerank.PageRankSortMapTask.invoke','org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.DoubleWritable,org.apache.hadoop.io.IntWritable>.<init>'
'skywriting.examples.skyhout.pagerank.PageRankZipTask.PageRankZipper.zip','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.collect'
'skywriting.examples.skyhout.pagerank.PageRankZipTask.invoke','org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.<init>'
'fr.eurecom.dsg.mapreduce.pagerank.PageRank.validateAndParseArgs','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.close'
'fr.eurecom.dsg.mapreduce.pagerank.PageRank.getPartFiles','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'fr.eurecom.dsg.mapreduce.pagerank.PageRank.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readFloat org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'fr.eurecom.dsg.mapreduce.pagerank.PageRank.isConverged','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readBoolean org.apache.hadoop.fs.FSDataInputStream.close'
'fr.eurecom.dsg.mapreduce.pagerank.PageRank.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hama.examples.PageRank.PageRankVertex.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.examples.PageRank.PageRankVertex.compute','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.hama.examples.PageRank.PagerankTextReader.parseVertex','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.hama.examples.PageRank.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'babel.content.pages.PageVersion.PageVersion','org.apache.hadoop.io.Writable.getClass'
'babel.content.pages.PageVersion.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Text.readString'
'babel.content.pages.PageVersion.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.Text.writeString'
'backtype.hadoop.pail.PailFormatTester.PailFormatTester','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'backtype.hadoop.pail.PailFormatTester.testInputFormat','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.InputFormat.getSplits org.apache.hadoop.mapred.InputFormat.getRecordReader org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.next org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.close'
'backtype.hadoop.pail.PailInputSplit.PailInputSplit','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getLocations'
'backtype.hadoop.pail.PailInputSplit.setRelPath','org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.equals org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.pail.PailInputSplit.write','org.apache.hadoop.io.WritableUtils.writeString'
'backtype.hadoop.pail.PailInputSplit.readFields','org.apache.hadoop.io.WritableUtils.readString'
'backtype.hadoop.pail.PailOpsTest.renameAndStructureAppendTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent'
'backtype.hadoop.pail.PailSpec.readFromFileSystem','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'backtype.hadoop.pail.PailSpec.writeToFileSystem','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'backtype.hadoop.pail.PailSpec.write','org.apache.hadoop.io.WritableUtils.writeString'
'backtype.hadoop.pail.PailSpec.readFields','org.apache.hadoop.io.WritableUtils.readString'
'com.asakusafw.compiler.windgate.testing.jdbc.PairJdbcSupport.ResultSetSupport.next','org.apache.hadoop.io.Text.set'
'ivory.lsh.PairOfIntSignatureTest.testReadWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.io.pair.PairOfWritablesTest.testBasic','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'edu.umd.cloud9.io.pair.PairOfWritablesTest.testSerialize','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.getNamedOutputsList','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.getNamedOutputFormatInstanceFile','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.getNamedOutputKeyClass','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.getNamedOutputValueClass','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.addNamedOutput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.addNamedOutputContext','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.setSpecificNamedOutputContext','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.setCountersEnabled','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.getCountersEnabled','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.RecordWriterWithCounter.write','org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter org.apache.hadoop.mapreduce.RecordWriter.write'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.RecordWriterWithCounter.close','org.apache.hadoop.mapreduce.RecordWriter.close'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.write','org.apache.hadoop.mapreduce.RecordWriter.write'
'com.datasalt.pangool.tuplemr.mapred.lib.output.PangoolMultipleOutputs.getRecordWriter','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.OutputFormat.getOutputCommitter org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.JobContext.<init> org.apache.hadoop.mapreduce.OutputFormat.checkOutputSpecs org.apache.hadoop.mapreduce.OutputFormat.getOutputCommitter org.apache.hadoop.mapreduce.OutputFormat.getRecordWriter'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.run','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.initializeM','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.ItemRatingVectorsMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.runSolver','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.SolveExplicitFeedbackMapper.setup','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.SolveImplicitFeedbackMapper.setup','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.AverageRatingMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.setUp','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.completeJobToyExample','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.completeJobImplicitToyExample','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.prefsToRatingsMapper','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.prefsToRatingsMapperTranspose','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.initializeMReducer','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.itemIDRatingMapper','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.completeJobToyExample','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.fpm.pfpgrowth.ParallelCountingMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.fpm.pfpgrowth.ParallelCountingReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set'
'org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthMapper.map','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthReducer.reduce','org.apache.hadoop.io.LongWritable.get'
'org.apache.mahout.utils.eval.ParallelFactorizationEvaluator.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.utils.eval.ParallelFactorizationEvaluator.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.utils.eval.ParallelFactorizationEvaluator.computeRmse','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.utils.eval.ParallelFactorizationEvaluator.PairsWithRatingMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.utils.eval.ParallelFactorizationEvaluator.ErrorReducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.NullWritable.get'
'Recommender.ParallelKNN.createNeighborhoods','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'Recommender.ParallelKNN.CalcMap.setup','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'Recommender.ParallelKNN.CalcMap.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'Recommender.ParallelKNN.CalcReduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'Recommender.ParallelKNN.runCalc','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.getName org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setMaxInputSplitSize org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.waitForCompletion'
'Recommender.ParallelKNN.createChunkNameFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'Recommender.ParallelKNN.getOutputPath','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init>'
'Recommender.ParallelKNN.removeOldOutputDir','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'edu.duke.starfish.jobopt.space.ParameterSpacePoint.populateConfiguration','org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.common.Parameters.toString','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.String>>.toString'
'org.apache.mahout.common.Parameters.parseParams','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.String>>.fromString'
'org.apache.mahout.common.Parameters.toString','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.String>>.toString'
'org.apache.mahout.common.Parameters.parseParams','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.String>>.fromString'
'edu.jhu.thrax.hadoop.jobs.ParaphraseAggregationJob.getJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput'
'org.apache.nutch.parse.ParseData.readFields','org.apache.hadoop.io.VersionMismatchException.<init> org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.apache.nutch.parse.ParseData.write','org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.parse.ParseData.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.close'
'mapreduce.ParseJson.ParseMapper.setup','org.apache.hadoop.hbase.util.Bytes.toBytes'
'mapreduce.ParseJson.ParseMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.util.Bytes.toStringBinary'
'mapreduce.ParseJson.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.parseColumn org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.waitForCompletion'
'mapreduce.ParseJson2.ParseMapper.setup','org.apache.hadoop.hbase.util.Bytes.toBytes'
'mapreduce.ParseJson2.ParseMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.util.Bytes.toStringBinary'
'mapreduce.ParseJson2.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.parseColumn org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'gov.llnl.ontology.mapreduce.ingest.ParseMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.ingest.ParseMR.setupConfiguration','org.apache.hadoop.conf.Configuration.set'
'gov.llnl.ontology.mapreduce.ingest.ParseMR.ParseMapper.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.parse.ParseOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.JobConf.getNumReduceTasks org.apache.hadoop.mapred.InvalidJobConfException.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.nutch.parse.ParseOutputFormat.getRecordWriter','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.getOutputCompressionType org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,UNRESOLVED.Parse>.<init>'
'org.apache.nutch.parse.ParseOutputFormat.write','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.ParsePluginsReader.parse','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getConfResourceAsInputStream'
'org.apache.nutch.parse.ParseResult.createParseResult','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.ParseResult.get','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.ParseResult.put','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.ParseSegment.map','org.apache.hadoop.io.WritableComparable.toString org.apache.hadoop.io.Text.set org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,UNRESOLVED.ParseImpl>.collect'
'org.apache.nutch.parse.ParseSegment.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable>.collect'
'org.apache.nutch.parse.ParseSegment.parse','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.parse.ParseSegment.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.parse.ParseSegment.run','org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.parse.ParseText.readFields','org.apache.hadoop.io.WritableUtils.readCompressedString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.VersionMismatchException.<init>'
'org.apache.nutch.parse.ParseText.write','org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.parse.ParseText.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.close'
'org.apache.nutch.parse.ParserChecker.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.ParserChecker.main','org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.mapred.ec2.parser.ParserMapRunner.run','org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.CrawlURL>.createKey org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.CrawlURL>.createValue org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.CrawlURL>.next org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.CrawlURL>.getProgress org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.CrawlURL>.getPos'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.buildRedirectObject','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.parseResultToJsonObject','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.parseFeedDocument','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.populateContentMetadata','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.toString org.apache.hadoop.io.MD5Hash.getDigest org.apache.hadoop.io.MD5Hash.getDigest org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.map','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.MD5Hash.<init> org.apache.hadoop.io.MD5Hash.getDigest org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.commoncrawl.protocol.ParseOutput>.collect org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.commitTask','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.configure','org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getLong'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.commoncrawl.protocol.ParseOutput>.<init>'
'org.commoncrawl.mapred.ec2.parser.ParserMapper.collect','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest.testProcessOutput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.Writer.append org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest.testConfigure','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest.PartialBuilderChecker.runJob','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.getInt'
'org.apache.mahout.df.mapreduce.partial.PartialBuilderTest.testProcessOutput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.Writer.append org.apache.hadoop.io.SequenceFile.Writer.close org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.mahout.df.mapreduce.partial.PartialBuilderTest.testConfigure','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapreduce.partial.PartialBuilderTest.PartialBuilderChecker.runJob','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.getInt'
'skywriting.examples.skyhout.common.PartialHashOutputCollector.PartialHashOutputCollector','org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<skywriting.examples.skyhout.common.K,skywriting.examples.skyhout.common.V>.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapreduce.partial.PartialSequentialBuilder.PartialSequentialBuilder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.df.mapreduce.partial.PartialSequentialBuilder.configureJob','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.setInt'
'org.apache.mahout.df.mapreduce.partial.PartialSequentialBuilder.runJob','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.getSplits org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.lib.input.TextInputFormat.createRecordReader org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue'
'org.apache.mahout.df.mapreduce.partial.PartialSequentialBuilder.parseOutput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.df.mapreduce.partial.PartialSequentialBuilder.secondStep','org.apache.hadoop.mapreduce.JobID.<init> org.apache.hadoop.mapreduce.JobContext.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init> org.apache.hadoop.mapreduce.lib.input.TextInputFormat.getSplits org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.lib.input.TextInputFormat.createRecordReader org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue'
'org.apache.mahout.vectorizer.common.PartialVectorMergeReducer.reduce','org.apache.hadoop.io.WritableComparable<?>.toString'
'org.apache.mahout.vectorizer.common.PartialVectorMergeReducer.setup','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.vectorizer.common.PartialVectorMerger.mergePartialVectors','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.vectorizer.common.PartialVectorMerger.getCommaSeparatedPaths','org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.vectorizer.common.PartialVectorMerger.mergePartialVectors','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.vectorizer.common.PartialVectorMerger.getCommaSeparatedPaths','org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.df.mapreduce.partial.PartitionBugTest.testProcessOutput','org.apache.hadoop.conf.Configuration.<init>'
'.PartitionByStationUsingMultipleOutputFormat.StationMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'.PartitionByStationUsingMultipleOutputFormat.StationReducer.reduce','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.Text>.collect'
'.PartitionByStationUsingMultipleOutputFormat.run','org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'.PartitionByStationUsingMultipleOutputFormat.main','org.apache.hadoop.util.ToolRunner.run'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.PartitionCrawlDBStep.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.PartitionCrawlDBStep.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.crawllistgen.PartitionCrawlDBStep.runStep','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.pig.newplan.logical.rules.PartitionFilterOptimizer.PartitionFilterPushDownTransformer.check','org.apache.hadoop.mapreduce.Job.<init>'
'edu.umd.cloud9.example.pagerank.PartitionGraph.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.pagerank.PartitionGraph.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.pagerank.PartitionGraph.run','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.rapleaf.hank.cascading.PartitionMarkerTap.PartitionMarkerRecordReader.next','org.apache.hadoop.io.IntWritable.set'
'com.rapleaf.hank.cascading.PartitionMarkerTap.PartitionMarkerRecordReader.createKey','org.apache.hadoop.io.IntWritable.<init>'
'com.rapleaf.hank.cascading.PartitionMarkerTap.PartitionMarkerRecordReader.createValue','org.apache.hadoop.io.IntWritable.<init>'
'com.rapleaf.hank.cascading.PartitionMarkerTap.PartitionMarkerScheme.sourceConfInit','org.apache.hadoop.mapred.JobConf.setInputFormat'
'com.rapleaf.hank.cascading.PartitionMarkerTap.PartitionMarkerInputSplit.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'com.rapleaf.hank.cascading.PartitionMarkerTap.PartitionMarkerInputSplit.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.nutch.crawl.PartitionUrlByHost.configure','org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.nutch.crawl.PartitionUrlByHost.getPartition','org.apache.hadoop.io.Text.toString'
'com.sap.hadoop.ds.list.PartitionedByteBasedListTest.test1','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getSerializedClass org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize'
'com.sap.hadoop.ds.list.PartitionedByteBasedListTest.test2','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getSerializedClass org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize'
'com.sap.hadoop.ds.sortedmap.PartitionedByteBasedSortedMapTest.setup','org.apache.hadoop.io.WritableComparator.get org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getSerializedClass org.apache.hadoop.io.VIntWritable.<init>'
'com.sap.hadoop.ds.sortedmap.PartitionedByteBasedSortedMapTest.testGeti','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.VIntWritable.get'
'com.sap.hadoop.ds.sortedmap.PartitionedByteBasedSortedMapTest.testKeyItr','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.VIntWritable.get'
'com.sap.hadoop.ds.sortedmap.PartitionedByteBasedSortedMapTest.testEntryItr','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VIntWritable.get'
'com.sap.hadoop.ds.sortedmap.PartitionedByteBasedSortedMapTest.testMultiplePut','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.VIntWritable.get'
'com.sap.hadoop.ds.sortedmap.PartitionedByteBasedSortedMapTest.testRandomPut','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.VIntWritable.get'
'edu.isi.mavuno.extract.PassageExtractor.setDocument','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'edu.isi.mavuno.extract.PassageExtractor.setupNextPassage','org.apache.hadoop.io.Text.set'
'com.manning.hip.ch3.passwd.Passwd.write','org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeString'
'com.manning.hip.ch3.passwd.Passwd.readFields','org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readString'
'com.manning.hip.ch3.passwd.Passwd.writeLong','org.apache.hadoop.io.WritableUtils.writeVLong'
'com.manning.hip.ch3.passwd.Passwd.readLong','org.apache.hadoop.io.WritableUtils.readVLong'
'com.manning.hip.ch3.passwd.PasswdInputFormat.isSplitable','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'com.manning.hip.ch3.passwd.PasswdInputFormat.PasswdRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize'
'com.manning.hip.ch3.passwd.PasswdInputFormat.PasswdRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue'
'com.manning.hip.ch3.passwd.PasswdInputFormat.PasswdRecordReader.parseLine','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue'
'com.manning.hip.ch3.passwd.PasswdInputFormat.PasswdRecordReader.getCurrentKey','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentKey'
'com.manning.hip.ch3.passwd.PasswdInputFormat.PasswdRecordReader.getProgress','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress'
'com.manning.hip.ch3.passwd.PasswdInputFormat.PasswdRecordReader.close','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'com.manning.hip.ch3.passwd.PasswdOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'com.cloudera.util.PathManager.PathManager','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.util.PathManager.open','org.apache.hadoop.fs.FileSystem.create'
'com.cloudera.util.PathManager.close','org.apache.hadoop.fs.FileSystem.rename'
'org.goldenorb.algorithms.singleSourceShortestPath.PathMessage.setWeight','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.pig.piggybank.storage.partition.PathPartitioner.getPartitionKeys','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.getName'
'com.cloudera.hadoop.hdfs.nfs.PathUtils.realPath','org.apache.hadoop.fs.Path.toUri'
'org.goldenorb.algorithms.singleSourceShortestPath.PathWritable.PathWritable','org.apache.hadoop.io.IntWritable.<init>'
'org.goldenorb.algorithms.singleSourceShortestPath.PathWritable.addVertex','org.apache.hadoop.io.Text.<init>'
'org.goldenorb.algorithms.singleSourceShortestPath.PathWritable.readFields','org.apache.hadoop.io.IntWritable.readFields'
'org.goldenorb.algorithms.singleSourceShortestPath.PathWritable.write','org.apache.hadoop.io.IntWritable.write'
'org.goldenorb.algorithms.singleSourceShortestPath.PathWritable.compareTo','org.apache.hadoop.io.IntWritable.compareTo'
'org.goldenorb.algorithms.singleSourceShortestPath.PathWritable.equals','org.apache.hadoop.io.IntWritable.equals'
'edu.isi.mavuno.app.distsim.PatternToPattern.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set'
'edu.isi.mavuno.app.distsim.PatternToPattern.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.oozie.service.PauseTransitService.PauseTransitRunnable.updateCoord','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.service.PauseTransitService.init','org.apache.hadoop.conf.Configuration.getInt'
'p3.runner.PcapCountUp.main','org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'net.ripe.hadoop.pcap.serde.PcapDeserializer.initialize','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector'
'net.ripe.hadoop.pcap.serde.PcapDeserializer.deserialize','org.apache.hadoop.io.ObjectWritable.get'
'p3.runner.PcapRate.main','org.apache.hadoop.mapred.JobConf.addResource org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.Path.<init>'
'p3.runner.PcapRealtimeTest.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'p3.runner.PcapRealtimeTest.main','org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath'
'net.ripe.hadoop.pcap.io.reader.PcapRecordReader.next','org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.progress'
'net.ripe.hadoop.pcap.io.reader.PcapRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'net.ripe.hadoop.pcap.io.reader.PcapRecordReader.createValue','org.apache.hadoop.io.ObjectWritable.<init>'
'net.ripe.hadoop.pcap.io.reader.PcapRecordReader.getPos','org.apache.hadoop.fs.Seekable.getPos'
'net.ripe.hadoop.pcap.io.reader.PcapRecordReaderTest.skipToEnd','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.ObjectWritable.<init>'
'net.ripe.hadoop.pcap.io.reader.PcapRecordReaderTest.startup','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.pentaho.hadoop.mapreduce.PentahoMapReduceBase.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getMapOutputKeyClass org.apache.hadoop.mapred.JobConf.getMapOutputValueClass org.apache.hadoop.mapred.JobConf.getOutputKeyClass org.apache.hadoop.mapred.JobConf.getOutputValueClass org.apache.hadoop.mapred.JobConf.get'
'org.pentaho.hadoop.mapreduce.PentahoMapReduceBase.isCombinerSingleThreaded','org.apache.hadoop.conf.Configuration.get'
'org.pentaho.hadoop.mapreduce.PentahoMapReduceBase.isReducerSingleThreaded','org.apache.hadoop.conf.Configuration.get'
'org.pentaho.hadoop.mapreduce.PentahoMapReduceBase.setDebugStatus','org.apache.hadoop.mapred.Reporter.setStatus'
'org.pentaho.hadoop.mapreduce.PentahoMapRunnable.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getMapOutputKeyClass org.apache.hadoop.mapred.JobConf.getMapOutputValueClass org.apache.hadoop.mapred.JobConf.getNumMapTasks org.apache.hadoop.mapred.JobConf.getNumReduceTasks org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.pentaho.hadoop.mapreduce.PentahoMapRunnable.setDebugStatus','org.apache.hadoop.mapred.Reporter.setStatus'
'org.pentaho.hadoop.mapreduce.PentahoMapRunnable.run','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K1,UNRESOLVED.V1>.createKey org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K1,UNRESOLVED.V1>.createValue org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K1,UNRESOLVED.V1>.next org.apache.hadoop.mapred.Reporter.setStatus'
'org.pentaho.hbase.mapred.PentahoTableRecordReaderImpl.restart','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setTimeStamp org.apache.hadoop.hbase.client.Scan.setTimeRange org.apache.hadoop.hbase.client.HTable.getScanner'
'org.pentaho.hbase.mapred.PentahoTableRecordReaderImpl.configureScanWithInputColumns','org.apache.hadoop.hbase.KeyValue.parseColumn org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.addFamily'
'org.pentaho.hbase.mapred.PentahoTableRecordReaderImpl.close','org.apache.hadoop.hbase.client.ResultScanner.close'
'org.pentaho.hbase.mapred.PentahoTableRecordReaderImpl.createKey','org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init>'
'org.pentaho.hbase.mapred.PentahoTableRecordReaderImpl.createValue','org.apache.hadoop.hbase.client.Result.<init>'
'org.pentaho.hbase.mapred.PentahoTableRecordReaderImpl.next','org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.io.ImmutableBytesWritable.set org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.util.Writables.copyWritable'
'org.archive.hadoop.PerMapOutputFormat.getOutputFilename','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'org.archive.hadoop.PerMapOutputFormat.getOutputFormat','org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.archive.hadoop.PerMapOutputFormat.getRecordWriter','org.apache.hadoop.mapred.OutputFormat<UNRESOLVED.K,UNRESOLVED.V>.getRecordWriter'
'.Perceptron.collectOutput','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.DoubleWritable>.collect'
'com.cloudera.flume.PerfHdfsIO.testCopy','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FileSystem.close'
'com.cloudera.flume.PerfHdfsIO.testDirectWrite','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.Writer.append org.apache.hadoop.io.SequenceFile.Writer.close org.apache.hadoop.fs.FileSystem.close'
'org.deephacks.tools4j.config.PerfTest.testHbase','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'org.deephacks.tools4j.config.PerfTest.createTable','org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.cloudata.core.PerformanceTest.EvaluationMapTask.configure','org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.core.PerformanceTest.EvaluationMapTask.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.collect'
'org.cloudata.core.PerformanceTest.EvaluationMapTask.setStatus','org.apache.hadoop.mapred.Reporter.setStatus'
'org.cloudata.core.PerformanceTest.runNIsMoreThanOne','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMaxMapAttempts org.apache.hadoop.mapred.JobConf.setMaxReduceAttempts org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'org.cloudata.core.PerformanceTest.writeInputFile','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.cloudata.core.PerformanceTest.main','org.apache.hadoop.conf.Configuration.<init>'
'com.sap.hadoop.ds.sortedmap.PersistentByteBasedSortedMapTest.setup','org.apache.hadoop.io.WritableComparator.get org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getSerializedClass org.apache.hadoop.io.VIntWritable.<init>'
'com.sap.hadoop.ds.sortedmap.PersistentByteBasedSortedMapTest.testGeti','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.VIntWritable.get'
'com.sap.hadoop.ds.sortedmap.PersistentByteBasedSortedMapTest.testKeyItr','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.VIntWritable.get'
'com.sap.hadoop.ds.sortedmap.PersistentByteBasedSortedMapTest.testEntryItr','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VIntWritable.get'
'com.sap.hadoop.ds.sortedmap.PersistentByteBasedSortedMapTest.testRandomPut','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.VIntWritable.get'
'com.manning.hip.ch6.PersonSortMapReduce.runSortJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch6.PersonSortMapReduce.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch6.PersonSortMapReduce.Reduce.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.godhuli.rhipe.PersonalServer.PersonalServer','org.apache.hadoop.conf.Configuration.<init>'
'org.godhuli.rhipe.PersonalServer.rhcat','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.godhuli.rhipe.PersonalServer.process','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.open'
'org.godhuli.rhipe.PersonalServer.sequenceAsBinary','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.godhuli.rhipe.PersonalServer.rhopensequencefile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.godhuli.rhipe.PersonalServer.rhgetkeys','org.apache.hadoop.conf.Configuration.setInt'
'org.godhuli.rhipe.PersonalServer.binaryAsSequence','org.apache.hadoop.conf.Configuration.<init>'
'org.godhuli.rhipe.PersonalServer.DelayedExceptionThrowing.globAndProcess','org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileUtil.stat2Paths'
'com.twitter.elephantbird.examples.PhoneNumberCounter.PhoneNumberCounterMapper.map','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'com.twitter.elephantbird.examples.PhoneNumberCounter.PhoneNumberCounterReducer.reduce','org.apache.hadoop.io.LongWritable.set'
'com.twitter.elephantbird.examples.PhoneNumberCounter.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.twitter.elephantbird.examples.PhoneNumberCounter.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.hooka.Phrase2CountMap.plusEquals','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.set'
'edu.umd.hooka.Phrase2CountMap.setPhraseCount','org.apache.hadoop.io.FloatWritable.<init>'
'edu.umd.hooka.Phrase2CountMap.getPhraseCount','org.apache.hadoop.io.FloatWritable.get'
'edu.umd.hooka.Phrase2CountMap.normalize','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.set'
'edu.umd.hooka.Phrase2CountMap.readFields','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.readFields'
'edu.umd.hooka.PhraseExtractAndCount.MapClass1.map','org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.hooka.PhraseExtractAndCount.MapClass2.map','org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.hooka.PhraseExtractAndCount.MapClass3.map','org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.hooka.PhraseExtractAndCount.MapClass4.map','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,edu.umd.hooka.alignment.IndexedFloatArray>.collect org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,edu.umd.hooka.alignment.IndexedFloatArray>.collect'
'edu.umd.hooka.PhraseExtractAndCount.Reduce.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.hooka.PhraseExtractAndCount.ReducePT.reduce','org.apache.hadoop.io.FloatWritable.set org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,org.apache.hadoop.io.FloatWritable>.collect'
'edu.umd.hooka.PhraseExtractAndCount.ReduceSumScores.reduce','org.apache.hadoop.mapred.OutputCollector<edu.umd.hooka.PhrasePair,edu.umd.hooka.alignment.IndexedFloatArray>.collect'
'edu.umd.hooka.PhraseExtractAndCount.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'com.inadco.hbl.compiler.Pig8CubeIncrementalCompilerBean.generateHbaseProtoStorageSpec','org.apache.hadoop.hbase.util.Bytes.toString'
'com.inadco.hbl.compiler.Pig8CubeIncrementalCompilerBean.generateHbaseByteArrayStorageSpec','org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.oozie.action.hadoop.PigActionExecutor.getLauncherMain','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.PigActionExecutor.setupLauncherConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.oozie.action.hadoop.PigActionExecutor.setupActionConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'org.apache.oozie.action.hadoop.PigActionExecutor.getStats','org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.PigActionExecutor.getExternalChildIDs','org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.PigActionExecutor.getDataFromPath','org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.action.hadoop.PigActionExecutor.getLauncherMain','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.PigActionExecutor.setupLauncherConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.oozie.action.hadoop.PigActionExecutor.setupActionConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'org.apache.pig.piggybank.storage.avro.PigAvroOutputFormat.setDeflateLevel','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.piggybank.storage.avro.PigAvroOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getCompressOutput org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBytesRawComparator.setConf','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapred.JobConf.get'
'org.apache.pig.impl.PigContext.instantiateObjectFromParams','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.springframework.data.hadoop.pig.PigContextFactoryBean.afterPropertiesSet','org.apache.hadoop.conf.Configuration.iterator'
'org.apache.pig.tools.counters.PigCounterHelper.incrCounter','org.apache.hadoop.mapreduce.Counter.increment'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigDateTimeRawComparator.setConf','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapred.JobConf.get'
'org.apache.pig.impl.io.PigFile.store','org.apache.hadoop.mapreduce.JobID.<init> org.apache.hadoop.mapreduce.OutputFormat<?,?>.checkOutputSpecs org.apache.hadoop.mapreduce.OutputFormat<?,?>.getOutputCommitter org.apache.hadoop.mapreduce.OutputCommitter.setupJob org.apache.hadoop.mapreduce.OutputCommitter.setupTask org.apache.hadoop.mapreduce.OutputFormat<?,?>.getRecordWriter org.apache.hadoop.mapreduce.RecordWriter<?,?>.close org.apache.hadoop.mapreduce.OutputCommitter.needsTaskCommit org.apache.hadoop.mapreduce.OutputCommitter.commitTask'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigFloatRawComparator.setConf','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapred.JobConf.get'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.mergeSplitSpecificConf','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getLoadFunc','org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getLoadLocation','org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.passLoadSignature','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.JobContext.getWorkingDirectory org.apache.hadoop.fs.FileSystem.setWorkingDirectory org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.InputFormat.getSplits org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getPigSplits','org.apache.hadoop.conf.Configuration.getLong'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigIntRawComparator.setConf','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapred.JobConf.get'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigLongRawComparator.setConf','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.PigMain.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.PigMain.setPigScript','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.PigMainWithOldAPI.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.PigMainWithOldAPI.setPigScript','org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.IllustratorContext.IllustratorContext','org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.setUpContext','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.storeCleanup','org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.cleanupJob','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.commitJob','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.abortJob','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.abortTask','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.commitTask','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.needsTaskCommit','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.setupTask','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.PigRecordWriter.close','org.apache.hadoop.mapreduce.RecordWriter.close'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.setLocation','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecsHelper','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.OutputFormat.checkOutputSpecs'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getStores','org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.setupUdfEnvAndStores','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init>'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.isConfPropEqual','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.PigServer.launchPlan','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init>'
'org.springframework.data.hadoop.pig.PigServerFactoryBean.createPigInstance','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.pig.tools.pigstats.PigStatsUtil.getMultiStoreCount','org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getCounters'
'org.apache.pig.tools.pigstats.PigStatsUtil.accumulateStats','org.apache.hadoop.mapred.jobcontrol.JobControl.getSuccessfulJobs org.apache.hadoop.mapred.jobcontrol.JobControl.getFailedJobs org.apache.hadoop.mapred.jobcontrol.Job.getMessage'
'org.apache.pig.tools.pigstats.PigStatsUtil.addSuccessJobStats','org.apache.hadoop.mapred.jobcontrol.Job.getJobConf org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID'
'org.apache.pig.tools.pigstats.PigStatusReporter.getCounter','org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter'
'org.apache.pig.tools.pigstats.PigStatusReporter.progress','org.apache.hadoop.mapreduce.TaskInputOutputContext.progress'
'org.apache.pig.tools.pigstats.PigStatusReporter.setStatus','org.apache.hadoop.mapreduce.TaskInputOutputContext.setStatus'
'org.apache.pig.builtin.PigStorage.getNext','org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentValue org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.pig.builtin.PigStorage.putNext','org.apache.hadoop.mapreduce.RecordWriter.write'
'org.apache.pig.builtin.PigStorage.prepareToRead','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'org.apache.pig.builtin.PigStorage.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'org.apache.pig.builtin.PigStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.builtin.PigStorage.setCompression','org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput'
'org.apache.oozie.action.hadoop.PigTestCase.testEmbeddedPigWithinPython','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.pig.test.PigTestLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPathFilter'
'org.apache.pig.test.PigTestLoader.TestPathFilter.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat.listStatus','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextRawComparator.setConf','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapred.JobConf.get'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTupleDefaultRawComparator.setConf','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.mapred.JobConf.get'
'com.manning.hip.ch13.mrunit.PipelineTest.setUp','org.apache.hadoop.mapred.lib.IdentityMapper<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.<init> org.apache.hadoop.mapred.lib.IdentityReducer<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.<init> org.apache.hadoop.mapred.lib.IdentityMapper<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.<init> org.apache.hadoop.mapred.lib.IdentityReducer<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.<init> org.apache.hadoop.mrunit.PipelineMapReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.<init> org.apache.hadoop.mrunit.types.Pair<org.apache.hadoop.mapred.Mapper,org.apache.hadoop.mapred.Reducer>.<init> org.apache.hadoop.mrunit.PipelineMapReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.addMapReduce org.apache.hadoop.mrunit.types.Pair<org.apache.hadoop.mapred.Mapper,org.apache.hadoop.mapred.Reducer>.<init> org.apache.hadoop.mrunit.PipelineMapReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.addMapReduce'
'com.manning.hip.ch13.mrunit.PipelineTest.testIdentityMapper','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.PipelineMapReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.oozie.action.hadoop.PipesMain.submitJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.filecache.DistributedCache.createSymlink org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.pipes.Submitter.jobSubmit'
'org.apache.oozie.action.hadoop.PipesMain.setPipes','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'eu.scape_project.pt.proc.PitProcessor.initialize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.jhu.thrax.hadoop.features.pivot.PivotedLhsGivenTargetPhraseFeature.pivot','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.features.pivot.PivotedRarityPenaltyFeature.pivot','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.features.pivot.PivotedRarityPenaltyFeature.aggregate','org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get'
'edu.jhu.thrax.hadoop.features.pivot.PivotedRarityPenaltyFeature.finalizeAggregation','org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.features.pivot.PivotedSourcePhraseGivenLHSFeature.pivot','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.features.pivot.PivotedSourcePhraseGivenTargetFeature.pivot','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.features.pivot.PivotedTargetPhraseGivenLHSFeature.pivot','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.features.pivot.PivotedTargetPhraseGivenSourceAndLHSFeature.pivot','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.features.pivot.PivotedTargetPhraseGivenSourceFeature.pivot','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.paraphrasing.PivotingReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.jhu.thrax.hadoop.paraphrasing.PivotingReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'edu.jhu.thrax.hadoop.paraphrasing.PivotingReducer.pivotOne','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put'
'edu.jhu.thrax.hadoop.paraphrasing.PivotingReducer.prune','org.apache.hadoop.io.MapWritable.containsKey org.apache.hadoop.io.MapWritable.get'
'edu.jhu.thrax.hadoop.paraphrasing.PivotingReducer.ParaphrasePattern.ParaphrasePattern','org.apache.hadoop.io.MapWritable.<init>'
'edu.jhu.thrax.hadoop.paraphrasing.PivotingReducer.PruningRule.applies','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get'
'com.odiago.flumebase.plan.PlanContext.PlanContext','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.nutch.plugin.PluginRepository.PluginRepository','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.plugin.PluginRepository.PluginRepository','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.plugin.PluginRepository.get','org.apache.hadoop.conf.Configuration.hashCode'
'org.apache.hive.pdk.PluginTest.runHive','org.apache.hadoop.hive.cli.CliDriver.run'
'edu.umd.cloud9.collection.pmc.PmcArticle.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'edu.umd.cloud9.collection.pmc.PmcArticle.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'lsh.hadoop.PointDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'hadoop.PointInputFormat.PointRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'hadoop.PointInputFormat.PointRecordReader.nextKeyValue','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine'
'hadoop.PointInputFormat.PointRecordReader.getCurrentValue','org.apache.hadoop.io.Text.toString'
'hadoop.PointInputFormat.PointRecordReader.close','org.apache.hadoop.util.LineReader.close'
'hadoop.PointOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'lsh.hadoop.PointReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'hadoop.PointWritable.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'hadoop.PointWritable.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'p3.pcap.examples.PopularityCountUp.Map.map','org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.PopularityCountUp.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.PopularityCountUp.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'p3.pcap.examples.PopularityGen.Map.map','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.PopularityGen.Reduce.reduce','org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.PopularityGen.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'ivory.core.preprocess.PositionalSequenceFileRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart'
'org.apache.mahout.cf.taste.hadoop.als.PredictionJobTest.smallIntegration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'fm.last.feathers.partition.Prefix.getPartition','org.apache.hadoop.typedbytes.TypedBytesWritable.getValue org.apache.hadoop.typedbytes.TypedBytesWritable.setValue'
'org.apache.mahout.text.PrefixAdditionFilter.process','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.text.PrefixAdditionFilter.process','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath'
'ivory.core.data.stat.PrefixEncodedGlobalStats.PrefixEncodedGlobalStats','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.stat.PrefixEncodedGlobalStats.loadDFStats','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.stat.PrefixEncodedGlobalStats.loadCFStats','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.stat.PrefixEncodedGlobalStats.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'ivory.core.data.stat.PrefixEncodedGlobalStatsWithIndex.PrefixEncodedGlobalStatsWithIndex','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.stat.PrefixEncodedGlobalStatsWithIndex.loadDfs','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.stat.PrefixEncodedGlobalStatsWithIndex.loadIdToTerm','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.stat.PrefixEncodedGlobalStatsWithIndex.loadCfs','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.core.data.stat.PrefixEncodedGlobalStatsWithIndex.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'ivory.core.data.dictionary.PrefixEncodedLexicographicallySortedDictionaryTest.test1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'ivory.core.data.dictionary.PrefixEncodedLexicographicallySortedDictionaryTest.test2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'nl.vu.datalayer.hbase.bulkload.PrefixMatch.PrefixMatchSPOCMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.mapreduce.Mapper.Context.write'
'nl.vu.datalayer.hbase.bulkload.PrefixMatch.build','org.apache.hadoop.hbase.util.Bytes.putBytes org.apache.hadoop.hbase.util.Bytes.putBytes org.apache.hadoop.hbase.util.Bytes.putBytes org.apache.hadoop.hbase.util.Bytes.putBytes org.apache.hadoop.hbase.io.ImmutableBytesWritable.set org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.mapreduce.Mapper.Context.write'
'nl.vu.datalayer.hbase.bulkload.PrefixMatch.PrefixMatchPOCSMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get'
'nl.vu.datalayer.hbase.bulkload.PrefixMatch.PrefixMatchOCSPMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get'
'nl.vu.datalayer.hbase.bulkload.PrefixMatch.PrefixMatchCSPOMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get'
'nl.vu.datalayer.hbase.bulkload.PrefixMatch.PrefixMatchCPSOMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get'
'nl.vu.datalayer.hbase.bulkload.PrefixMatch.PrefixMatchOSPCMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get'
'org.apache.mahout.classifier.email.PrepEmailMapper.map','org.apache.hadoop.io.WritableComparable<?>.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.classifier.email.PrepEmailVectorsDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.classifier.email.PrepEmailVectorsDriver.run','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion'
'nl.waredingen.graphs.partition.PrepareJob.ToAdjacencyList.complete','org.apache.hadoop.util.StringUtils.joinObjects'
'nl.waredingen.graphs.patternfind.PrepareSequenceFileJob.SetEdgeFeatures.operate','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.MapWritable.put'
'nl.waredingen.graphs.patternfind.PrepareSequenceFileJob.SetNodeFeatures.operate','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.MapWritable.put'
'nl.waredingen.graphs.partition.PrepareWithFlagsJob.ToAdjacencyList.complete','org.apache.hadoop.util.StringUtils.joinObjects org.apache.hadoop.util.StringUtils.joinObjects'
'ivory.app.PreprocessClueWebEnglish.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.app.PreprocessClueWebEnglish.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.getCanonicalName org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'ivory.app.PreprocessClueWebEnglish.main','org.apache.hadoop.util.ToolRunner.run'
'ivory.app.PreprocessGov2.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.app.PreprocessGov2.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.getCanonicalName org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'ivory.app.PreprocessGov2.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ivory.lsh.bitext.PreprocessHelper.loadModels','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'ivory.lsh.bitext.PreprocessHelper.loadZhModels','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.lsh.bitext.PreprocessHelper.loadDeModels','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.lsh.bitext.PreprocessHelper.loadEnModels','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'ivory.lsh.bitext.PreprocessHelper.createFDocVectors','org.apache.hadoop.io.Text.toString'
'ivory.lsh.bitext.PreprocessHelper.createEDocVectors','org.apache.hadoop.io.Text.toString'
'ivory.lsh.bitext.PreprocessHelper.getESentences','org.apache.hadoop.io.Text.<init>'
'ivory.lsh.bitext.PreprocessHelper.getFSentences','org.apache.hadoop.io.Text.<init>'
'ivory.app.PreprocessMedline.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.app.PreprocessMedline.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'ivory.app.PreprocessMedline.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ivory.core.driver.PreprocessTRECChinese.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.core.driver.PreprocessTRECChinese.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'ivory.core.driver.PreprocessTRECChinese.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ivory.app.PreprocessTrec45.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.app.PreprocessTrec45.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'ivory.app.PreprocessTrec45.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ivory.app.PreprocessWt10g.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.app.PreprocessWt10g.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.getCanonicalName org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'ivory.app.PreprocessWt10g.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'.PrimeNumberPartitioner.getPartition','org.apache.hadoop.io.LongWritable.get'
'.PrimeNumberRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'.PrimeNumberRecordReader.createValue','org.apache.hadoop.io.NullWritable.get'
'.PrimeNumberRecordReader.next','org.apache.hadoop.io.LongWritable.set'
'goraci.Print.run','org.apache.hadoop.conf.Configuration.<init>'
'goraci.Print.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.metanalysis.PrintEvents.printEvents','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.file.rfile.PrintInfo.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusClusterEvaluationJob.ProClusClusterEvaluationJob','org.apache.hadoop.conf.Configuration.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusClusterEvaluationJob.parseArgs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusClusterEvaluationJob.commonInitialization','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusClusterEvaluationJob.getOutputFile','org.apache.hadoop.fs.Path.suffix'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusClusterEvaluationJob.computeFinalMetric','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.io.DoubleWritable.get'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusClusterEvaluationJob.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusClusterEvaluationJob.writeOutputSequence','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.Path.toUri org.apache.hadoop.io.DoubleWritable.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.ProClusGreedyInitializationJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.parseArgs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.commonInitialization','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.mkdirs'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.getOutputFile','org.apache.hadoop.fs.Path.suffix'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.run','org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.runGreedy','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.fileForIteration','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.delete'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.inputFileForIteration','org.apache.hadoop.fs.Path.suffix'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.buildFirstMedoidSet','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.io.Text.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationJob.readFirstElement','org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.listStatus'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusGreedyInitializationMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.toString org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusPointsAssignmentMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusPointsAssignmentMapper.readClusterSet','org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusPointsAssignmentReducer.reduce','org.apache.hadoop.io.Text.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusPointsClusteringJob.ProClusPointsClusteringJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusPointsClusteringJob.parseArgs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusPointsClusteringJob.commonInitialization','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusPointsClusteringJob.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusPointsClusteringReducer.reduce','org.apache.hadoop.io.Text.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementJob.ProClusRefinementJob','org.apache.hadoop.conf.Configuration.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementJob.parseArgs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementJob.commonInitialization','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementJob.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementJob.getOutputFile','org.apache.hadoop.fs.Path.suffix'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementJob.buildInitialClusters','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.io.IntWritable.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementJob.runSelectDimensions','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementJob.pathForOutputData','org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementMapper.map','org.apache.hadoop.io.Text.toString'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusRefinementMapper.readClusterSet','org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusSampler.ProClusSampler','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusSampler.parseArgs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusSampler.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.clustering.proclus.algorithm.ProClusSampler.countLines','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.suffix'
'org.apache.accumulo.server.problems.ProblemReport.removeFromMetadataTable','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.problems.ProblemReport.saveToMetadataTable','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.problems.ProblemReport.getZPath','org.apache.hadoop.io.Text.<init>'
'org.apache.ivory.entity.parser.ProcessEntityParserTest.init','org.apache.hadoop.hdfs.MiniDFSCluster.<init>'
'org.apache.hcatalog.mapreduce.ProgressReporter.setStatus','org.apache.hadoop.mapreduce.TaskInputOutputContext.setStatus'
'org.apache.hcatalog.mapreduce.ProgressReporter.getCounter','org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter'
'org.apache.hcatalog.mapreduce.ProgressReporter.incrCounter','org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter'
'org.apache.hcatalog.mapreduce.ProgressReporter.progress','org.apache.hadoop.mapreduce.TaskInputOutputContext.progress org.apache.hadoop.mapreduce.TaskAttemptContext.progress'
'org.apache.cassandra.hadoop.Progressable.progress','org.apache.hadoop.mapreduce.TaskAttemptContext.progress'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.ProgressableReporter.progress','org.apache.hadoop.mapreduce.TaskAttemptContext.progress org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.mapreduce.TaskAttemptContext.progress'
'org.apache.oozie.util.PropertiesUtils.checkDisallowedProperties','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.PropertiesUtils.checkDisallowedProperties','org.apache.hadoop.conf.Configuration.get'
'com.inadco.ecoadapters.hive.ProtoInspectorFactory.createProtobufInspector','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector'
'com.inadco.ecoadapters.hive.ProtoInspectorFactory.createListInspector','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardListObjectInspector'
'com.inadco.ecoadapters.hive.ProtoSerDe.initialize','org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.SerDeException.<init> org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.inadco.ecoadapters.hive.ProtoSerDe.deserialize','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'com.inadco.ecoadapters.hive.ProtoSerDe.serialize','org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.inadco.ecoadapters.hive.ProtoSerDe.toError','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getAllStructFieldRefs'
'com.datasalt.utils.commons.io.ProtoStuffDeserializer.newInstance','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.twitter.elephantbird.hive.serde.ProtobufDeserializer.initialize','org.apache.hadoop.conf.Configuration.getClassByName org.apache.hadoop.hive.serde2.SerDeException.<init>'
'com.twitter.elephantbird.hive.serde.ProtobufDeserializer.deserialize','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'com.twitter.elephantbird.hive.serde.ProtobufDeserializer.getObjectInspector','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector'
'org.pingles.cascading.protobuf.ProtobufFlowTest.writePersonToSequenceFile','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.BytesWritable.<init>'
'com.twitter.elephantbird.examples.ProtobufMRExample.TextMapper.map','org.apache.hadoop.io.Text.toString'
'com.twitter.elephantbird.examples.ProtobufMRExample.runTextToLzo','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.twitter.elephantbird.examples.ProtobufMRExample.LzoMapper.map','org.apache.hadoop.io.Text.<init>'
'com.twitter.elephantbird.examples.ProtobufMRExample.runLzoToText','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.twitter.elephantbird.examples.ProtobufMRExample.SortMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'com.twitter.elephantbird.examples.ProtobufMRExample.SortReducer.reduce','org.apache.hadoop.io.Text.<init>'
'com.twitter.elephantbird.examples.ProtobufMRExample.runSorter','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.twitter.elephantbird.examples.ProtobufMRExample.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.get'
'com.squareup.cascading2.scheme.ProtobufScheme.sinkConfInit','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat'
'com.squareup.cascading2.scheme.ProtobufScheme.source','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'com.squareup.cascading2.scheme.ProtobufScheme.sink','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.NullWritable.get'
'com.squareup.cascading2.serialization.ProtobufSerialization.ProtobufComparator.compare','org.apache.hadoop.io.WritableComparator.compareBytes'
'com.twitter.elephantbird.util.Protobufs.getProtobufClass','org.apache.hadoop.conf.Configuration.getClassByName'
'com.twitter.elephantbird.util.Protobufs.getTypeRef','org.apache.hadoop.conf.Configuration.get'
'com.twitter.elephantbird.util.Protobufs.toText','org.apache.hadoop.io.Text.<init>'
'com.twitter.elephantbird.util.Protobufs.mergeFromText','org.apache.hadoop.io.Text.getBytes'
'com.twitter.elephantbird.util.Protobufs.getProtobufClass','org.apache.hadoop.conf.Configuration.getClassByName'
'com.twitter.elephantbird.util.Protobufs.getTypeRef','org.apache.hadoop.conf.Configuration.get'
'com.twitter.elephantbird.util.Protobufs.setClassConf','org.apache.hadoop.conf.Configuration.set'
'org.dryfish.john.sftp.Protocol.process_realpath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_setstat','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_open','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_opendir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_readdir','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_mkdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_rename','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_rmdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Trash.<init> org.apache.hadoop.fs.Trash.moveToTrash org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_remove','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Trash.<init> org.apache.hadoop.fs.Trash.moveToTrash org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_read','org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_lstat','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.process_fstat','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.security.AccessControlException.getMessage'
'org.dryfish.john.sftp.Protocol.makeLongName','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getReplication org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.nutch.protocol.ProtocolStatus.readFields','org.apache.hadoop.io.WritableUtils.readCompressedStringArray org.apache.hadoop.io.WritableUtils.readStringArray org.apache.hadoop.io.VersionMismatchException.<init>'
'org.apache.nutch.protocol.ProtocolStatus.write','org.apache.hadoop.io.WritableUtils.writeStringArray'
'org.commoncrawl.util.ProtocolStatus.readFields','org.apache.hadoop.io.WritableUtils.readCompressedStringArray org.apache.hadoop.io.WritableUtils.readStringArray org.apache.hadoop.io.VersionMismatchException.<init>'
'org.commoncrawl.util.ProtocolStatus.write','org.apache.hadoop.io.WritableUtils.writeStringArray'
'com.inadco.ecoadapters.ecor.tests.Prototest.prototest','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'com.datasalt.pangool.tuplemr.mapred.lib.output.ProxyOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.OutputFormat.getRecordWriter'
'com.datasalt.pangool.tuplemr.mapred.lib.output.ProxyOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.OutputFormat.checkOutputSpecs'
'com.datasalt.pangool.tuplemr.mapred.lib.output.ProxyOutputFormat.createOutputFormatIfNeeded','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.datasalt.pangool.tuplemr.mapred.lib.output.ProxyOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getWorkPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.OutputFormat.getOutputCommitter'
'com.datasalt.pangool.tuplemr.mapred.lib.output.ProxyOutputFormat.ProxyOutputCommitter.setupJob','org.apache.hadoop.mapreduce.OutputCommitter.setupJob'
'com.datasalt.pangool.tuplemr.mapred.lib.output.ProxyOutputFormat.ProxyOutputCommitter.setupTask','org.apache.hadoop.mapreduce.OutputCommitter.setupTask'
'com.datasalt.pangool.tuplemr.mapred.lib.output.ProxyOutputFormat.ProxyOutputCommitter.commitTask','org.apache.hadoop.mapreduce.OutputCommitter.commitTask'
'com.datasalt.pangool.tuplemr.mapred.lib.output.ProxyOutputFormat.ProxyOutputCommitter.abortTask','org.apache.hadoop.mapreduce.OutputCommitter.abortTask'
'org.commoncrawl.service.listcrawler.ProxyPurgeUtils.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource'
'org.commoncrawl.service.listcrawler.ProxyPurgeUtils.listCandidates','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete'
'org.commoncrawl.service.listcrawler.ProxyPurgeUtils.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'org.commoncrawl.service.listcrawler.ProxyServer.initServer','org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.service.listcrawler.ProxyServlet2.service','org.apache.hadoop.record.Buffer.<init>'
'org.apache.nutch.tools.PruneIndexTool.main','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getConfResourceAsInputStream'
'eu.scape_project.pt.mapred.PtRecordReader.nextKeyValue','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.inmobi.databus.PublishMissingPathsTest.testPublishMissingPaths','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.close'
'com.inmobi.databus.PublishMissingPathsTest.VerifyMissingPublishPaths','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'test.modelgen.table.model.PurchaseTranError.setPurchaseNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setPurchaseType','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setTradeType','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setTradeNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setStoreCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setBuyerCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setPurchaseTypeCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setSellerCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setTenantCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setShipmentStoreCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setShipmentSalesTypeCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setDeductionCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setAccountCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setOwnershipFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setCutoffFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setPayoutFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError.setDisposeNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setPurchaseNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setPurchaseType','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setTradeType','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setTradeNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setStoreCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setBuyerCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setPurchaseTypeCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setSellerCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setTenantCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setShipmentStoreCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setShipmentSalesTypeCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setDeductionCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setAccountCode','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setOwnershipFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setCutoffFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setPayoutFlag','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setDisposeNo','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setErrorCause','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.PurchaseTranError2.setErrorCode','org.apache.hadoop.io.Text.modify'
'org.apache.oozie.service.PurgeService.init','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.PurgeService.init','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'client.PutIdenticalExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'client.PutListExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'org.childtv.hadoop.hbase.mapred.PutTableOutputFormat.configure','org.apache.hadoop.mapred.JobConf.get'
'org.childtv.hadoop.hbase.mapred.PutTableOutputFormat.createBatchUpdates','org.apache.hadoop.hbase.io.BatchUpdate.<init> org.apache.hadoop.hbase.io.BatchUpdate.put org.apache.hadoop.hbase.io.BatchUpdate.setTimestamp'
'client.PutWriteBufferExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.isAutoFlush org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.get'
'com.lightboxtechnologies.spectrum.PythonJobTest.testUnboxDouble','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.outputQHat','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<? super org.apache.hadoop.io.Writable,? super org.apache.mahout.math.hadoop.stochasticsvd.DenseBlockWritable>.collect'
'org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.outputR','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<? super org.apache.hadoop.io.Writable,? super org.apache.mahout.math.VectorWritable>.collect'
'org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.secondPass','org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.close','org.apache.hadoop.mapred.lib.MultipleOutputs.close'
'org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.getTempQw','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.Path.toString'
'nl.vu.datalayer.hbase.bulkload.QuadBreakDown.TripleToResourceMapper.setup','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'nl.vu.datalayer.hbase.bulkload.QuadBreakDown.TripleToResourceMapper.map','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get'
'nl.vu.datalayer.hbase.bulkload.QuadBreakDown.TripleToResourceMapper.getNewStatement','org.apache.hadoop.io.Writable.toString'
'nl.vu.datalayer.hbase.bulkload.QuadBreakDown.TripleToResourceReducer.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getWorkOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter'
'nl.vu.datalayer.hbase.bulkload.QuadBreakDown.TripleToResourceReducer.handleLiteral','org.apache.hadoop.mapreduce.Counter.increment'
'nl.vu.datalayer.hbase.bulkload.QuadBreakDown.TripleToResourceReducer.saveTypeIdToFile','org.apache.hadoop.hbase.io.ImmutableBytesWritable.set'
'org.apache.jena.tdbloader4.io.QuadWritable.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.jena.tdbloader4.io.QuadWritable.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'filters.QualifierFilterExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.BinaryComparator.<init> org.apache.hadoop.hbase.filter.QualifierFilter.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setFilter org.apache.hadoop.hbase.client.HTable.get'
'org.commoncrawl.mapred.pipelineV3.domainmeta.iptohost.QuantcastWhitelistByIPReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.gora.query.impl.QueryBase.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong'
'org.gora.query.impl.QueryBase.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong'
'org.apache.gora.query.impl.QueryBase.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong'
'org.apache.gora.query.impl.QueryBase.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong'
'org.gora.examples.mapreduce.QueryCounter.createJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass'
'org.gora.examples.mapreduce.QueryCounter.countQuery','org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'org.gora.examples.mapreduce.QueryCounter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ch.sentric.hbase.service.QueryDaoImpl.getQueries','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getNoVersionMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'fr.eurecom.dsg.hbase.QueryOne.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.close'
'meetup.beeno.QueryOpts.setStartKey','org.apache.hadoop.hbase.util.Bytes.toBytes'
'meetup.beeno.QueryOpts.setStopKey','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.talis.hbase.rdf.layout.hash.QueryRunnerHash.tableFind','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner'
'com.talis.hbase.rdf.layout.simple.QueryRunnerSimple.tableFind','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner'
'com.talis.hbase.rdf.layout.verticalpartitioning.QueryRunnerVP.tableFind','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner'
'com.talis.hbase.rdf.layout.vpindexed.QueryRunnerVPIndexed.tableFind','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner'
'com.talis.hbase.rdf.layout.vpindexed.QueryRunnerVPIndexed.nodeIterator','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.BinaryPrefixComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner'
'com.talis.hbase.rdf.layout.vpindexed.QueryRunnerVPIndexed.checkOSExists','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty'
'com.talis.hbase.rdf.layout.vpindexed.QueryRunnerVPIndexed.checkSPOExists','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty'
'com.talis.hbase.rdf.layout.verticalpartitioning.QueryRunnerVerticallyPartitioned.tableFind','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner'
'meetup.beeno.Query.execute','org.apache.hadoop.hbase.client.ResultScanner.close'
'meetup.beeno.Query.getCriteriaFilter','org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.filter.PageFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter'
'meetup.beeno.Query.debugFilter','org.apache.hadoop.hbase.filter.Filter.getClass'
'org.apache.accumulo.examples.simple.shard.Query.main','org.apache.hadoop.io.Text.<init>'
'org.apache.hcatalog.templeton.QueueStatusBean.QueueStatusBean','org.apache.hadoop.mapred.JobProfile.getJobID'
'org.apache.hcatalog.templeton.QueueStatusBean.QueueStatusBean','org.apache.hadoop.mapred.JobProfile.getJobID'
'com.senseidb.indexing.hadoop.reduce.RAMDirectoryUtil.writeRAMFiles','org.apache.hadoop.io.Text.writeString'
'com.senseidb.indexing.hadoop.reduce.RAMDirectoryUtil.readRAMFiles','org.apache.hadoop.io.Text.readString'
'org.apache.hcatalog.rcfile.RCFileMapReduceInputFormat.createRecordReader','org.apache.hadoop.mapreduce.InputSplit.toString org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus'
'org.apache.hcatalog.rcfile.RCFileMapReduceInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.hcatalog.rcfile.RCFileMapReduceRecordReader.close','org.apache.hadoop.hive.ql.io.RCFile.Reader.close'
'org.apache.hcatalog.rcfile.RCFileMapReduceRecordReader.getProgress','org.apache.hadoop.hive.ql.io.RCFile.Reader.getPosition'
'org.apache.hcatalog.rcfile.RCFileMapReduceRecordReader.nextKeyValue','org.apache.hadoop.hive.ql.io.RCFile.Reader.getCurrentRow'
'org.apache.hcatalog.rcfile.RCFileMapReduceRecordReader.next','org.apache.hadoop.hive.ql.io.RCFile.Reader.next org.apache.hadoop.hive.ql.io.RCFile.Reader.lastSeenSyncPos'
'org.apache.hcatalog.rcfile.RCFileMapReduceRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.hive.ql.io.RCFile.Reader.getPosition org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.hive.ql.io.RCFile.Reader.sync org.apache.hadoop.hive.ql.io.RCFile.Reader.getPosition org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.<init>'
'com.twitter.elephantbird.mapreduce.output.RCFileOutputFormat.setColumnNumber','org.apache.hadoop.conf.Configuration.setInt'
'com.twitter.elephantbird.mapreduce.output.RCFileOutputFormat.getColumnNumber','org.apache.hadoop.conf.Configuration.getInt'
'com.twitter.elephantbird.mapreduce.output.RCFileOutputFormat.createRCFileWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.SequenceFile.Metadata.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Metadata.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getFileSystem'
'com.twitter.elephantbird.mapreduce.input.RCFileProtobufInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.twitter.elephantbird.mapreduce.input.RCFileProtobufInputFormat.ProtobufReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.serde2.ColumnProjectionUtils.setFullyReadColumns org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.hive.serde2.ColumnProjectionUtils.setReadColumnIDs'
'com.twitter.elephantbird.mapreduce.input.RCFileProtobufInputFormat.ProtobufReader.getCurrentProtobufValue','org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.get org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getLength org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getData org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getStart org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getLength org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.get org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getLength org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getData org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getStart org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getLength'
'com.twitter.elephantbird.mapreduce.output.RCFileProtobufOutputFormat.init','org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.<init> org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.set'
'com.twitter.elephantbird.mapreduce.output.RCFileProtobufOutputFormat.ProtobufWriter.write','org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.set'
'com.twitter.elephantbird.mapreduce.output.RCFileProtobufOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.twitter.elephantbird.pig.load.RCFileProtobufPigLoader.setLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.twitter.elephantbird.mapreduce.input.RCFileThriftTupleInputFormat.TupleReader.getCurrentTupleValue','org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.get org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getLength org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getData org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getStart org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getLength'
'com.twitter.elephantbird.util.RCFileUtil.readMetadata','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.serde2.ColumnProjectionUtils.setFullyReadColumns org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Metadata.get org.apache.hadoop.io.SequenceFile.Metadata.get'
'com.twitter.elephantbird.util.RCFileUtil.findColumnsToRead','org.apache.hadoop.conf.Configuration.get'
'com.twitter.elephantbird.util.RCFileUtil.setRequiredFieldConf','org.apache.hadoop.conf.Configuration.set'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.READHandler.doHandle','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FileSystem.getFileStatus'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.REMOVEHandler.doHandle','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getModificationTime'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.RENAMEHandler.doHandle','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'nl.vu.datalayer.hbase.connection.RESTConnection.RESTConnection','org.apache.hadoop.hbase.rest.client.Cluster.<init> org.apache.hadoop.hbase.rest.client.Cluster.add org.apache.hadoop.hbase.rest.client.Client.<init>'
'nl.vu.datalayer.hbase.connection.RESTConnection.close','org.apache.hadoop.hbase.rest.client.Client.shutdown'
'org.apache.accumulo.core.file.rfile.RFile.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.accumulo.core.file.rfile.RFile.seekRandomly','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.file.rfile.RFileTest.TestRFile.openWriter','org.apache.hadoop.fs.FSDataOutputStream.<init>'
'org.apache.accumulo.core.file.rfile.RFileTest.TestRFile.closeWriter','org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.accumulo.core.file.rfile.RFileTest.TestRFile.openReader','org.apache.hadoop.fs.FSDataInputStream.<init>'
'org.apache.accumulo.core.file.rfile.RFileTest.TestRFile.closeReader','org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.accumulo.core.file.rfile.RFileTest.test3','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.file.rfile.RFileTest.test10','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.file.rfile.RFileTest.test11','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.file.rfile.RFileTest.runVersionTest','org.apache.hadoop.fs.FSDataInputStream.<init>'
'org.godhuli.rhipe.RHMRHelper.addJobConfToEnvironment','org.apache.hadoop.conf.Configuration.iterator org.apache.hadoop.conf.Configuration.get'
'org.godhuli.rhipe.RHMRHelper.doPartitionRelatedSetup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.godhuli.rhipe.RHMRHelper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.godhuli.rhipe.RHMRHelper.checkOuterrThreadsThrowable','org.apache.hadoop.util.StringUtils.stringifyException'
'org.godhuli.rhipe.RHMRHelper.writeCMD','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.godhuli.rhipe.RHMRHelper.write','org.apache.hadoop.io.WritableComparable.write'
'org.godhuli.rhipe.RHMRHelper.MROutputThread.readRecord','org.apache.hadoop.io.WritableComparable.readFields org.apache.hadoop.io.Writable.readFields'
'org.godhuli.rhipe.RHMRHelper.MROutputThread.run','org.apache.hadoop.mapreduce.TaskInputOutputContext<org.apache.hadoop.io.WritableComparable,UNRESOLVED.RHBytesWritable,org.apache.hadoop.io.WritableComparable,UNRESOLVED.RHBytesWritable>.write org.apache.hadoop.mapreduce.TaskInputOutputContext<org.apache.hadoop.io.WritableComparable,UNRESOLVED.RHBytesWritable,org.apache.hadoop.io.WritableComparable,UNRESOLVED.RHBytesWritable>.setStatus org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.godhuli.rhipe.RHMRHelper.MRErrorThread.run','org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter org.apache.hadoop.mapreduce.TaskInputOutputContext.setStatus org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter org.apache.hadoop.mapreduce.TaskInputOutputContext.progress org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.godhuli.rhipe.RHMRHelper.copyFiles','org.apache.hadoop.fs.Path.<init>'
'org.godhuli.rhipe.RHMRMapper.getPipeCommand','org.apache.hadoop.conf.Configuration.get'
'org.godhuli.rhipe.RHMRMapper.setup','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'org.godhuli.rhipe.RHMRReducer.getPipeCommand','org.apache.hadoop.conf.Configuration.get'
'org.godhuli.rhipe.RHMRReducer.getDoPipe','org.apache.hadoop.conf.Configuration.getInt'
'org.godhuli.rhipe.RHMRReducer.setup','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.getBoolean'
'org.godhuli.rhipe.RHMapFileOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.getOutputCompressionType org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.RecordWriter<org.godhuli.rhipe.RHBytesWritable,org.godhuli.rhipe.RHBytesWritable>.<init>'
'org.godhuli.rhipe.RHMapFileOutputFormat.getReaders','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.godhuli.rhipe.RHMapFileOutputFormat.getEntry','org.apache.hadoop.mapreduce.Partitioner<org.godhuli.rhipe.RHBytesWritable,org.godhuli.rhipe.RHBytesWritable>.getPartition org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.godhuli.rhipe.RHBytesWritable,org.godhuli.rhipe.RHBytesWritable>.getPartition'
'org.godhuli.rhipe.RHMapFileOutputFormat.getPartForKey','org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.godhuli.rhipe.RHBytesWritable,org.godhuli.rhipe.RHBytesWritable>.getPartition'
'org.godhuli.rhipe.RHReader.set','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get'
'org.godhuli.rhipe.RHText.RHText','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.godhuli.rhipe.RHText.set','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString'
'org.godhuli.rhipe.RHText.setAndFinis','org.apache.hadoop.io.Text.set'
'org.godhuli.rhipe.RHText.finis','org.apache.hadoop.io.Text.toString'
'org.godhuli.rhipe.RHText.readFields','org.apache.hadoop.io.Text.set'
'org.godhuli.rhipe.RHWriter.set','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get'
'com.taobao.adfs.distributed.rpc.RPC.Invocation.toByteArray','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'com.taobao.adfs.distributed.rpc.RPC.Invocation.Invocation','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'com.taobao.adfs.distributed.rpc.RPC.getProxy','org.apache.hadoop.net.NetUtils.getDefaultSocketFactory'
'org.mule.module.hbase.api.impl.RPCHBaseService.RPCHBaseService','org.apache.hadoop.hbase.client.HTableFactory.<init> org.apache.hadoop.hbase.HBaseConfiguration.create'
'org.mule.module.hbase.api.impl.RPCHBaseService.doWithHBaseAdmin','org.apache.hadoop.hbase.client.HBaseAdmin.isMasterRunning org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.client.HBaseAdmin.isTableDisabled org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.setInMemory org.apache.hadoop.hbase.HColumnDescriptor.setScope org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.addColumn org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.client.HTableInterface.getTableDescriptor org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.HTableDescriptor.getFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.modifyColumn org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteColumn org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTableInterface.delete org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.setTimeRange org.apache.hadoop.hbase.client.Scan.setTimeStamp org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.client.HTableInterface.checkAndPut org.apache.hadoop.hbase.client.HTableInterface.checkAndDelete org.apache.hadoop.hbase.client.HTableInterface.lockRow org.apache.hadoop.hbase.client.HTableInterface.unlockRow'
'org.mule.module.hbase.api.impl.RPCHBaseService.loadPropertiesInDescriptor','org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HColumnDescriptor.setBlocksize org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setCompactionCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setInMemory org.apache.hadoop.hbase.HColumnDescriptor.setTimeToLive org.apache.hadoop.hbase.HColumnDescriptor.setBlockCacheEnabled org.apache.hadoop.hbase.HColumnDescriptor.setBloomFilterType org.apache.hadoop.hbase.HColumnDescriptor.setScope org.apache.hadoop.hbase.HColumnDescriptor.setValue'
'org.mule.module.hbase.api.impl.RPCHBaseService.exists','org.apache.hadoop.hbase.client.Result.isEmpty'
'org.mule.module.hbase.api.impl.RPCHBaseService.ScannerAndResults.ScannerAndResults','org.apache.hadoop.hbase.client.ResultScanner.next'
'org.mule.module.hbase.api.impl.RPCHBaseService.ResultIterable.firstPage','org.apache.hadoop.hbase.client.HTableInterface.getScanner'
'org.mule.module.hbase.api.impl.RPCHBaseService.addProperties','org.apache.hadoop.conf.Configuration.set'
'org.mule.module.hbase.api.impl.RPCHBaseService.doFlush','org.apache.hadoop.hbase.client.HBaseAdmin.flush'
'org.mule.module.hbase.api.impl.RPCHBaseService.createGet','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.Get.setTimeStamp'
'org.mule.module.hbase.api.impl.RPCHBaseService.createPut','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.setWriteToWAL'
'org.mule.module.hbase.api.impl.RPCHBaseService.createDelete','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.Delete.deleteFamily'
'org.mule.module.hbase.api.impl.RPCHBaseService.createHTable','org.apache.hadoop.hbase.client.HTableInterfaceFactory.createHTableInterface'
'org.mule.module.hbase.api.impl.RPCHBaseService.createHBaseAdmin','org.apache.hadoop.hbase.client.HBaseAdmin.<init>'
'org.mule.module.hbase.api.impl.RPCHBaseService.destroyHBaseAdmin','org.apache.hadoop.hbase.client.HBaseAdmin.getConfiguration org.apache.hadoop.hbase.client.HConnectionManager.deleteConnection'
'org.mule.module.hbase.api.impl.RPCHBaseService.doWithHTable','org.apache.hadoop.hbase.client.HTableInterface.close'
'org.mule.module.hbase.api.impl.RPCHBaseServiceTestDriver.testRow','org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.isEmpty'
'org.mule.module.hbase.api.impl.RPCHBaseServiceTestDriver.testScanRow','org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.list'
'org.mule.module.hbase.api.impl.RPCHBaseServiceTestDriver.testCheckOperations','org.apache.hadoop.hbase.client.Result.getColumnLatest'
'com.cloudera.hadoop.hdfs.nfs.rpc.RPCServer.shutdown','org.apache.hadoop.io.IOUtils.closeSocket'
'com.inadco.ecoadapters.r.RReducer.reduce','org.apache.hadoop.io.Text.toString'
'org.apache.nutch.parse.rss.RSSParser.main','org.apache.hadoop.io.Text.<init>'
'com.nexr.rhive.hive.udf.RUDAF.getEvaluator','org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.init','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector org.apache.hadoop.io.Text.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.iterate','org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getString org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getString org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getDouble org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.terminatePartial','org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.merge','org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.terminate','org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.getConnection','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.loadExportedRScript','org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.tranformR2Hive','org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.handleList','org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDAF.GenericRUDAF.handleVector','org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDF.evaluate','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter.convert org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter.convert org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter.convert org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter.convert org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hive.serde2.io.DoubleWritable.<init> org.apache.hadoop.hive.ql.metadata.HiveException.<init> org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDF.initialize','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.<init> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getCategory org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveCategory org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName'
'com.nexr.rhive.hive.udf.RUDF.loadExportedRScript','org.apache.hadoop.hive.ql.metadata.HiveException.<init>'
'com.nexr.rhive.hive.udf.RUDF.getConnection','org.apache.hadoop.hive.ql.exec.UDFArgumentException.<init>'
'org.godhuli.rhipe.RXLineRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.util.LineReader.readLine'
'org.godhuli.rhipe.RXLineRecordReader.nextKeyValue','org.apache.hadoop.util.LineReader.readLine'
'org.godhuli.rhipe.RXLineRecordReader.close','org.apache.hadoop.util.LineReader.close'
'org.godhuli.rhipe.RXTextInputFormat.isSplitable','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'org.godhuli.rhipe.RXTextInputFormat.createRecordReader','org.apache.hadoop.mapreduce.InputSplit.toString org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus'
'org.godhuli.rhipe.RXTextOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'org.apache.hama.examples.RandBench.RandBSP.bsp','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.getBytes'
'org.apache.accumulo.examples.simple.client.RandomBatchWriter.createMutation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'skywriting.examples.skyhout.kmeans.RandomClusterSequenceFileGenerator.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.RawLocalFileSystem.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.setConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.set'
'org.apache.mahout.clustering.kmeans.RandomSeedGenerator.buildRandom','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.kmeans.RandomSeedGenerator.buildRandom','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Text.<init>'
'com.hadoopilluminated.examples.RandomTextWriter.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'com.hadoopilluminated.examples.RandomTextWriter.Map.configure','org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt'
'com.hadoopilluminated.examples.RandomTextWriter.Map.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.setStatus'
'com.hadoopilluminated.examples.RandomTextWriter.Map.generateSentence','org.apache.hadoop.io.Text.<init>'
'com.hadoopilluminated.examples.RandomTextWriter.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob'
'com.hadoopilluminated.examples.RandomTextWriter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.RandomTextWriter.<init> org.apache.hadoop.util.ToolRunner.run'
'skywriting.examples.skyhout.kmeans.RandomVectorSequenceFileGenerator.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.RawLocalFileSystem.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.setConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.set'
'org.apache.giraph.examples.RandomWalkWorkerContext.initializeSources','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.open'
'org.apache.giraph.examples.RandomWalkWorkerContext.preApplication','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getFloat'
'com.hadoopilluminated.examples.RandomWriter.RandomInputFormat.getSplits','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileSplit.<init>'
'com.hadoopilluminated.examples.RandomWriter.RandomInputFormat.RandomRecordReader.next','org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.Text.set'
'com.hadoopilluminated.examples.RandomWriter.RandomInputFormat.RandomRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'com.hadoopilluminated.examples.RandomWriter.RandomInputFormat.RandomRecordReader.createValue','org.apache.hadoop.io.Text.<init>'
'com.hadoopilluminated.examples.RandomWriter.RandomInputFormat.getRecordReader','org.apache.hadoop.mapred.FileSplit.getPath'
'com.hadoopilluminated.examples.RandomWriter.Map.map','org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.setStatus'
'com.hadoopilluminated.examples.RandomWriter.Map.configure','org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt'
'com.hadoopilluminated.examples.RandomWriter.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob'
'com.hadoopilluminated.examples.RandomWriter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.nexr.rhive.hive.udf.RangeKeyUDF.initialize','org.apache.hadoop.hive.ql.session.SessionState.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.ql.session.SessionState.getConf org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.<init> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.getTypeName'
'com.nexr.rhive.hive.udf.RangeKeyUDF.evaluate','org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.getPrimitiveJavaObject'
'org.apache.accumulo.core.client.mapreduce.lib.partition.RangePartitioner.findPartition','org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.client.mapreduce.lib.partition.RangePartitioner.getNumSubBins','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.accumulo.core.client.mapreduce.lib.partition.RangePartitioner.getCutPoints','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.mapreduce.lib.partition.RangePartitioner.setSplitFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.accumulo.core.client.mapreduce.lib.partition.RangePartitioner.setNumSubBins','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'edu.umd.cloud9.example.pagerank.RangePartitioner.getPartition','org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.example.pagerank.RangePartitioner.configure','org.apache.hadoop.conf.Configuration.getInt'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.RankAndCrawlStatsJoinStep.Phase1Mapper.map','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.RankAndCrawlStatsJoinStep.Phase2Reducer.reduce','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.RankAndCrawlStatsJoinStep.runStep','org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobClient.runJob'
'com.m6d.hiveudf.RankTest.testRank','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'edu.jhu.thrax.hadoop.features.mapred.RarityPenaltyFeature.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.NullWritable.get'
'org.sifarish.social.RatingDifference.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.sifarish.social.RatingDifference.DiffMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set'
'org.sifarish.social.RatingDifference.DiffReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'com.lightboxtechnologies.spectrum.RawFileInputFormat.RawFileRecordReader.getCurrentKey','org.apache.hadoop.io.NullWritable.get'
'fm.last.feathers.output.RawFileOutputFormat.RawFileRecordWriter.readRawBytes','org.apache.hadoop.io.BytesWritable.getBytes'
'fm.last.feathers.output.RawFileOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create'
'org.apache.misc.RawRecordFilter.main','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.hbql.client.HConnectionManager.newConnection org.apache.hadoop.hbase.hbql.client.HConnection.execute org.apache.hadoop.hbase.hbql.client.HConnection.getMapping org.apache.hadoop.hbase.hbql.mapping.TableMapping.newRecordFilter org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'org.commoncrawl.util.RawRecordReader.RawRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getStart'
'backtype.cascading.scheme.RawSequenceFile.sourceConfInit','org.apache.hadoop.mapred.JobConf.setInputFormat'
'org.goldenorb.io.input.RawSplit.setBytes','org.apache.hadoop.io.BytesWritable.set'
'org.goldenorb.io.input.RawSplit.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.BytesWritable.readFields org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Text.readString'
'org.goldenorb.io.input.RawSplit.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.BytesWritable.write org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.Text.writeString'
'edu.ucla.sspace.hadoop.RawTextCooccurrenceMapper.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.ReRunXCommand.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.ReRunXCommand.eagerLoadState','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getStringCollection'
'org.apache.oozie.command.wf.ReRunXCommand.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.ReRunXCommand.eagerLoadState','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getStringCollection'
'tv.floe.caduceus.hadoop.compression.lzo.readLZOFile.ReadLZOFileExampleJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setCompressMapOutput org.apache.hadoop.mapred.TextOutputFormat.setOutputCompressorClass org.apache.hadoop.mapred.TextOutputFormat.setCompressOutput org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'tv.floe.caduceus.hadoop.compression.lzo.readLZOFile.ReadLZOFileExampleJob.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'tv.floe.caduceus.hadoop.compression.lzo.readLZOFile.ReadLZOFileExampleJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'tv.floe.caduceus.hadoop.compression.lzo.readLZOFile.ReadLZOFileMapper.map','org.apache.hadoop.io.Text.toString'
'ivory.core.util.ReadSequenceFile.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'ivory.core.util.ReadSequenceFile.readSequenceFile','org.apache.hadoop.fs.FileSystem.getConf'
'ivory.core.util.ReadSequenceFile.readSequenceFilesInDir','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'edu.umd.cloud9.io.ReadSequenceFile.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'edu.umd.cloud9.io.ReadSequenceFile.readSequenceFile','org.apache.hadoop.fs.FileSystem.getConf'
'edu.umd.cloud9.io.ReadSequenceFile.readSequenceFilesInDir','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.hcatalog.utils.ReadWrite.Map.map','org.apache.hadoop.io.Text.<init>'
'org.apache.hcatalog.utils.ReadWrite.run','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.hcatalog.utils.ReadWrite.main','org.apache.hadoop.util.ToolRunner.run'
'org.springframework.data.hadoop.batch.ReadWriteHdfsTest.testWorkflow','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getClass org.apache.hadoop.fs.FileSystem.exists'
'org.springframework.data.hadoop.batch.ReadWriteHdfsTest.testWorkflowNS','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.run','org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.setIOSort','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest.testItemIDIndexMapper','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest.testToItemPrefsMapper','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest.testToItemPrefsMapperBooleanData','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest.testSimilarityMatrixRowWrapperMapper','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest.testItemFilterMapper','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest.testCompleteJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest.testCompleteJobBoolean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest.testCompleteJobWithFiltering','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.conf.Configuration.getInt'
'org.hackreduce.examples.RecordCounter.RecordCounterReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'org.hackreduce.examples.RecordCounter.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.hackreduce.examples.bixi.RecordCounter.configureJob','org.apache.hadoop.mapreduce.Job.setInputFormatClass'
'org.hackreduce.examples.bixi.RecordCounter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.hackreduce.examples.ngram.one_gram.RecordCounter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.hackreduce.examples.flights.RecordCounter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'tap.formats.record.RecordFormat.setupOutput','org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass'
'tap.formats.record.RecordFormat.setupInput','org.apache.hadoop.mapred.JobConf.setInputFormat'
'org.apache.flume.channel.recoverable.memory.RecoverableMemoryChannelEvent.write','org.apache.hadoop.io.MapWritable.write'
'org.apache.flume.channel.recoverable.memory.RecoverableMemoryChannelEvent.readFields','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.MapWritable.readFields'
'org.apache.flume.channel.recoverable.memory.RecoverableMemoryChannelEvent.toMapWritable','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put'
'org.apache.flume.channel.recoverable.memory.RecoverableMemoryChannelEvent.fromMapWritable','org.apache.hadoop.io.MapWritable.entrySet'
'org.apache.oozie.service.RecoveryService.RecoveryRunnable.runBundleRecovery','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.RecoveryService.init','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.RecoveryService.mergeConfig','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.duke.starfish.jobopt.rrs.RecursiveRandomSearch.RecursiveRandomSearch','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat'
'com.hadoopilluminated.ch01.Reduce21.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'edu.duke.starfish.whatif.oracle.ReduceProfileOracle.initializeCommonVariables','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'edu.duke.starfish.whatif.oracle.ReduceProfileOracle.calcVirtualReduceCountersShuffleSortPhase','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getFloat'
'ReduceSideJoin.ReduceSideJoinDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.MultipleInputs.addInputPath org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'ReduceSideJoin.ReduceSideJoinUserLogsMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'ScoreFriends.Job1.ReduceSideJoin_1_UserScoreMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'ReduceSideJoin.ReduceSideReducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Reducer.Context.write'
'ScoreFriends.Job1.ReduceSide_1_Reducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Reducer.Context.write'
'shark.operators.ReduceSinkWrapper.initEvaluatorsAndReturnStruct','org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initEvaluatorsAndReturnStruct'
'com.manning.hip.ch7.shortestpath.Reduce.setup','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration'
'com.manning.hip.ch7.shortestpath.Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Mapper.Context.getCounter org.apache.hadoop.mapreduce.v2.api.records.Counter.increment org.apache.hadoop.mapreduce.Mapper.Context.getCounter'
'com.manning.hip.ch7.pagerank.Reduce.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'massive.logs.Reduce.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'org.freeeed.main.Reduce.reduce','org.apache.hadoop.io.MD5Hash.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Reducer.Context.write'
'org.freeeed.main.Reduce.processMap','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'org.freeeed.main.Reduce.setup','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.freeeed.main.Reduce.cleanup','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.freeeed.main.Reduce.getAllMetadata','org.apache.hadoop.io.MapWritable.keySet org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.Text.toString'
'step2.Reducer2.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'step5.Reducer5.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'tap.core.ReducerBridge.configure','org.apache.hadoop.mapred.JobConf.getOutputFormat org.apache.hadoop.mapred.JobConf.getOutputFormat org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getOutputFormat org.apache.hadoop.mapred.JobConf.getOutputKeyClass org.apache.hadoop.mapred.JobConf.getOutputValueClass org.apache.hadoop.mapred.lib.MultipleOutputs.addMultiNamedOutput org.apache.hadoop.mapred.lib.MultipleOutputs.<init>'
'tap.core.ReducerBridge.getReducer','org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'tap.core.ReducerBridge.ReduceCollector._collect','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector.collect org.apache.hadoop.mapred.OutputCollector.collect org.apache.hadoop.mapred.OutputCollector.collect org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector.collect'
'tap.core.ReducerBridge.ReduceCollector.collect','org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector'
'tap.core.ReducerBridge.close','org.apache.hadoop.mapred.lib.MultipleOutputs.close'
'extramuros.java.jobs.stats.dispersion.Reducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'extramuros.java.jobs.clustering.validation.daviesbouldin.Reducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.springframework.data.hadoop.hbase.Reducer.run','org.apache.hadoop.conf.Configuration.get'
'org.apache.yaoql.impl.ReflectionResultAccessor.ReflectionResultAccessor','org.apache.hadoop.hbase.hbql.mapping.MappingContext.<init>'
'org.commoncrawl.mapred.pipelineV3.RegExFilter.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getLong'
'org.commoncrawl.mapred.pipelineV3.RegExFilter.run','org.apache.hadoop.mapred.RecordReader<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.createKey org.apache.hadoop.mapred.RecordReader<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.createValue org.apache.hadoop.mapred.RecordReader<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.next org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.apache.mahout.utils.regex.RegexConverterDriver.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.utils.regex.RegexConverterDriver.main','org.apache.hadoop.util.ToolRunner.run'
'.RegexExcludePathFilter.accept','org.apache.hadoop.fs.Path.toString'
'org.apache.flume.sink.hbase.RegexHbaseEventSerializer.getActions','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'hadoop.RegexParserMap.configure','org.apache.hadoop.mapred.JobConf.get'
'hadoop.RegexParserMap.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'.RegexPathFilter.accept','org.apache.hadoop.fs.Path.toString'
'org.apache.nutch.urlfilter.api.RegexURLFilterBase.setConf','org.apache.hadoop.conf.Configuration.getConfResourceAsReader'
'org.hbase.tdg.RegionObserverExample.preGet','org.apache.hadoop.hbase.client.Get.getRow org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.client.Get.getRow org.apache.hadoop.hbase.KeyValue.<init>'
'coprocessor.RegionObserverExample.preGet','org.apache.hadoop.hbase.client.Get.getRow org.apache.hadoop.hbase.util.Bytes.toStringBinary org.apache.hadoop.hbase.client.Get.getRow org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.client.Get.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.KeyValue.<init>'
'org.apache.nutch.microformats.reltag.RelTagQueryFilter.setConf','org.apache.hadoop.conf.Configuration.getFloat'
'edu.isi.mavuno.util.Relation.Relation','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.beymani.proximity.RelativeDensity.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.beymani.proximity.RelativeDensity.DensityMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'org.beymani.proximity.RelativeDensity.DensityReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean'
'org.beymani.proximity.RelativeDensity.DensityReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'org.beymani.proximity.RelativeDensity.main','org.apache.hadoop.util.ToolRunner.run'
'org.styloot.maryjane.RemoteLocation.RemoteLocation','org.apache.hadoop.fs.Path.<init>'
'com.bah.culvert.adapter.RemotingIterator.RemotingIterator','org.apache.hadoop.conf.Configuration.<init>'
'com.bah.culvert.adapter.RemotingIterator.hasNext','org.apache.hadoop.conf.Configuration.get'
'com.bah.culvert.adapter.RemotingIterator.next','org.apache.hadoop.conf.Configuration.get'
'org.apache.accumulo.server.util.RemoveEntriesForMissingFiles.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'edu.umd.cloud9.collection.wikipedia.RepackWikipedia.MyMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobConf.get'
'edu.umd.cloud9.collection.wikipedia.RepackWikipedia.MyMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.collection.wikipedia.WikipediaPage>.collect'
'edu.umd.cloud9.collection.wikipedia.RepackWikipedia.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapred.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapred.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.collection.wikipedia.RepackWikipedia.main','org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.sqoop.testutil.ReparseMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.cloudera.sqoop.testutil.ReparseMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.collect org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.megalon.ReplServer.applyChanges','org.apache.hadoop.hbase.client.HTableInterface.batch'
'org.lilyproject.repository.impl.RepositoryMetrics.RepositoryMetrics','org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.<init> org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.<init> org.apache.hadoop.metrics.MetricsUtil.getContext org.apache.hadoop.metrics.MetricsUtil.createRecord org.apache.hadoop.metrics.MetricsContext.registerUpdater'
'org.lilyproject.repository.impl.RepositoryMetrics.shutdown','org.apache.hadoop.metrics.MetricsContext.unregisterUpdater'
'org.lilyproject.repository.impl.RepositoryMetrics.doUpdates','org.apache.hadoop.metrics.util.MetricsRegistry.getMetricsList org.apache.hadoop.metrics.util.MetricsBase.pushMetric org.apache.hadoop.metrics.MetricsRecord.update'
'org.apache.nutch.tools.compat.ReprUrlFixer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.io.MapWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.MapWritable.remove org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum>.collect'
'org.apache.nutch.tools.compat.ReprUrlFixer.update','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.compat.ReprUrlFixer.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.tools.compat.ReprUrlFixer.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.mahout.clustering.evaluation.RepresentativePointsDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.evaluation.RepresentativePointsDriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.evaluation.RepresentativePointsDriver.writeInitialState','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.evaluation.RepresentativePointsDriver.runIterationSeq','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.evaluation.RepresentativePointsDriver.runIterationMR','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.clustering.evaluation.RepresentativePointsMapper.cleanup','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.evaluation.RepresentativePointsMapper.mapPoint','org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.clustering.evaluation.RepresentativePointsMapper.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.clustering.evaluation.RepresentativePointsMapper.getRepresentativePoints','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.evaluation.RepresentativePointsReducer.cleanup','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.evaluation.RepresentativePointsReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.apache.giraph.comm.RequestFailureTest.setUp','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration'
'org.apache.giraph.comm.RequestFailureTest.getRequest','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.giraph.comm.RequestFailureTest.checkResult','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'org.apache.giraph.comm.netty.handler.RequestServerHandler.RequestServerHandler','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt'
'org.apache.giraph.comm.RequestTest.setUp','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration'
'org.apache.giraph.comm.RequestTest.sendVertexPartition','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.giraph.comm.RequestTest.sendWorkerMessagesRequest','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'org.apache.giraph.comm.RequestTest.sendPartitionMutationsRequest','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.setInputFormat','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setInputFormatClass'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.setNumSamples','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.setMaxRecordsToRead','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.setUseSamplesNumberPerInputSplit','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.getNumSamples','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.getMaxRecordsToRead','org.apache.hadoop.conf.Configuration.getInt'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.getInputFormat','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.InputFormat<UNRESOLVED.K,UNRESOLVED.V>.getSplits'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.InputFormat<UNRESOLVED.K,UNRESOLVED.V>.createRecordReader'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.ReservoirSamplerRecordReader.ReservoirSamplerRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.ReservoirSamplerRecordReader.initialize','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.initialize org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.getCurrentValue org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.io.WritableUtils.clone'
'com.manning.hip.ch4.sampler.ReservoirSamplerInputFormat.ReservoirSamplerRecordReader.close','org.apache.hadoop.mapreduce.RecordReader<UNRESOLVED.K,UNRESOLVED.V>.close'
'org.goldenorb.util.ResourceAllocator.assignResources','org.apache.hadoop.mapred.InvalidJobConfException.<init>'
'org.goldenorb.util.ResourceAllocatorTest.testEnoughCapacityWithPPM','org.apache.hadoop.mapred.InvalidJobConfException.printStackTrace'
'org.goldenorb.util.ResourceAllocatorTest.testEnoughCapacity','org.apache.hadoop.mapred.InvalidJobConfException.printStackTrace'
'org.goldenorb.util.ResourceAllocatorTest.testUnbalancedAssignment','org.apache.hadoop.mapred.InvalidJobConfException.printStackTrace'
'org.goldenorb.util.ResourceAllocatorTest.insufficientCapacity','org.apache.hadoop.mapred.InvalidJobConfException.printStackTrace'
'com.hbasebook.hush.ResourceManager.ResourceManager','org.apache.hadoop.hbase.client.HTablePool.<init>'
'com.hbasebook.hush.ResourceManager.getTable','org.apache.hadoop.hbase.client.HTablePool.getTable'
'com.hbasebook.hush.ResourceManager.putTable','org.apache.hadoop.hbase.client.HTablePool.putTable'
'org.apache.giraph.comm.netty.handler.ResponseClientHandler.ResponseClientHandler','org.apache.hadoop.conf.Configuration.getBoolean'
'org.cloudata.core.tablet.backup.RestoreJob.runRestore','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMaxMapAttempts org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.cloudata.core.tablet.backup.RestoreJob.RestoreTextInputFormat.getSplits','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init>'
'org.cloudata.core.tablet.backup.RestoreMap.map','org.apache.hadoop.io.Text.toString'
'org.cloudata.core.tablet.backup.RestoreMap.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.core.tablet.backup.RestorePartitionMap.map','org.apache.hadoop.io.Text.toString'
'org.cloudata.core.tablet.backup.RestorePartitionMap.close','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'com.urbanairship.datacube.ResultComparator.compare','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow'
'com.bah.culvert.constraints.filter.ResultFilter.readFields','org.apache.hadoop.io.ObjectWritable.readFields org.apache.hadoop.io.ObjectWritable.get org.apache.hadoop.io.ObjectWritable.readFields org.apache.hadoop.io.ObjectWritable.get'
'com.bah.culvert.constraints.filter.ResultFilter.write','org.apache.hadoop.io.ObjectWritable.set org.apache.hadoop.io.ObjectWritable.write org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.write'
'com.asakusafw.runtime.flow.ResultOutput.add','org.apache.hadoop.mapreduce.RecordWriter<java.lang.Object,java.lang.Object>.write'
'com.asakusafw.runtime.flow.ResultOutput.getKey','org.apache.hadoop.io.NullWritable.get'
'com.asakusafw.runtime.flow.ResultOutput.close','org.apache.hadoop.mapreduce.Counter.increment org.apache.hadoop.mapreduce.RecordWriter<java.lang.Object,java.lang.Object>.close'
'org.pentaho.hadoop.mapreduce.converter.converters.ResultPassThroughConverter.canConvert','org.apache.hadoop.hbase.client.Result.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.ResultPassThroughConverter.convert','org.apache.hadoop.hbase.client.Result.<init> org.apache.hadoop.hbase.util.Writables.copyWritable'
'org.apache.sqoop.util.ResultSetPrinter.printResultSet','org.apache.hadoop.util.StringUtils.stringifyException'
'ivory.core.util.ResultWriter.ResultWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'edu.ucsc.srl.damasc.netcdf.io.Result.write','org.apache.hadoop.io.Text.writeString'
'edu.ucsc.srl.damasc.netcdf.io.Result.readFields','org.apache.hadoop.io.Text.readString'
'ivory.core.RetrievalEnvironment.RetrievalEnvironment','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'ivory.core.RetrievalEnvironment.initialize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'ivory.core.RetrievalEnvironment.createPath','org.apache.hadoop.fs.Path.<init>'
'ivory.core.RetrievalEnvironment.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'com.bah.culvert.constraints.RetrieveColumns.readFields','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.ObjectWritable.setConf org.apache.hadoop.io.ObjectWritable.readFields org.apache.hadoop.io.ObjectWritable.get'
'com.bah.culvert.constraints.RetrieveColumns.write','org.apache.hadoop.io.ObjectWritable.<init> org.apache.hadoop.io.ObjectWritable.write'
'org.apache.accumulo.examples.simple.shard.Reverse.main','org.apache.hadoop.io.Text.<init>'
'org.apache.hcatalog.hbase.snapshot.RevisionManagerConfiguration.addResources','org.apache.hadoop.conf.Configuration.addDefaultResource org.apache.hadoop.conf.Configuration.addResource'
'org.apache.hcatalog.hbase.snapshot.RevisionManagerConfiguration.create','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.HBaseConfiguration.merge'
'org.apache.hcatalog.hbase.snapshot.RevisionManagerEndpointClient.open','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.coprocessorProxy'
'com.basho.riak.hadoop.config.RiakConfig.addLocation','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.basho.riak.hadoop.config.RiakConfig.getRiakLocatons','org.apache.hadoop.conf.Configuration.get'
'com.basho.riak.hadoop.config.RiakConfig.setHadoopClusterSize','org.apache.hadoop.conf.Configuration.setInt'
'com.basho.riak.hadoop.config.RiakConfig.getHadoopClusterSize','org.apache.hadoop.conf.Configuration.getInt'
'com.basho.riak.hadoop.config.RiakConfig.getKeyLister','org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.get'
'com.basho.riak.hadoop.config.RiakConfig.setKeyLister','org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setStrings'
'com.basho.riak.hadoop.config.RiakConfig.getOutputBucket','org.apache.hadoop.conf.Configuration.get'
'com.basho.riak.hadoop.config.RiakConfig.setOutputBucket','org.apache.hadoop.conf.Configuration.set'
'com.basho.riak.hadoop.RiakInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.basho.riak.hadoop.RiakRecordWriter.RiakRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.basho.riak.hadoop.RiakRecordWriter.write','org.apache.hadoop.io.Text.toString'
'org.commoncrawl.service.crawler.RobotRulesParser.setConf','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'ivory.regression.basic.Robust04_Basic.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.basic.Robust04_Basic_LCE.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.basic.Robust04_NonPositional_Baselines.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'com.sematext.hbase.hut.RollbackUpdatesMrJob.RollbackUpdatesMapper.map','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Delete.<init>'
'com.sematext.hbase.hut.RollbackUpdatesMrJob.createSubmittableJob','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.client.Scan.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'com.sematext.hbase.hut.RollbackUpdatesMrJob.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.datasalt.pangool.tuplemr.mapred.RollupReducer.indexMismatch','org.apache.hadoop.io.RawComparator<java.lang.Object>.compare'
'org.apache.accumulo.core.client.impl.RootTabletLocator.locateTablet','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append'
'com.atlantbh.jmeter.plugins.hbasecomponents.utils.Row2XML.row2xmlstring','org.apache.hadoop.hbase.client.Result.getNoVersionMap org.apache.hadoop.hbase.client.Result.getFamilyMap'
'com.atlantbh.jmeter.plugins.hbasecomponents.utils.Row2XML.row2xmlStringLatest','org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.KeyValue.getTimestamp'
'coprocessor.RowCountEndpoint.getCount','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.getRegion org.apache.hadoop.hbase.regionserver.InternalScanner.next org.apache.hadoop.hbase.regionserver.InternalScanner.close'
'coprocessor.RowCountEndpoint.getRowCount','org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.<init>'
'com.ning.metrics.action.hdfs.data.RowFileContentsIteratorFactory.build','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.toUri'
'org.apache.accumulo.core.iterators.user.RowFilter.skipRows','org.apache.hadoop.io.Text.equals'
'org.apache.mahout.utils.vectors.RowIdJob.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'org.apache.mahout.utils.vectors.RowIdJob.main','org.apache.hadoop.util.ToolRunner.run'
'.RowKeyConverter.makeObservationRowKey','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.putBytes org.apache.hadoop.hbase.util.Bytes.putLong'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.before','org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.HBaseTestingUtility.startMiniZKCluster org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.createTable'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.after','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniZKCluster'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.testGet','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getValue'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.testSimpleScanBounded','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.<init>'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.testSimpleScanUnbounded','org.apache.hadoop.hbase.client.Scan.<init>'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.testMapReduceBounded','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.<init>'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.testMapReduceUnbounded','org.apache.hadoop.hbase.client.Scan.<init>'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.writeTestData','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.testSimpleScanInternal','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.compareTo org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toInt'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.testMapReduceInternal','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'com.sematext.hbase.wd.RowKeyDistributorTestBase.RowCounterMapper.map','org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.KeyValue.getValue'
'com.facebook.tsdb.tsdash.server.data.hbase.RowKey.baseTsFromRowKey','org.apache.hadoop.hbase.util.Bytes.toInt'
'org.lilyproject.repository.impl.lock.RowLock.createRowLock','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'org.lilyproject.repository.impl.lock.RowLock.getTimestamp','org.apache.hadoop.hbase.util.Bytes.toLong'
'org.lilyproject.repository.impl.lock.test.RowLockerTest.setUpBeforeClass','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTable.<init>'
'org.lilyproject.repository.impl.lock.test.RowLockerTest.testLockUnlock','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.repository.impl.lock.test.RowLockerTest.testLockTwice','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.repository.impl.lock.test.RowLockerTest.testLockTimesOut','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.repository.impl.lock.test.RowLockerTest.testPut','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.repository.impl.lock.test.RowLockerTest.testPutLockOtherRow','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.lilyproject.rowlog.impl.test.RowLogLocalEndToEndTest.testMultipleSubscriptions','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.rowlog.impl.test.RowLogLocalEndToEndTest.testMultipleSubscriptionsOrder','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.rowlog.impl.test.RowLogShardTest.testSingleMessage','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.rowlog.impl.test.RowLogShardTest.testMultipleMessages','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.rowlog.impl.test.RowLogShardTest.testBatchSize','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.rowlog.impl.test.RowLogShardTest.testMultipleConsumers','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.rowlog.impl.test.RowLogShardTest.testMessageDoesNotExistForConsumer','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.rowlog.impl.test.RowLogShardTest.testProblematicMessage','org.apache.hadoop.hbase.util.Bytes.toBytes'
'nl.waredingen.graphs.misc.RowNumberJob.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'nl.waredingen.graphs.misc.RowNumberJob.RowNumberMapper.setup','org.apache.hadoop.io.ByteWritable.set'
'nl.waredingen.graphs.misc.RowNumberJob.RowNumberMapper.cleanup','org.apache.hadoop.io.ByteWritable.set'
'nl.waredingen.graphs.misc.RowNumberJob.RowNumberReducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.apache.accumulo.examples.simple.client.RowOperations.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.run','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.VectorNormMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.VectorNormMapper.cleanup','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.MergeVectorsReducer.setup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.MergeVectorsReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.CooccurrencesMapper.setup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.CooccurrencesMapper.map','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.SimilarityReducer.setup','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.SimilarityReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.UnsymmetrifyMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'com.ning.metrics.action.hdfs.data.RowSmile.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt'
'com.ning.metrics.action.hdfs.data.RowSmile.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'com.ning.metrics.action.hdfs.data.RowText.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'com.ning.metrics.action.hdfs.data.RowText.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'com.ning.metrics.action.hdfs.data.RowThrift.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'com.ning.metrics.action.hdfs.data.RowThrift.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'com.odiago.flumebase.testutil.RtsqlTestCase.setUp','org.apache.hadoop.conf.Configuration.<init>'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.RuleWritable','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.set','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.DoubleWritable.write'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.DoubleWritable.readFields'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.sourceAlignmentArray','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.targetAlignmentArray','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.sameYield','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.equals','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.DoubleWritable.equals'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.hashCode','org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.DoubleWritable.hashCode'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.toString','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.compareTo','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.DoubleWritable.compareTo'
'edu.jhu.thrax.hadoop.datatypes.RuleWritable.YieldAndAlignmentComparator.compare','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize'
'org.apache.accumulo.server.test.scalability.Run.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'ivory.smrf.retrieval.distributed.RunDistributedRetrievalServers.ServerMapper.HeartbeatRunnable.run','org.apache.hadoop.mapred.Reporter.incrCounter'
'ivory.smrf.retrieval.distributed.RunDistributedRetrievalServers.ServerMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.get'
'ivory.smrf.retrieval.distributed.RunDistributedRetrievalServers.ServerMapper.map','org.apache.hadoop.io.Text.toString'
'ivory.smrf.retrieval.distributed.RunDistributedRetrievalServers.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.smrf.retrieval.distributed.RunDistributedRetrievalServers.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'ivory.smrf.retrieval.distributed.RunDistributedRetrievalServers.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.MapClass.map','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.MapWithInMapperCombiningClass.cleanup','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.CombineClass.reduce','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.set'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.ReduceClass.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.ReduceClass.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.FloatWritable.get'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.ReduceClass.cleanup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeFloat org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.MapPageRankMassDistributionClass.setup','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getInt'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.phase1','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configurable.setConf org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Writable>.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Partitioner<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Writable>.getPartition org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readFloat org.apache.hadoop.fs.FSDataInputStream.close'
'edu.umd.cloud9.example.pagerank.RunPageRankSchimmy.phase2','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'ivory.smrf.retrieval.RunQueryLocal.RunQueryLocal','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.cascade.retrieval.RunQueryLocalCascade.RunQueryLocalCascade','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.huahinframework.manager.queue.RunQueue.call','org.apache.hadoop.util.ToolRunner.run'
'test.modelgen.table.model.RunningJobflows.setBatchId','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.RunningJobflows.setJobflowId','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.RunningJobflows.setTargetName','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.RunningJobflows.setExecutionId','org.apache.hadoop.io.Text.modify'
'org.godhuli.rhipe.S2B.runme','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.commoncrawl.util.S3ArcFileReader.run','org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.util.S3ArcFileReader.contentAvailable','org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.hadoop.io.S3GetMetdataJob.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.hadoop.io.S3GetMetdataJob.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.TaskAttemptID.forName org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get'
'org.commoncrawl.hadoop.io.S3GetMetdataJob.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.commoncrawl.protocol.CrawlURLMetadata>.collect'
'org.commoncrawl.hadoop.io.S3GetMetdataJob.run','org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.shared.ArcFileItem>.createKey org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.shared.ArcFileItem>.createValue org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.shared.ArcFileItem>.next org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.TaskAttemptID.getId org.apache.hadoop.mapred.TaskAttemptID.getId org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.TaskAttemptID.getId org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'com.urbanairship.hbackup.S3Test.multipartHdfsToS3Test','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.urbanairship.hbackup.S3Test.hdfsToS3MtimeTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.livingsocial.hive.udf.SMax.SMaxStringEvaluator.iterate','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.cloudera.sqoop.manager.SQLServerManagerExportManualTest.setUp','org.apache.hadoop.conf.Configuration.setStrings'
'com.cloudera.sqoop.manager.SQLServerManagerImportManualTest.setUp','org.apache.hadoop.conf.Configuration.setStrings'
'com.cloudera.sqoop.manager.SQLServerManagerImportManualTest.doImportAndVerify','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.IOUtils.closeStream'
'com.github.srec.hadoop.SRecTestNGMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.NullWritable.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.rename'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.sniffInputLabelType','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.compare','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.loadDistributedRowMatrix','org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.loadUpperTriangularMatrix','org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDTestsHelper.generateDenseInput','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'org.apache.mahout.math.hadoop.stochasticsvd.SSVDTestsHelper.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set'
'com.cloudera.seismic.crunch.SUPipeline.main','org.apache.hadoop.util.ToolRunner.run'
'semvec.mahout.SVDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'uk.co.mrry.mercator.mapreduce.SWLineRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'uk.co.mrry.mercator.mapreduce.SWLineRecordReader.nextKeyValue','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.set'
'uk.co.mrry.mercator.mapreduce.SWLineRecordWriter.writeObject','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.getLength'
'uk.co.mrry.mercator.mapreduce.SWMapEntryPoint.invoke','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.Mapper.run'
'uk.co.mrry.mercator.mapreduce.SWMapperOutputCollector.SWMapperOutputCollector','org.apache.hadoop.io.serializer.WritableSerialization.<init>'
'uk.co.mrry.mercator.mapreduce.SWMapperOutputCollector.write','org.apache.hadoop.mapreduce.Partitioner<uk.co.mrry.mercator.mapreduce.K,uk.co.mrry.mercator.mapreduce.V>.getPartition'
'uk.co.mrry.mercator.mapreduce.SWMapperOutputCollector.close','org.apache.hadoop.io.serializer.WritableSerialization.getSerializer org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.io.Writable>.open org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.io.Writable>.serialize org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.io.Writable>.serialize org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.io.Writable>.close'
'uk.co.mrry.mercator.mapreduce.SWReduceInputMerger.SWReduceInputMerger','org.apache.hadoop.io.serializer.WritableSerialization.<init> org.apache.hadoop.io.WritableComparator.get'
'uk.co.mrry.mercator.mapreduce.SWReduceInputMerger.fetchFromStream','org.apache.hadoop.io.serializer.WritableSerialization.getDeserializer org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.io.Writable>.open org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.io.Writable>.deserialize org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.io.Writable>.deserialize'
'com.asakusafw.example.direct.seqfile.writable.SalesDetailWritable.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'com.asakusafw.example.direct.seqfile.writable.SalesDetailWritable.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.sifarish.feature.SameTypeSimilarity.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.sifarish.feature.SameTypeSimilarity.main','org.apache.hadoop.util.ToolRunner.run'
'org.sifarish.feature.SameTypeSimilarity.SimilarityMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.sifarish.feature.SameTypeSimilarity.SimilarityMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'org.sifarish.feature.SameTypeSimilarity.SimilarityReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean'
'org.sifarish.feature.SameTypeSimilarity.SimilarityReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'org.goldenorb.types.message.SampleBooleanMessageTest.SampleBooleanMessageTest','org.apache.hadoop.io.BooleanWritable.<init>'
'org.goldenorb.types.message.SampleBooleanMessageTest.startServer','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.ipc.RPC.waitForProxy'
'org.goldenorb.types.message.SampleBooleanMessageTest.testRPC','org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.io.BooleanWritable.get'
'edu.duke.starfish.whatif.junit.SampleDataSetModel.generateMapInputSpecs','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getBoolean'
'org.commoncrawl.hadoop.template.SampleHadoopJob.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.hadoop.template.SampleHadoopJob.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.TaskAttemptID.forName org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt'
'org.commoncrawl.hadoop.template.SampleHadoopJob.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.hadoop.template.SampleHadoopJob.run','org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.shared.ArcFileItem>.createKey org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.shared.ArcFileItem>.createValue org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.shared.ArcFileItem>.next org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.TaskAttemptID.getId org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.mapred.TaskAttemptID.getId org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.commoncrawl.protocol.shared.ArcFileItem>.close'
'ivory.lsh.eval.SampleIntDocVectors.MyMapper.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.util.LineReader.close'
'ivory.lsh.eval.SampleIntDocVectors.MyMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,ivory.core.data.document.WeightedIntDocVector>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,ivory.core.data.document.WeightedIntDocVector>.collect'
'ivory.lsh.eval.SampleIntDocVectors.MyReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,ivory.core.data.document.WeightedIntDocVector>.collect'
'ivory.lsh.eval.SampleIntDocVectors.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'ivory.lsh.eval.SampleIntDocVectors.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.manning.hip.ch6.SampleJob.runSortJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.setTaskOutputFilter org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.JobClient.monitorAndPrintJob org.apache.hadoop.mapred.TaskTracker.RunningJob.getFailureInfo'
'org.apache.pig.impl.builtin.SampleLoader.skipNext','org.apache.hadoop.mapreduce.RecordReader<?,?>.nextKeyValue'
'com.manning.hip.ch4.sort.total.SampleMapReduce.runSortJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch4.sort.total.SampleMapReduce.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch4.sort.total.SampleMapReduce.Reduce.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.apache.oozie.example.SampleMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.collect'
'edu.duke.starfish.whatif.junit.SampleProfiles.getTeraSortConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'edu.duke.starfish.whatif.junit.SampleProfiles.getWordCountConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.example.SampleReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.collect'
'org.sleuthkit.web.sampleapp.server.SampleServiceImpl.getData','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.setConf org.apache.hadoop.mapred.JobClient.getAllJobs org.apache.hadoop.mapred.JobStatus.getJobID org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.getJobName'
'org.sleuthkit.web.sampleapp.server.SampleServiceImpl.processJob','org.apache.hadoop.mapred.RunningJob.getJobState org.apache.hadoop.mapred.RunningJob.getJobState'
'org.sleuthkit.web.sampleapp.server.SampleServiceImpl.setRowData','org.apache.hadoop.mapred.JobStatus.getStartTime org.apache.hadoop.mapred.JobStatus.mapProgress org.apache.hadoop.mapred.JobStatus.reduceProgress'
'ivory.lsh.eval.SampleSignatures.MyMapper.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.util.LineReader.close'
'ivory.lsh.eval.SampleSignatures.MyMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,ivory.lsh.data.Signature>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,ivory.lsh.data.Signature>.collect'
'ivory.lsh.eval.SampleSignatures.MyReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,ivory.lsh.data.Signature>.collect'
'ivory.lsh.eval.SampleSignatures.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'ivory.lsh.eval.SampleSignatures.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'ivory.lsh.eval.SampleTermDocVectors.MyMapper.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.util.LineReader.close'
'ivory.lsh.eval.SampleTermDocVectors.MyMapper.map','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.map.HMapSFW>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.map.HMapSFW>.collect'
'ivory.lsh.eval.SampleTermDocVectors.MyReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,edu.umd.cloud9.io.map.HMapSFW>.collect'
'ivory.lsh.eval.SampleTermDocVectors.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.getJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'ivory.lsh.eval.SampleTermDocVectors.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.goldenorb.types.message.SampleTextMessageTest.SampleTextMessageTest','org.apache.hadoop.io.Text.<init>'
'org.goldenorb.types.message.SampleTextMessageTest.startServer','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.ipc.RPC.waitForProxy'
'org.goldenorb.types.message.SampleTextMessageTest.testRPC','org.apache.hadoop.io.Text.<init>'
'gr.ntua.cslab.distributed.sampler.SamplerJob.runSampler','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'gr.ntua.cslab.distributed.sampler.SamplerJob.getCuts','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init>'
'gr.ntua.cslab.distributed.sampler.SamplerJob.clean','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete'
'gr.ntua.cslab.distributed.sampler.SamplerMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'gr.ntua.cslab.distributed.sampler.SamplerMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<gr.ntua.cslab.data.TupleWritable,org.apache.hadoop.io.IntWritable>.collect'
'gr.ntua.cslab.distributed.sampler.SamplerMapper.close','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<gr.ntua.cslab.data.TupleWritable,org.apache.hadoop.io.IntWritable>.collect'
'de.jungblut.bsp.SamplingSort.bsp','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'de.jungblut.bsp.SamplingSort.PartitionMessage.write','org.apache.hadoop.io.IntWritable.write'
'de.jungblut.bsp.SamplingSort.PartitionMessage.readFields','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.readFields'
'de.jungblut.bsp.SamplingSort.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'de.jungblut.bsp.SamplingSort.printOutput','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'de.jungblut.bsp.SamplingSort.generateInput','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.IntWritable.set'
'org.apache.giraph.comm.netty.handler.SaslClientHandler.decode','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.comm.netty.SaslNettyServer.SaslNettyServer','org.apache.hadoop.mapreduce.security.token.JobTokenSecretManager.checkAvailableForRead'
'org.apache.giraph.comm.netty.SaslNettyServer.SaslDigestCallbackHandler.handle','org.apache.hadoop.mapreduce.security.token.JobTokenSecretManager.retrievePassword org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier.getUser'
'edu.umd.cloud9.collection.clue.ScanBlockCompressedSequenceFile.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.fs.FileSystem.get'
'meetup.beeno.ScanByIndex.createScanner','org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner'
'meetup.beeno.ScanByIndex.getIndexScanner','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.Scan.setStopRow org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner'
'meetup.beeno.ScanByIndex.addIndexFilters','org.apache.hadoop.hbase.filter.PageFilter.<init> org.apache.hadoop.hbase.filter.FilterList.<init>'
'meetup.beeno.ScanByIndex.IndexScannerWrapper.close','org.apache.hadoop.hbase.client.ResultScanner.close'
'meetup.beeno.ScanByIndex.IndexScannerWrapper.next','org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.HTable.get'
'client.ScanCacheBatchExample.scan','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setBatch org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.close'
'client.ScanCacheBatchExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init>'
'org.apache.accumulo.server.test.functional.ScanIteratorTest.run','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.cloudata.examples.weblink.ScanJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob'
'org.cloudata.examples.weblink.ScanJob.ScanJobMapper.configure','org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.examples.weblink.ScanJob.ScanJobMapper.close','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.cloudata.examples.weblink.ScanJob.ScanJobReducer.close','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.apache.accumulo.server.test.randomwalk.image.ScanMeta.visit','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.ScanRangeTest.createCF','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.ScanRangeTest.createCQ','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.ScanRangeTest.createRow','org.apache.hadoop.io.Text.<init>'
'client.ScanTimeoutExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.close'
'com.nearinfinity.hbase.dsl.Scanner.Scanner','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.<init>'
'com.nearinfinity.hbase.dsl.Scanner.iterator','org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator'
'com.nearinfinity.hbase.dsl.Scanner.setFilter','org.apache.hadoop.hbase.client.Scan.setFilter'
'com.nearinfinity.hbase.dsl.Scanner.addFamily','org.apache.hadoop.hbase.client.Scan.addFamily'
'com.nearinfinity.hbase.dsl.Scanner.addColumn','org.apache.hadoop.hbase.client.Scan.addColumn'
'com.nearinfinity.hbase.dsl.Scanner.setTimestamp','org.apache.hadoop.hbase.client.Scan.setTimeStamp'
'com.nearinfinity.hbase.dsl.Scanner.setTimeRange','org.apache.hadoop.hbase.client.Scan.setTimeRange'
'com.nearinfinity.hbase.dsl.Scanner.allVersions','org.apache.hadoop.hbase.client.Scan.setMaxVersions'
'org.lilyproject.hbaseindex.ScannerQueryResult.next','org.apache.hadoop.hbase.client.ResultScanner.next'
'org.lilyproject.hbaseindex.ScannerQueryResult.close','org.apache.hadoop.hbase.client.ResultScanner.close'
'org.apache.oozie.service.SchemaService.loadSchema','org.apache.hadoop.conf.Configuration.getStrings'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.ArraySummaryNode.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.ArraySummaryNode.readFields','org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.BooleanSummaryNode.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.BooleanSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.BytesSummaryNode.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.BytesSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.DoubleSummaryNode.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.DoubleSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.EnumSummaryNode.write','org.apache.hadoop.io.UTF8.writeString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.EnumSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.FixedSummaryNode.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.FixedSummaryNode.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.FloatSummaryNode.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.FloatSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.IntegerSummaryNode.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.IntegerSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.LongSummaryNode.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.LongSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.MapSummaryNode.write','org.apache.hadoop.io.UTF8.writeString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.MapSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.NullSummaryNode.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.NullSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.RecordSummaryNode.write','org.apache.hadoop.io.UTF8.writeString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.RecordSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.StringSummaryNode.write','org.apache.hadoop.io.UTF8.writeString org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.StringSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.UTF8.readString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.UnionSummaryNode.write','org.apache.hadoop.io.UTF8.writeString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.UnionSummaryNode.readFields','org.apache.hadoop.io.UTF8.readString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.write','org.apache.hadoop.io.UTF8.writeString'
'com.cloudera.recordbreaker.schemadict.SchemaStatisticalSummary.readFields','org.apache.hadoop.io.UTF8.readString'
'edu.umd.cloud9.io.SchemaTest.testWritableFields','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init>'
'com.linkedin.haivvreo.SchemaToTypeInfo.initTypeMap','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo'
'com.linkedin.haivvreo.SchemaToTypeInfo.generateRecordTypeInfo','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo'
'com.linkedin.haivvreo.SchemaToTypeInfo.generateMapTypeInfo','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getMapTypeInfo'
'com.linkedin.haivvreo.SchemaToTypeInfo.generateArrayTypeInfo','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getListTypeInfo'
'com.linkedin.haivvreo.SchemaToTypeInfo.generateUnionTypeInfo','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getUnionTypeInfo'
'com.linkedin.haivvreo.SchemaToTypeInfo.generateEnumTypeInfo','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo'
'org.apache.pig.data.SchemaTupleFrontend.SchemaTupleFrontendGenHelper.internalCopyAllGeneratedToDistributedCache','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.filecache.DistributedCache.createSymlink org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.toString org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.data.SchemaTupleFrontend.SchemaTupleFrontendGenHelper.generateAll','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.pig.data.SchemaTupleFrontend.copyAllGeneratedToDistributedCache','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.ScoreLeftDistribution','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.getScore','org.apache.hadoop.io.DoubleWritable.get'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.clone','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.readFields','org.apache.hadoop.io.DoubleWritable.readFields'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.write','org.apache.hadoop.io.DoubleWritable.write'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.toString','org.apache.hadoop.io.DoubleWritable.toString'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.fromString','org.apache.hadoop.io.DoubleWritable.set'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.hashCode','org.apache.hadoop.io.DoubleWritable.hashCode'
'fr.insarennes.fafdti.builder.ScoreLeftDistribution.equals','org.apache.hadoop.io.DoubleWritable.equals'
'co.nubetech.hiho.similarity.ngram.ScoreReducer.reduce','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.nutch.scoring.ScoringFilters.ScoringFilters','org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.scoring.ScoringFilters.ScoringFilters','org.apache.hadoop.conf.Configuration.get'
'org.bradheintz.travsales.ScoringMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.bradheintz.travsales.ScoringMapper.setup','org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.harioca.shell.ScriptExecutorTest.testExecuteScript','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.pig.test.utils.ScriptSchemaTestLoader.getSchema','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.tools.pigstats.ScriptState.addSettingsToConf','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.tools.pigstats.ScriptState.getHadoopVersion','org.apache.hadoop.util.VersionInfo.getVersion'
'org.apache.pig.tools.pigstats.ScriptState.setPigFeature','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.tools.pigstats.ScriptState.setJobParents','org.apache.hadoop.conf.Configuration.set'
'org.springframework.data.hadoop.scripting.ScriptingTest.testRhinoHadoopScript','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get'
'.Search.PageRankMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'.Search.PageRankReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'.Search.GetPageRank','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion'
'de.tuberlin.dima.aim3.assignment3.SearchAsMatrixVectorMultiplicationTest.search','org.apache.hadoop.conf.Configuration.<init>'
'de.tuberlin.dima.aim3.assignment3.SearchAsMatrixVectorMultiplicationTest.readResult','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.NullWritable.get'
'.SearchEmail.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getBytes'
'.SearchEmail.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobClient.runJob'
'.SearchEmail.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.tools.SearchLoadTester.testSearch','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.tools.SearchLoadTester.main','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.accumulo.server.test.randomwalk.shard.Search.visit','org.apache.hadoop.io.Text.<init>'
'org.apache.jena.tdbloader4.SecondDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.jena.tdbloader4.SecondDriver.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SecondaryKeyPartitioner.setConf','org.apache.hadoop.conf.Configuration.get'
'org.fusesource.fabric.hadoop.hdfs.SecondaryNameNodeFactory.doCreate','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init> org.apache.hadoop.util.Daemon.<init> org.apache.hadoop.util.Daemon.start'
'org.fusesource.fabric.hadoop.hdfs.SecondaryNameNodeFactory.doDelete','org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shutdown'
'de.tuberlin.dima.aim3.assignment1.SecondarySortBookSort.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'de.tuberlin.dima.aim3.assignment1.SecondarySortBookSort.SecondarySortBookSortReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get'
'SecondarySort.SecondarySortDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'SecondarySort.SecondarySortReducer.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapreduce.Reducer.Context.write'
'ScoreFriends.Job2.SecondarySort_2_UserFriendMapper.map','org.apache.hadoop.io.Text.toString'
'com.inmobi.databus.utils.SecureLoginUtil.login','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.security.SecurityUtil.login org.apache.hadoop.security.UserGroupInformation.getLoginUser'
'org.apache.hcatalog.templeton.SecureProxySupport.SecureProxySupport','org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled'
'org.apache.hcatalog.templeton.SecureProxySupport.open','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.security.token.Token.<init> org.apache.hadoop.security.token.Token<?>.decodeFromUrlString org.apache.hadoop.io.Text.<init> org.apache.hadoop.security.token.Token<?>.setService'
'org.apache.hcatalog.templeton.SecureProxySupport.close','org.apache.hadoop.fs.Path.toUri'
'org.apache.hcatalog.templeton.SecureProxySupport.addEnv','org.apache.hadoop.fs.Path.toUri'
'org.apache.hcatalog.templeton.SecureProxySupport.getFSDelegationToken','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.hcatalog.templeton.SecureProxySupport.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.security.UserGroupInformation.getShortUserName org.apache.hadoop.fs.FileSystem.getDelegationToken org.apache.hadoop.security.Credentials.<init> org.apache.hadoop.security.token.Token<?>.getService org.apache.hadoop.security.Credentials.addToken org.apache.hadoop.security.token.Token<?>.getService org.apache.hadoop.security.Credentials.addToken org.apache.hadoop.security.Credentials.writeTokenStorageFile org.apache.hadoop.security.UserGroupInformation.getUserName org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDelegationToken'
'org.apache.hcatalog.templeton.SecureProxySupport.writeProxyDelegationTokens','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.hcatalog.templeton.SecureProxySupport.buildHcatDelegationToken','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init> org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.doAs'
'org.apache.accumulo.server.test.randomwalk.security.SecurityHelper.getFs','org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.math.hadoop.similarity.SeedVectorUtil.loadSeedVectors','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Writable.getClass'
'org.apache.nutch.tools.proxy.SegmentHandler.SegmentPathFilter.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.nutch.tools.proxy.SegmentHandler.Segment.getReaders','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.Path.toString'
'org.apache.nutch.tools.proxy.SegmentHandler.Segment.getEntry','org.apache.hadoop.mapred.MapFileOutputFormat.getEntry'
'org.apache.nutch.tools.proxy.SegmentHandler.SegmentHandler','org.apache.hadoop.fs.FileSystem.get'
'org.apache.nutch.tools.proxy.SegmentHandler.handle','org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.service.crawler.SegmentLoader.loadCrawlSegmentFPInfo','org.apache.hadoop.io.WritableName.setName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.commoncrawl.service.crawler.SegmentLoader.loadCrawlSegment','org.apache.hadoop.io.WritableName.setName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.commoncrawl.service.crawler.SegmentLoader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource'
'org.apache.nutch.segment.SegmentMerger.ObjectInputFormat.getRecordReader','org.apache.hadoop.mapred.InputSplit.toString org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.SequenceFileRecordReader<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.<init>'
'org.apache.nutch.segment.SegmentMerger.SegmentOutputFormat.getRecordWriter','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.<init>'
'org.apache.nutch.segment.SegmentMerger.SegmentOutputFormat.ensureSequenceFile','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileOutputFormat.getOutputCompressionType org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.nutch.segment.SegmentMerger.SegmentOutputFormat.ensureMapFile','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileOutputFormat.getOutputCompressionType org.apache.hadoop.fs.Path.toString'
'org.apache.nutch.segment.SegmentMerger.setConf','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getLong'
'org.apache.nutch.segment.SegmentMerger.configure','org.apache.hadoop.mapred.JobConf.getNumReduceTasks'
'org.apache.nutch.segment.SegmentMerger.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect'
'org.apache.nutch.segment.SegmentMerger.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect'
'org.apache.nutch.segment.SegmentMerger.merge','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.segment.SegmentMerger.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.segment.SegmentMerger.ObjectInputFormat.getRecordReader','org.apache.hadoop.mapred.InputSplit.toString org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.SequenceFileRecordReader<org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable>.<init> org.apache.hadoop.mapred.SequenceFileRecordReader<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.<init>'
'org.apache.nutch.segment.SegmentMerger.ObjectInputFormat.next','org.apache.hadoop.mapred.SequenceFileRecordReader<org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable>.next'
'org.apache.nutch.segment.SegmentMerger.ObjectInputFormat.close','org.apache.hadoop.mapred.SequenceFileRecordReader<org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable>.close'
'org.apache.nutch.segment.SegmentMerger.SegmentOutputFormat.getRecordWriter','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.<init>'
'org.apache.nutch.segment.SegmentMerger.SegmentOutputFormat.ensureSequenceFile','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileOutputFormat.getOutputCompressionType org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.nutch.segment.SegmentMerger.SegmentOutputFormat.ensureMapFile','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileOutputFormat.getOutputCompressionType org.apache.hadoop.fs.Path.toString'
'org.apache.nutch.segment.SegmentMerger.setConf','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getLong'
'org.apache.nutch.segment.SegmentMerger.configure','org.apache.hadoop.mapred.JobConf.getNumReduceTasks'
'org.apache.nutch.segment.SegmentMerger.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect'
'org.apache.nutch.segment.SegmentMerger.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper>.collect'
'org.apache.nutch.segment.SegmentMerger.merge','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.segment.SegmentMerger.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.segmenter.SegmentMover.findLatestDatabaseTimestamp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.commoncrawl.mapred.segmenter.SegmentMover.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename'
'org.apache.nutch.segment.SegmentPart.get','org.apache.hadoop.mapred.FileSplit.getPath'
'org.apache.nutch.segment.SegmentReader.InputCompatMapper.map','org.apache.hadoop.io.WritableComparable.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect'
'org.apache.nutch.segment.SegmentReader.TextOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.<init>'
'org.apache.nutch.segment.SegmentReader.SegmentReader','org.apache.hadoop.fs.FileSystem.get'
'org.apache.nutch.segment.SegmentReader.configure','org.apache.hadoop.fs.FileSystem.get'
'org.apache.nutch.segment.SegmentReader.createJobConf','org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean'
'org.apache.nutch.segment.SegmentReader.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.apache.nutch.segment.SegmentReader.dump','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.segment.SegmentReader.append','org.apache.hadoop.fs.FileSystem.open'
'org.apache.nutch.segment.SegmentReader.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.segment.SegmentReader.getMapRecords','org.apache.hadoop.mapred.MapFileOutputFormat.getReaders'
'org.apache.nutch.segment.SegmentReader.getSeqRecords','org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders org.apache.hadoop.io.Writable.equals'
'org.apache.nutch.segment.SegmentReader.list','org.apache.hadoop.fs.Path.getName'
'org.apache.nutch.segment.SegmentReader.getStats','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.mapred.MapFileOutputFormat.getReaders org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.mapred.MapFileOutputFormat.getReaders'
'org.apache.nutch.segment.SegmentReader.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.mapred.segmenter.Segmenter.generateCrawlSegments','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.rename'
'com.cloudera.seismic.segy.SegyLoader.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter'
'com.cloudera.seismic.segy.SegyLoader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.seismic.segy.SegyUnloader.write','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'com.cloudera.seismic.segy.SegyUnloader.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.cloudera.seismic.segy.SegyUnloader.accept','org.apache.hadoop.fs.Path.getName'
'com.cloudera.seismic.segy.SegyUnloader.main','org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mains.SelectRandomTaggedNodes.addCategories','org.apache.hadoop.hbase.client.Get.<init>'
'com.senseidb.indexing.hadoop.map.SenseiMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<com.senseidb.indexing.hadoop.keyvalueformat.Shard,com.senseidb.indexing.hadoop.keyvalueformat.IntermediateForm>.collect'
'com.senseidb.indexing.hadoop.map.SenseiMapper.configure','org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapred.JobConf.getClass org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.senseidb.indexing.hadoop.map.SenseiMapper.setAnalyzer','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.senseidb.indexing.hadoop.map.SenseiMapper.setSchema','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.get'
'com.zinnia.nectar.regression.hadoop.primitive.jobs.SeparateColumnsJob.call','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.crunch.io.seq.SeqFileHelper.newInstance','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.cloudera.sqoop.testutil.SeqFileReader.getSeqFileReader','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.testutil.SeqFileReader.getFirstValue','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.Reader.getKeyClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.SequenceFile.Reader.getValueClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.SequenceFile.Reader.getValueClassName org.apache.hadoop.io.SequenceFile.Reader.next org.apache.hadoop.io.SequenceFile.Reader.getCurrentValue org.apache.hadoop.io.SequenceFile.Reader.close'
'org.apache.crunch.io.seq.SeqFileReaderFactory.SeqFileReaderFactory','org.apache.hadoop.io.NullWritable.get'
'org.apache.crunch.io.seq.SeqFileSource.read','org.apache.hadoop.fs.FileSystem.get'
'org.apache.crunch.io.seq.SeqFileSourceTarget.SeqFileSourceTarget','org.apache.hadoop.fs.Path.<init>'
'hipi.imagebundle.SeqImageBundle.openForWrite','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter'
'hipi.imagebundle.SeqImageBundle.openForRead','org.apache.hadoop.fs.FileSystem.get'
'hipi.imagebundle.SeqImageBundle.addImage','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.BytesWritable.<init>'
'hipi.imagebundle.SeqImageBundle.prepareNext','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.BytesWritable.getBytes'
'hipi.unittest.SeqImageBundleTestCase.createImageBundleAndOpen','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.flume.handlers.hdfs.SeqfileEventSink.open','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.RawSequenceFileWriter.createWriter org.apache.hadoop.io.FlushingSequenceFileWriter.createWriter'
'com.cloudera.flume.handlers.hdfs.SeqfileEventSink.open','org.apache.hadoop.io.FlushingSequenceFileWriter.createWriter'
'com.cloudera.flume.handlers.hdfs.SeqfileEventSource.open','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'com.cloudera.seismic.su.SequenceFileCallback.SequenceFileCallback','org.apache.hadoop.io.BytesWritable.<init>'
'com.cloudera.seismic.su.SequenceFileCallback.write','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.NullWritable.get'
'com.digitalpebble.behemoth.io.sequencefile.SequenceFileConverterJob.main','org.apache.hadoop.util.ToolRunner.run'
'com.digitalpebble.behemoth.io.sequencefile.SequenceFileConverterJob.run','org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.digitalpebble.behemoth.io.sequencefile.SequenceFileConverterMapper.map','org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Writable.write org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Text.<init>'
'edu.jhu.thrax.util.SequenceFileCreator.main','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.set'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator.SequenceFileDirIterator','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator.apply','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator.SequenceFileDirIterator','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator.apply','org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.SequenceFileDirValueIterator','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.apply','org.apache.hadoop.fs.FileStatus.getPath'
'com.mozilla.hadoop.fs.SequenceFileDirectoryReader.SequenceFileDirectoryReader','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.getName'
'com.mozilla.hadoop.fs.SequenceFileDirectoryReader.close','org.apache.hadoop.fs.FileSystem.close'
'com.lightboxtechnologies.spectrum.SequenceFileExport.SequenceFileExportMapper.SequenceFileExportMapper','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put'
'com.lightboxtechnologies.spectrum.SequenceFileExport.SequenceFileExportMapper.setup','org.apache.hadoop.conf.Configuration.getStringCollection'
'com.lightboxtechnologies.spectrum.SequenceFileExport.SequenceFileExportMapper.encodeHex','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'com.lightboxtechnologies.spectrum.SequenceFileExport.SequenceFileExportMapper.map','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.set'
'com.lightboxtechnologies.spectrum.SequenceFileExport.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.sqoop.mapreduce.SequenceFileImportMapper.setup','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getWorkOutputPath'
'backtype.hadoop.formats.SequenceFileInputStream.SequenceFileInputStream','org.apache.hadoop.fs.FileSystem.getConf'
'backtype.hadoop.formats.SequenceFileInputStream.readRawRecord','org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable.iterator','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.core.iterators.system.SequenceFileIterator.next','org.apache.hadoop.io.SequenceFile.Reader.next org.apache.hadoop.io.SequenceFile.Reader.next'
'org.apache.accumulo.core.iterators.system.SequenceFileIterator.close','org.apache.hadoop.io.SequenceFile.Reader.close'
'pl.edu.icm.coansys.commons.hbase.SequenceFileKeysSampler.createParitionFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner.setPartitionFile org.apache.hadoop.mapreduce.lib.partition.InputSampler.writePartitionFile'
'pl.edu.icm.coansys.commons.hbase.SequenceFileKeysSampler.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.flume.collector.SequenceFileMerger.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.loadModel','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.loadWeightMatrix','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.loadFeatureWeights','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.loadLabelWeights','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.loadThetaNormalizer','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.loadSumWeight','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.readLabelSums','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.readLabelDocumentCounts','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.readSigmaJSigmaK','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.mahout.classifier.bayes.io.SequenceFileModelReader.readVocabCount','org.apache.hadoop.io.DoubleWritable.get'
'com.cloudera.flume.handlers.seqfile.SequenceFileOutputFormat.SequenceFileOutputFormat','org.apache.hadoop.io.SequenceFile.getCompressionType org.apache.hadoop.io.compress.DefaultCodec.<init>'
'com.cloudera.flume.handlers.seqfile.SequenceFileOutputFormat.format','org.apache.hadoop.fs.FSDataOutputStream.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.Writer.append'
'com.cloudera.flume.handlers.seqfile.SequenceFileOutputFormat.build','org.apache.hadoop.io.SequenceFile.getCompressionType'
'backtype.hadoop.formats.SequenceFileOutputStream.SequenceFileOutputStream','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.SequenceFile.createWriter'
'backtype.hadoop.formats.SequenceFileOutputStream.writeRaw','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.NullWritable.get'
'com.inadco.ecoadapters.pig.SequenceFileProtobufLoader.SequenceFileProtobufLoader','org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.<init>'
'com.inadco.ecoadapters.pig.SequenceFileProtobufLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'com.inadco.ecoadapters.pig.SequenceFileProtobufLoader.getNext','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.getCurrentValue org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength'
'com.inadco.ecoadapters.pig.SequenceFileProtobufStorage.SequenceFileProtobufStorage','org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.<init>'
'com.inadco.ecoadapters.pig.SequenceFileProtobufStorage.setStoreLocation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass'
'com.inadco.ecoadapters.pig.SequenceFileProtobufStorage.putNext','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.write'
'org.commoncrawl.hadoop.mergeutils.SequenceFileReader.SequenceFileReader','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.hadoop.mergeutils.SequenceFileReader.readAndSpill','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.avro.mapred.SequenceFileRecordReader.SequenceFileRecordReader','org.apache.hadoop.mapred.FileSplit.getPath'
'org.apache.hama.bsp.SequenceFileRecordReader.SequenceFileRecordReader','org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.hama.bsp.SequenceFileRecordReader.createKey','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.hama.bsp.SequenceFileRecordReader.createValue','org.apache.hadoop.util.ReflectionUtils.newInstance'
'de.jungblut.crawl.SequenceFileResultWriter.open','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'de.jungblut.crawl.SequenceFileResultWriter.write','org.apache.hadoop.io.Text.<init>'
'de.jungblut.crawl.SequenceFileResultWriter.getOutputPath','org.apache.hadoop.fs.Path.<init>'
'de.jungblut.crawl.SequenceFileResultWriter.asText','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'org.commoncrawl.hadoop.mergeutils.SequenceFileSpillWriter.SequenceFileSpillWriter','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.createWriter'
'org.commoncrawl.hadoop.mergeutils.SequenceFileSpillWriter.run','org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.hadoop.mergeutils.SequenceFileSpillWriter.close','org.apache.hadoop.fs.FSDataOutputStream.close'
'org.commoncrawl.hadoop.mergeutils.SequenceFileSpillWriter.spillRawRecord2','org.apache.hadoop.io.SequenceFile.ValueBytes.<init>'
'pl.edu.icm.coansys.commons.hbase.SequenceFileSplitAlgorithm.split','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.BytesWritable.copyBytes org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.seqfile.SequenceFileStockWriter.main','org.apache.hadoop.fs.Path.<init>'
'com.manning.hip.ch3.seqfile.SequenceFileStockWriter.write','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.compress.DefaultCodec.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'com.inadco.ecoadapters.pig.SequenceFileStorage.SequenceFileStorage','org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.<init>'
'com.inadco.ecoadapters.pig.SequenceFileStorage.setStoreLocation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass'
'com.inadco.ecoadapters.pig.SequenceFileStorage.putNext','org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable>.write'
'com.twitter.elephantbird.pig.store.SequenceFileStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.fs.Path.<init>'
'com.twitter.elephantbird.pig.store.SequenceFileStorage.ensureUDFContext','org.apache.hadoop.conf.Configuration.get'
'com.twitter.elephantbird.pig.store.SequenceFileStorage.setCompression','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.io.compress.CompressionCodec.getClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput'
'com.twitter.elephantbird.pig.store.SequenceFileStorage.getOutputFormat','org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat<com.twitter.elephantbird.pig.store.K,com.twitter.elephantbird.pig.store.V>.<init>'
'edu.isi.mavuno.app.util.SequenceFileToText.MyMapper.map','org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Text.set'
'edu.isi.mavuno.app.util.SequenceFileToText.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.isi.mavuno.app.util.SequenceFileToText.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.manning.hip.ch11.SequenceFileTupleLoader.getNext','org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader<org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable>.nextKeyValue org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader<org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable>.getCurrentValue'
'com.manning.hip.ch11.SequenceFileTupleLoader.getInputFormat','org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat<org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable>.<init>'
'com.manning.hip.ch11.SequenceFileTupleLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'com.manning.hip.ch11.SequenceFileTupleStoreFunc.getOutputFormat','org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.<init>'
'com.manning.hip.ch11.SequenceFileTupleStoreFunc.setStoreLocation','org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'com.manning.hip.ch11.SequenceFileTupleStoreFunc.putNext','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.RecordWriter.write'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtil.openReader','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtil.openWriter','org.apache.hadoop.fs.FSDataOutputStream.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.createWriter'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtil.InputStreamFileSystem.open','org.apache.hadoop.fs.FSDataInputStream.<init>'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.LocalFileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.LocalFileSystem.setWorkingDirectory'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.tearDown','org.apache.hadoop.fs.LocalFileSystem.setWorkingDirectory'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.read','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.clear org.apache.hadoop.fs.LocalFileSystem.getFileStatus org.apache.hadoop.fs.LocalFileSystem.pathToFile org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.read_new','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.clear org.apache.hadoop.fs.LocalFileSystem.getFileStatus org.apache.hadoop.fs.LocalFileSystem.pathToFile org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.original_large','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.read_large','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.set org.apache.hadoop.fs.LocalFileSystem.getFileStatus org.apache.hadoop.fs.LocalFileSystem.pathToFile org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.write','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.LocalFileSystem.pathToFile org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.write_large','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.fs.LocalFileSystem.pathToFile org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get'
'com.asakusafw.runtime.io.sequencefile.SequenceFileUtilTest.write_compressed','org.apache.hadoop.io.compress.DefaultCodec.<init> org.apache.hadoop.io.compress.DefaultCodec.setConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.fs.LocalFileSystem.pathToFile org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get'
'eu.scape_project.tb.lsdr.seqfileutility.SequenceFileUtility.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.io.SequenceFileUtils.readFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getConf'
'edu.umd.cloud9.io.SequenceFileUtils.readFileIntoMap','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'edu.umd.cloud9.io.SequenceFileUtils.readDirectory','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'edu.umd.cloud9.io.SequenceFileUtils.readKeys','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getConf'
'edu.umd.cloud9.io.SequenceFileUtils.readValues','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getConf'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator.SequenceFileValueIterator','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator.computeNext','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.mahout.utils.vectors.io.SequenceFileVectorWriter.write','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.utils.vectors.io.SequenceFileVectorWriter.write','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'co.nubetech.hiho.testdata.SequenceFileWriteDemo.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.getClass org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.IOUtils.closeStream'
'.SequenceFileWriteDemo.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.getClass org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.IOUtils.closeStream'
'eu.scape_project.tb.lsdr.seqfileutility.SequenceFileWriter.SequenceFileWriter','org.apache.hadoop.conf.Configuration.<init>'
'eu.scape_project.tb.lsdr.seqfileutility.SequenceFileWriter.writeTextLines','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'eu.scape_project.tb.lsdr.seqfileutility.SequenceFileWriter.writeFileContent','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.BytesWritable.set'
'eu.scape_project.tb.lsdr.seqfileutility.SequenceFileWriter.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.mahout.text.SequenceFilesFromCsvFilter.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.text.SequenceFilesFromCsvFilter.process','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.text.SequenceFilesFromDirectory.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.text.SequenceFilesFromDirectory.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.mahout.text.SequenceFilesFromDirectoryFilter.SequenceFilesFromDirectoryFilter','org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.text.SequenceFilesFromDirectoryFilter.accept','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.text.SequenceFilesFromDirectory.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.text.SequenceFilesFromDirectory.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.text.SequenceFilesFromMailArchives.createSequenceFiles','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.text.SequenceFilesFromMailArchives.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.text.SequenceFilesFromMailArchivesTest.testMain','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.text.SequenceFilesFromMailArchives.ChunkedWriter.ChunkedWriter','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.mahout.text.SequenceFilesFromMailArchives.ChunkedWriter.getPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.text.SequenceFilesFromMailArchives.ChunkedWriter.write','org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getBytes'
'org.sleuthkit.hadoop.SequenceFsEntryText.runPipeline','org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'com.smartitengineering.cms.spi.impl.workspace.SequenceObjectConverter.getPutForTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.smartitengineering.cms.spi.impl.workspace.SequenceObjectConverter.rowsToObject','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong'
'org.softlang.test.Serialization.createAndWriteCompany','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.createWriter'
'com.digitalpebble.behemoth.SerializationTest.setUp','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'com.digitalpebble.behemoth.SerializationTest.tearDown','org.apache.hadoop.fs.FileSystem.close'
'com.digitalpebble.behemoth.SerializationTest.testSerialization','org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Writer.append org.apache.hadoop.io.SequenceFile.Writer.close org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Reader.next org.apache.hadoop.io.SequenceFile.Reader.close org.apache.hadoop.fs.FileSystem.delete'
'cascading.tuple.hadoop.SerializedPipesPlatformTest.InsertBytes.operate','org.apache.hadoop.io.BytesWritable.<init>'
'cascading.tuple.hadoop.SerializedPipesPlatformTest.ReplaceAsBytes.operate','org.apache.hadoop.io.BytesWritable.<init>'
'cascading.tuple.hadoop.SerializedPipesPlatformTest.InsertBoolean.operate','org.apache.hadoop.io.BooleanWritable.<init>'
'cascading.tuple.hadoop.SerializedPipesPlatformTest.testSimpleGroup','org.apache.hadoop.io.BooleanWritable.getName'
'org.apache.hcatalog.templeton.Server.verifyUser','org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled'
'org.apache.accumulo.server.ServerConstants.getInstanceIdLocation','org.apache.hadoop.fs.Path.<init>'
'org.apache.accumulo.server.ServerConstants.getDataVersionLocation','org.apache.hadoop.fs.Path.<init>'
'com.odiago.flumebase.server.ServerMain.ServerMain','org.apache.hadoop.conf.Configuration.<init>'
'com.odiago.flumebase.server.ServerMain.run','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.accumulo.server.data.ServerMutation.readFields','org.apache.hadoop.io.WritableUtils.readVLong'
'org.apache.accumulo.server.data.ServerMutation.write','org.apache.hadoop.io.WritableUtils.writeVLong'
'org.apache.accumulo.server.data.ServerMutationTest.test','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.ReflectionUtils.copy'
'org.apache.hcatalog.templeton.Server.verifyUser','org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled'
'org.styloot.maryjane.thriftserver.Server.Server','org.apache.hadoop.fs.Path.<init>'
'org.styloot.maryjane.thriftserver.Server.readFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read'
'org.styloot.maryjane.thriftserver.Server.getPathRoot','org.apache.hadoop.fs.Path.getParent'
'org.styloot.maryjane.thriftserver.Server.main','org.apache.hadoop.fs.Path.<init>'
'com.jointhegrid.hive_test.ServiceHive.ServiceHive','org.apache.hadoop.hive.service.HiveClient.<init>'
'org.apache.oozie.service.Services.Services','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.Services.init','org.apache.hadoop.conf.Configuration.getClasses org.apache.hadoop.conf.Configuration.getClasses org.apache.hadoop.util.VersionInfo.getVersion'
'org.apache.oozie.service.Services.destroy','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.service.Services.setServiceInternal','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.server.ServletLauncher.service','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.server.ServletRegistry.loadServlet','org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.hadoop.hdfs.nfs.nfs4.Session.Session','org.apache.hadoop.fs.FileSystem.get'
'org.visitante.basic.SessionExtractor.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.visitante.basic.SessionExtractor.SessionMapper.map','org.apache.hadoop.io.Text.toString'
'org.visitante.basic.SessionExtractor.SessionReducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'org.visitante.basic.SessionExtractor.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.memcached.demo.SetLogProbInMemcached.MyMapper.configure','org.apache.hadoop.mapred.JobConf.get'
'edu.umd.cloud9.example.memcached.demo.SetLogProbInMemcached.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.Text.toString'
'edu.umd.cloud9.example.memcached.demo.SetLogProbInMemcached.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobClient.runJob'
'com.cloudera.hadoop.hdfs.nfs.nfs4.attrs.SetModifyTimeHandler.set','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getAccessTime org.apache.hadoop.fs.FileSystem.setTimes org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getAccessTime org.apache.hadoop.fs.FileSystem.setTimes'
'org.apache.accumulo.server.test.randomwalk.bulk.Setup.visit','org.apache.hadoop.fs.FileSystem.get'
'com.senseidb.indexing.hadoop.keyvalueformat.Shard.setIndexShards','org.apache.hadoop.conf.Configuration.set'
'com.senseidb.indexing.hadoop.keyvalueformat.Shard.getIndexShards','org.apache.hadoop.conf.Configuration.get'
'com.senseidb.indexing.hadoop.keyvalueformat.Shard.write','org.apache.hadoop.io.Text.writeString'
'com.senseidb.indexing.hadoop.keyvalueformat.Shard.readFields','org.apache.hadoop.io.Text.readString'
'org.apache.accumulo.server.test.randomwalk.shard.ShardFixture.genSplits','org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.service.crawlhistoryV2.ShardThread.run','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readInt'
'org.commoncrawl.service.crawlhistoryV2.ShardThread.appendLogFileRecords','org.apache.hadoop.io.DataInputBuffer.readInt org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.DataInputBuffer.readLong org.apache.hadoop.io.DataInputBuffer.readLong'
'org.commoncrawl.service.crawlhistoryV2.ShardThread.getTLogFilePathGivenId','org.apache.hadoop.fs.Path.<init>'
'com.senseidb.indexing.hadoop.reduce.ShardWriter.ShardWriter','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs'
'com.senseidb.indexing.hadoop.reduce.ShardWriter.setParameters','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt'
'com.senseidb.indexing.hadoop.reduce.ShardWriter.moveFromTempToPerm','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'com.senseidb.indexing.hadoop.reduce.ShardWriter.moveToTrash','org.apache.hadoop.fs.Trash.<init> org.apache.hadoop.fs.Trash.moveToTrash org.apache.hadoop.fs.Trash.expunge'
'org.apache.oozie.action.hadoop.SharelibUtils.copySharelibJarsToFileSytem','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.hadoop.SharelibUtils.addToDistributedCache','org.apache.hadoop.filecache.DistributedCache.addFileToClassPath'
'org.apache.accumulo.core.util.shell.Shell.getDefaultInstance','org.apache.hadoop.fs.Path.<init>'
'tv.floe.IvoryMonkey.hadoop.fs.Shell.init','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Trash.<init>'
'tv.floe.IvoryMonkey.hadoop.fs.Shell.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'tv.floe.IvoryMonkey.hadoop.fs.Shell.run','org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.equals'
'tv.floe.IvoryMonkey.hadoop.fs.Shell.close','org.apache.hadoop.fs.FileSystem.close'
'tv.floe.IvoryMonkey.hadoop.fs.Shell.main','org.apache.hadoop.util.ToolRunner.run'
'com.nearinfinity.hbase.dsl.types.ShortConverter.fromBytes','org.apache.hadoop.hbase.util.Bytes.toShort'
'com.nearinfinity.hbase.dsl.types.ShortConverter.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.giraph.benchmark.ShortestPathsBenchmark.main','org.apache.hadoop.util.ToolRunner.run'
'.ShowFileStatusTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'.ShowFileStatusTest.tearDown','org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'.ShowFileStatusTest.throwsFileNotFoundForNonExistentFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'.ShowFileStatusTest.fileStatusForFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getReplication org.apache.hadoop.fs.FileStatus.getBlockSize org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission'
'.ShowFileStatusTest.fileStatusForDirectory','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getReplication org.apache.hadoop.fs.FileStatus.getBlockSize org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getPermission'
'com.asakusafw.compiler.flow.stage.ShuffleGroupingComparatorEmitterTest.simple','org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare'
'com.asakusafw.runtime.io.util.ShuffleKeyTest.partition','org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition org.apache.hadoop.mapreduce.Partitioner<com.asakusafw.runtime.io.util.ShuffleKey,?>.getPartition'
'com.asakusafw.compiler.flow.stage.ShufflePartitionerEmitterTest.simple','org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition org.apache.hadoop.mapreduce.Partitioner<java.lang.Object,java.lang.Object>.getPartition'
'com.asakusafw.compiler.flow.stage.ShuffleSortComparatorEmitterTest.simple','org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare org.apache.hadoop.io.RawComparator<org.apache.hadoop.io.Writable>.compare'
'com.zinnia.nectar.regression.hadoop.primitive.jobs.SigmaJob.call','org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.chain.ChainMapper.addMapper org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.chain.ChainMapper.addMapper org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.<init> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.addJob org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.allFinished org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.SigmaSqMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'com.zinnia.nectar.regression.hadoop.primitive.jobs.SigmaXYJob.call','org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.chain.ChainMapper.addMapper org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.chain.ChainMapper.addMapper org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.<init> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.addJob org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.allFinished org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.stop org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.SigmaXYMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'ivory.lsh.Signature64Test.testReadWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'ivory.lsh.Signature64Test.testSignatureSizeOnDisk','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'ivory.lsh.Signature64Test.testWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.SignatureFactory.getSignature','org.apache.hadoop.conf.Configuration.get'
'ivory.lsh.SignatureTest.testReadWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'ivory.lsh.SignatureTest.testSignatureSizeOnDisk','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'ivory.lsh.SignatureTest.testWrite','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.<init>'
'org.apache.gora.accumulo.util.SignedBinaryEncoderTest.testShort','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo'
'org.apache.gora.accumulo.util.SignedBinaryEncoderTest.testInt','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo'
'org.apache.gora.accumulo.util.SignedBinaryEncoderTest.testLong','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo'
'org.apache.gora.accumulo.util.SignedBinaryEncoderTest.testDouble','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo'
'org.apache.gora.accumulo.util.SignedBinaryEncoderTest.testFloat','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo'
'org.apache.giraph.examples.SimpleAggregatorWriter.initialize','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Mapper.Context.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create'
'org.apache.giraph.examples.SimpleAggregatorWriter.writeAggregator','org.apache.hadoop.fs.FSDataOutputStream.flush'
'org.apache.giraph.examples.SimpleAggregatorWriter.close','org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.giraph.examples.SimpleCombinerVertex.compute','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.get'
'org.apache.giraph.examples.SimpleFailVertex.compute','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.accumulo.server.gc.SimpleGarbageCollector.cleanUpDeletedTableDirs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.accumulo.server.gc.SimpleGarbageCollector.getCandidates','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.accumulo.server.gc.SimpleGarbageCollector.deleteFiles','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.gc.SimpleGarbageCollector.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.io.Text.<init>'
'org.apache.flume.sink.hbase.SimpleHbaseEventSerializer.getActions','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'org.apache.flume.sink.hbase.SimpleHbaseEventSerializer.getIncrements','org.apache.hadoop.hbase.client.Increment.<init> org.apache.hadoop.hbase.client.Increment.addColumn'
'org.apache.giraph.examples.SimpleInDegreeCountVertex.compute','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.set'
'com.manning.hip.ch3.json.SimpleJsonOutputMapReduce.Reduce.setup','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch3.json.SimpleJsonOutputMapReduce.Reduce.cleanup','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch3.json.SimpleJsonOutputMapReduce.Reduce.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch3.json.SimpleJsonOutputMapReduce.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.ucsc.srl.damasc.netcdf.map.SimpleMaxMapper.map','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.LongWritable.set'
'edu.ucsc.srl.damasc.netcdf.reduce.SimpleMaxReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.LongWritable.get'
'edu.ucsc.srl.damasc.netcdf.combine.SimpleMedianCombiner.reduce','org.apache.hadoop.io.LongWritable.toString'
'com.manning.hip.ch13.SimpleMovingAverage.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch13.SimpleMovingAverage.Map.setup','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.Text.<init>'
'com.manning.hip.ch13.SimpleMovingAverage.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.manning.hip.ch13.SimpleMovingAverage.Reduce.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.set'
'com.manning.hip.ch13.SimpleMovingAverage.Reduce2.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.set'
'org.apache.giraph.graph.SimpleMutableVertex.apply','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.graph.SimpleMutableVertex.getEdgeValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.graph.SimpleMutableVertex.addEdgeRequest','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.graph.SimpleMutableVertex.readFields','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.examples.SimpleMutateGraphVertex.compute','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.giraph.examples.SimpleOutDegreeCountVertex.compute','org.apache.hadoop.io.LongWritable.set'
'org.apache.giraph.examples.SimplePageRankVertex.compute','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.SimplePageRankVertex.SimplePageRankVertexWorkerContext.preSuperstep','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.get'
'org.apache.giraph.examples.SimplePageRankVertex.SimplePageRankVertexReader.getCurrentVertex','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.examples.SimplePageRankVertex.SimplePageRankVertexOutputFormat.SimplePageRankVertexWriter.writeVertex','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.pig.tools.pigstats.SimplePigStats.getNumberBytes','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'org.apache.pig.tools.pigstats.SimplePigStats.getNumberRecords','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'org.apache.pig.tools.pigstats.SimplePigStats.getOutputAlias','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'org.apache.pig.tools.pigstats.SimplePigStats.addJobStats','org.apache.hadoop.mapred.jobcontrol.Job.toString org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID org.apache.hadoop.mapred.jobcontrol.Job.getJobConf'
'org.apache.pig.tools.pigstats.SimplePigStats.addJobStatsForNative','org.apache.hadoop.mapred.JobID.<init>'
'org.apache.pig.tools.pigstats.SimplePigStats.setBackendException','org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID'
'org.apache.giraph.examples.SimpleShortestPathsVertex.compute','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.examples.SimpleSumCombiner.combine','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.vectorizer.SimpleTextEncodingVectorizer.createVectors','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.giraph.examples.SimpleTextVertexOutputFormat.SimpleTextVertexWriter.writeVertex','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.manning.hip.ch12.crunch.SimpleTokenize.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.giraph.examples.SimpleTriangleClosingVertexTest.testSuperstepZero','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.giraph.examples.SimpleTriangleClosingVertexTest.testSuperstepOne','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.cloudata.examples.upload.SimpleUploaderMapReduce.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMaxReduceAttempts org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete'
'org.cloudata.examples.upload.SimpleUploaderMapReduce.SimpleUploaderReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.cloudata.examples.upload.SimpleUploaderMapReduce.SimpleUploaderReducer.configure','org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.examples.upload.SimpleUploaderMapReduce.SimpleUploaderMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.apache.giraph.graph.SimpleVertex.apply','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.graph.SimpleVertex.getEdgeValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.giraph.graph.SimpleVertex.readFields','org.apache.hadoop.io.NullWritable.get'
'com.nearinfinity.hadoop.wordcount.SimpleWordCount.MapClass.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.nearinfinity.hadoop.wordcount.SimpleWordCount.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'com.nearinfinity.hadoop.wordcount.SimpleWordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.megalon.test.SimpleWrite.main','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Result.getValue'
'filters.SingleColumnValueFilterExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.SubstringComparator.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.filter.SingleColumnValueFilter.setFilterIfMissing org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setFilter org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.urbanairship.datacube.collectioninputformat.SingleItemRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'org.goldenorb.algorithms.singleSourceShortestPath.SingleSourceShortestPathReader.buildVertex','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init>'
'org.goldenorb.algorithms.singleSourceShortestPath.SingleSourceShortestPathVertex.compute','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.io.IntWritable.<init>'
'org.goldenorb.algorithms.singleSourceShortestPath.SingleSourceShortestPathWriter.vertexWrite','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.urbanairship.datacube.collectioninputformat.SingleValueSplit.readFields','org.apache.hadoop.io.Writable.readFields'
'com.urbanairship.datacube.collectioninputformat.SingleValueSplit.write','org.apache.hadoop.io.Writable.write'
'sizzle.runtime.SizzleCombiner.setConf','org.apache.hadoop.conf.Configuration.getBoolean'
'sizzle.runtime.SizzleMapper.setConf','org.apache.hadoop.conf.Configuration.getBoolean'
'sizzle.runtime.SizzleRunner.job','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.mapreduce.Cluster.<init> org.apache.hadoop.mapreduce.Job.getInstance org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass'
'org.apache.hcatalog.hbase.SkeletonHBaseTest.createTable','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.apache.hcatalog.hbase.SkeletonHBaseTest.Context.start','org.apache.hadoop.hbase.HBaseConfiguration.create'
'org.apache.hcatalog.hbase.SkeletonHBaseTest.Context.stop','org.apache.hadoop.fs.FileUtil.fullyDelete'
'com.manning.hip.ch6.SkewLogsJob.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.collect'
'com.manning.hip.ch6.SkewLogsJob.Reduce.configure','org.apache.hadoop.mapred.JobConf.getInt'
'com.manning.hip.ch6.SkewLogsJob.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setProfileEnabled org.apache.hadoop.mapred.JobConf.setProfileParams org.apache.hadoop.mapred.JobConf.setProfileTaskRange org.apache.hadoop.mapred.JobConf.setProfileTaskRange org.apache.hadoop.mapred.JobClient.runJob'
'filters.SkipFilterExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.filter.BinaryComparator.<init> org.apache.hadoop.hbase.filter.ValueFilter.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.filter.SkipFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close'
'com.hadoopilluminated.examples.SleepJob.getPartition','org.apache.hadoop.io.IntWritable.get'
'com.hadoopilluminated.examples.SleepJob.SleepInputFormat.getRecordReader','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getNumReduceTasks org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable>.<init>'
'com.hadoopilluminated.examples.SleepJob.SleepInputFormat.next','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set'
'com.hadoopilluminated.examples.SleepJob.SleepInputFormat.createKey','org.apache.hadoop.io.IntWritable.<init>'
'com.hadoopilluminated.examples.SleepJob.SleepInputFormat.createValue','org.apache.hadoop.io.IntWritable.<init>'
'com.hadoopilluminated.examples.SleepJob.map','org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.NullWritable>.collect'
'com.hadoopilluminated.examples.SleepJob.reduce','org.apache.hadoop.mapred.Reporter.setStatus'
'com.hadoopilluminated.examples.SleepJob.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.mapred.JobConf.getLong'
'com.hadoopilluminated.examples.SleepJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.gridmix.SleepJob.<init> org.apache.hadoop.util.ToolRunner.run'
'com.hadoopilluminated.examples.SleepJob.run','org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'com.hadoopilluminated.examples.SleepJob.setupJobConf','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setSpeculativeExecution org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt'
'org.apache.mahout.cf.taste.hadoop.slopeone.SlopeOneAverageDiffsJob.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.cf.taste.hadoop.slopeone.SlopeOneAverageDiffsJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.cf.taste.hadoop.slopeone.SlopeOneDiffsToAveragesReducer.reduce','org.apache.hadoop.io.FloatWritable.get'
'org.apache.mahout.cf.taste.hadoop.slopeone.SlopeOneDiffsToAveragesReducer.reduce','org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.mahout.cf.taste.hadoop.slopeone.SlopeOnePrefsToDiffsReducer.reduce','org.apache.hadoop.io.FloatWritable.<init>'
'org.sifarish.social.SlopeOneRating.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.sifarish.social.SlopeOneRating.SlopeOneMapper.map','org.apache.hadoop.io.Text.toString'
'org.sifarish.social.SlopeOneRating.SlopeOneReducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'com.manning.hip.ch5.SmallFilesRead.readFromAvro','org.apache.hadoop.io.IOUtils.cleanup org.apache.hadoop.io.IOUtils.cleanup'
'com.manning.hip.ch5.SmallFilesRead.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch5.SmallFilesWrite.writeToAvro','org.apache.hadoop.io.IOUtils.cleanup org.apache.hadoop.io.IOUtils.cleanup'
'com.manning.hip.ch5.SmallFilesWrite.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.lightboxtechnologies.nsrl.SmallTableLoader.load','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.ning.metrics.serialization.hadoop.SmileInputFormat.getInputPaths','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.util.StringUtils.split org.apache.hadoop.util.StringUtils.unEscapeString org.apache.hadoop.fs.Path.<init>'
'com.ning.metrics.serialization.hadoop.SmileInputFormat.listStatus','org.apache.hadoop.mapreduce.JobContext.getCredentials org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.mapreduce.lib.input.InvalidInputException.<init>'
'com.ning.metrics.serialization.hadoop.SmileInputFormat.getSplits','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.getFileBlockLocations org.apache.hadoop.fs.BlockLocation.getHosts org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapreduce.lib.input.FileSplit.<init>'
'com.ning.metrics.serialization.hadoop.SmileInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus'
'com.ning.metrics.serialization.hadoop.SmileInputFormat.SmilePathFilter.accept','org.apache.hadoop.fs.Path.getName'
'com.ning.metrics.serialization.hadoop.pig.SmileStorage.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'com.ning.metrics.serialization.hadoop.pig.SmileStorage.getNext','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentValue'
'org.pentaho.hadoop.shim.cdh4.SnappyShim.isHadoopSnappyAvailable','org.apache.hadoop.io.compress.SnappyCodec.isNativeCodeLoaded'
'org.pentaho.hadoop.shim.mapr.SnappyShim.isHadoopSnappyAvailable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.compress.SnappyCodec.isNativeSnappyLoaded'
'org.pentaho.hadoop.shim.cdh3u4.SnappyShim.isHadoopSnappyAvailable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.compress.SnappyCodec.isNativeSnappyLoaded'
'com.mozilla.pig.load.SnippetDateLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'org.apache.solr.core.SolbaseCoreContainer.readSchemaXML','org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.solr.core.SolbaseCoreContainer.readSchemaXMLBytes','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue'
'org.apache.solr.core.SolbaseCoreContainer.writeSchema','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTableInterface.put'
'org.solbase.indexer.SolbaseIndexUtil.getScanner','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setBatch org.apache.hadoop.hbase.client.Scan.setCaching'
'org.solbase.SolbaseIndexWriter.flush','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTableInterface.put'
'org.solbase.indexer.SolbaseIndexer.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.solbase.lucenehbase.SolbaseTermDocs.next','org.apache.hadoop.hbase.util.Bytes.toInt'
'org.solbase.SolbaseUtil.getTable','org.apache.hadoop.hbase.client.SolbaseHTablePool.getTable'
'org.solbase.SolbaseUtil.getLocalTable','org.apache.hadoop.hbase.client.SolbaseHTablePool.getTable org.apache.hadoop.hbase.client.HTable.setAutoFlush'
'org.solbase.SolbaseUtil.getTermVectorTableName','org.apache.hadoop.hbase.util.Bytes.toString'
'org.solbase.SolbaseUtil.getUniqChecksumUserMediaTableName','org.apache.hadoop.hbase.util.Bytes.toString'
'org.solbase.SolbaseUtil.getTermVectorVersionIDTableName','org.apache.hadoop.hbase.util.Bytes.toString'
'org.solbase.SolbaseUtil.getDocTableName','org.apache.hadoop.hbase.util.Bytes.toString'
'org.solbase.SolbaseUtil.getDocKeyIdMapTableName','org.apache.hadoop.hbase.util.Bytes.toString'
'org.solbase.SolbaseUtil.getSequenceTableName','org.apache.hadoop.hbase.util.Bytes.toString'
'org.solbase.SolbaseUtil.releaseTable','org.apache.hadoop.hbase.client.SolbaseHTablePool.putTable'
'org.solbase.SolbaseUtil.generateTermKey','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'org.solbase.SolbaseUtil.generateTermBeginKey','org.apache.hadoop.hbase.util.Bytes.add'
'org.solbase.SolbaseUtil.generateTermEndKey','org.apache.hadoop.hbase.util.Bytes.add'
'org.solbase.SolbaseUtil.getDocId','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toInt'
'org.solbase.SolbaseUtil.generateDocId','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTableInterface.put'
'org.solbase.SolbaseUtil.generateUniqId','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue'
'org.solbase.SolbaseUtil.getSequenceId','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong'
'org.solbase.SolbaseUtil.createTable','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.solbase.SolbaseUtil.setupHColumnDescriptor','org.apache.hadoop.hbase.HColumnDescriptor.setCompressionType org.apache.hadoop.hbase.HColumnDescriptor.setScope org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions'
'org.solbase.SolbaseUtil.createSITable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.solbase.SolbaseUtil.createTermVectorTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'org.solbase.SolbaseUtil.createTermVectorVersionIDTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'org.solbase.SolbaseUtil.createDocKeyIdMapTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'org.solbase.SolbaseUtil.createDocTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.solbase.SolbaseUtil.createSequenceTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'org.solbase.SolbaseUtil.createUniqChecksumUserMediaTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily'
'org.solbase.SolbaseUtil.truncateTable','org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.solbase.SolbaseUtil.randomize','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.SolrRecord.readFields','org.apache.hadoop.io.Text.readString'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.SolrRecord.write','org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.SolrInputFormat.getRecordReader','org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.Text,org.apache.nutch.indexer.solr.SolrDeleteDuplicates.SolrRecord>.<init>'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.SolrInputFormat.createKey','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.SolrInputFormat.next','org.apache.hadoop.io.Text.set'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.configure','org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.reduce','org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.nutch.indexer.solr.SolrDeleteDuplicates.main','org.apache.hadoop.util.ToolRunner.run'
'lsh.solr.SolrDriver.main','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.nutch.indexer.solr.SolrIndexer.indexSolr','org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setReduceSpeculativeExecution org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get'
'org.apache.nutch.indexer.solr.SolrIndexer.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.indexer.solr.SolrIndexer.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.solr.hadoop.SolrOutputFormat.setSolrWriterThreadCount','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.solr.hadoop.SolrOutputFormat.getSolrWriterThreadCount','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.solr.hadoop.SolrOutputFormat.setSolrWriterQueueSize','org.apache.hadoop.conf.Configuration.setInt'
'org.apache.solr.hadoop.SolrOutputFormat.getSolrWriterQueueSize','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.solr.hadoop.SolrOutputFormat.getZipName','org.apache.hadoop.conf.Configuration.get'
'org.apache.solr.hadoop.SolrOutputFormat.setOutputZipFormat','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.solr.hadoop.SolrOutputFormat.isOutputZipFormat','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.solr.hadoop.SolrOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.solr.hadoop.SolrOutputFormat.setupSolrHomeCache','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.filecache.DistributedCache.addCacheArchive org.apache.hadoop.filecache.DistributedCache.getCacheArchives org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.solr.hadoop.SolrOutputFormat.getBatchSize','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.solr.hadoop.SolrOutputFormat.setBatchSize','org.apache.hadoop.conf.Configuration.setInt'
'com.datasalt.pangool.solr.SolrRecordWriter.getOutFileName','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskID.getId'
'com.datasalt.pangool.solr.SolrRecordWriter.SolrRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getLocalPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.startLocalOutput org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.getJobName org.apache.hadoop.conf.Configuration.get'
'com.datasalt.pangool.solr.SolrRecordWriter.findSolrConfig','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.getLocalCacheArchives org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName'
'com.datasalt.pangool.solr.SolrRecordWriter.close','org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.fs.FileSystem.completeLocalOutput org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus'
'com.datasalt.pangool.solr.SolrRecordWriter.packZipFile','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete'
'com.datasalt.pangool.solr.SolrRecordWriter.zipDirectory','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.LocalFileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.LocalFileSystem.listStatus org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.LocalFileSystem.open org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.solr.hadoop.SolrRecordWriter.getOutFileName','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskID.getId'
'org.apache.solr.hadoop.SolrRecordWriter.SolrRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getLocalPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.startLocalOutput org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.TaskAttemptContext.getJobName org.apache.hadoop.conf.Configuration.get'
'org.apache.solr.hadoop.SolrRecordWriter.findSolrConfig','org.apache.hadoop.filecache.DistributedCache.getLocalCacheArchives org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName'
'org.apache.solr.hadoop.SolrRecordWriter.close','org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus org.apache.hadoop.fs.FileSystem.completeLocalOutput org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.TaskAttemptContext.setStatus'
'org.apache.solr.hadoop.SolrRecordWriter.packZipFile','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete'
'org.apache.solr.hadoop.SolrRecordWriter.zipDirectory','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.LocalFileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.LocalFileSystem.listStatus org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.LocalFileSystem.open org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.nutch.searcher.SolrSearchBean.search','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.nutch.indexer.solr.SolrWriter.open','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getInt'
'test.SomeMainClass.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.size'
'test.SomeMainClass.dumpConfiguration','org.apache.hadoop.conf.Configuration.hashCode org.apache.hadoop.conf.Configuration.toString'
'eu.stratosphere.sopremo.server.SopremoTestServer.close','org.apache.hadoop.fs.FileSystem.closeAll'
'.SortByTemperatureToMapFile.run','org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapred.JobClient.runJob'
'.SortByTemperatureToMapFile.main','org.apache.hadoop.util.ToolRunner.run'
'.SortByTemperatureUsingHashPartitioner.run','org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapred.JobClient.runJob'
'.SortByTemperatureUsingHashPartitioner.main','org.apache.hadoop.util.ToolRunner.run'
'org.archive.wayback.hadoop.SortDriver.main','org.apache.hadoop.util.ProgramDriver.<init> org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.addClass org.apache.hadoop.util.ProgramDriver.driver'
'com.zinnia.nectar.regression.hadoop.primitive.jobs.SortJob.call','org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.chain.ChainMapper.addMapper org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.chain.ChainMapper.addMapper org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.<init> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.addJob org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.allFinished org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'com.manning.hip.ch7.friendsofafriend.SortMapReduce.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Mapper.Context.write org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch7.friendsofafriend.SortMapReduce.Reduce.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.SortMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'gr.ntua.cslab.distributed.sort.SortMapper.configure','org.apache.hadoop.mapred.JobConf.get'
'gr.ntua.cslab.distributed.sort.SortMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<gr.ntua.cslab.data.TupleWritable,org.apache.hadoop.io.Text>.collect'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.setInputPath','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init>'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.getInputPath','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.addPartitionOutputNames','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.setCompressedInput','org.apache.hadoop.conf.Configuration.setBoolean'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.getCompressedInput','org.apache.hadoop.conf.Configuration.getBoolean'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.setFilterField','org.apache.hadoop.conf.Configuration.setInt'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.getFilterField','org.apache.hadoop.conf.Configuration.getInt'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.setFilterPath','org.apache.hadoop.conf.Configuration.set'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.getFilterPath','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.setFilterSURT','org.apache.hadoop.conf.Configuration.setBoolean'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.getFilterSURT','org.apache.hadoop.conf.Configuration.getBoolean'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.parseInputSplit','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getBlockSize org.apache.hadoop.fs.FileSystem.getFileBlockLocations org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.BlockLocation.getHosts org.apache.hadoop.fs.BlockLocation.getHosts'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.SortMergeRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open'
'org.archive.hadoop.mapreduce.SortMergeInputFormat.SortMergeRecordReader.nextKeyValue','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'gr.ntua.cslab.distributed.sort.samplebased.SortPartitionerBySamples.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'gr.ntua.cslab.distributed.sort.SortReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'pl.edu.icm.coansys.logsanalysis.jobs.SortUsagesPart.SorterMap.map','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'pl.edu.icm.coansys.logsanalysis.jobs.SortUsagesPart.SorterReduce.reduce','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.NullWritable.get'
'pl.edu.icm.coansys.logsanalysis.jobs.SortUsagesPart.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'pl.edu.icm.coansys.logsanalysis.jobs.SortUsagesPart.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.webgraph.driver.SortWebGraph.Partition.configure','org.apache.hadoop.mapred.JobConf.getInt'
'edu.umd.cloud9.webgraph.driver.SortWebGraph.Partition.getPartition','org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.webgraph.driver.SortWebGraph.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.webgraph.driver.SortWebGraph.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapred.SequenceFileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.webgraph.driver.SortWebGraph.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.runtime.stage.collector.SortableSlot.begin','org.apache.hadoop.io.DataOutputBuffer.reset'
'com.asakusafw.runtime.stage.collector.SortableSlot.addByte','org.apache.hadoop.io.DataOutputBuffer.writeByte'
'com.asakusafw.runtime.stage.collector.SortableSlot.addRandom','org.apache.hadoop.io.DataOutputBuffer.writeInt'
'com.asakusafw.runtime.stage.collector.SortableSlot.add','org.apache.hadoop.io.Writable.write'
'com.asakusafw.runtime.stage.collector.SortableSlot.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'com.asakusafw.runtime.stage.collector.SortableSlot.readFields','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataOutputBuffer.write'
'com.asakusafw.runtime.stage.collector.SortableSlot.compareTo','org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableComparator.compareBytes'
'com.asakusafw.runtime.stage.collector.SortableSlot.hashCode','org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength'
'com.asakusafw.runtime.stage.collector.SortableSlot.Comparator.compare','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableComparator.readVInt org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableComparator.readVInt org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableComparator.readVInt org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableComparator.readVInt'
'skywriting.examples.skyhout.common.SortedInputSequenceFileOutputReduceDriver.SortedInputSequenceFileOutputReduceDriver','org.apache.hadoop.fs.Path.<init>'
'skywriting.examples.skyhout.common.SortedInputTextOutputReduceDriver.SortedInputTextOutputReduceDriver','org.apache.hadoop.fs.Path.<init>'
'ivory.ptc.SortedPseudoTestCollection.MyMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'ivory.ptc.SortedPseudoTestCollection.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<ivory.ptc.data.PseudoQuery,ivory.ptc.data.PseudoJudgments>.collect'
'ivory.ptc.SortedPseudoTestCollection.MyReducer.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'ivory.ptc.SortedPseudoTestCollection.MyReducer.reduce','org.apache.hadoop.mapred.OutputCollector<ivory.ptc.data.PseudoQuery,ivory.ptc.data.PseudoJudgments>.collect'
'ivory.ptc.SortedPseudoTestCollection.runTool','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobClient.runJob'
'gr.ntua.cslab.distributed.sort.qidbased.SorterBasedOnQid.runSort','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobClient.runJob'
'edu.jhu.thrax.hadoop.features.mapred.SourcePhraseGivenTargetandLHSFeature.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.NullWritable.get'
'org.apache.accumulo.core.iterators.system.SourceSwitchingIteratorTest.nk','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.SparseColumnFamilyTest.run','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.SparseRowBlockAccumulator.flushBlock','org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.mahout.math.hadoop.stochasticsvd.SparseRowBlockWritable>.collect'
'com.mycompany.hiaex.SpatialJoin.Mapper1.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.util.StringUtils.getStrings'
'com.mycompany.hiaex.SpatialJoin.Reducer1.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.util.StringUtils.getStrings org.apache.hadoop.util.StringUtils.join org.apache.hadoop.io.Text.<init>'
'com.mycompany.hiaex.SpatialJoin.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.mycompany.hiaex.SpatialJoin.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.stats.entropy.SpecificConditionalEntropyMapper.map','org.apache.hadoop.io.Text.set'
'org.apache.mahout.math.stats.entropy.SpecificConditionalEntropyReducer.reduce','org.apache.hadoop.io.DoubleWritable.set'
'org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.nesscomputing.hbase.spill.SpillReader.runLoop','org.apache.hadoop.hbase.client.HTable.put'
'cascading.tuple.SpillableTupleHadoopTest.testSpillListCompressed','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.util.ReflectionUtils.newInstance'
'cascading.tuple.SpillableTupleHadoopTest.performListTest','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.io.Text.<init>'
'cascading.tuple.SpillableTupleHadoopTest.testSpillMap','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.tuple.SpillableTupleHadoopTest.testSpillMapCompressed','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set'
'cascading.tuple.SpillableTupleHadoopTest.performMapTest','org.apache.hadoop.io.Text.<init>'
'com.nearinfinity.blur.mapreduce.SpinLock.main','org.apache.hadoop.util.Progressable.<init>'
'com.nearinfinity.blur.mapreduce.SpinLock.copyLock','org.apache.hadoop.util.Progressable.progress org.apache.hadoop.mapreduce.Reducer.Context.setStatus'
'org.apache.mahout.classifier.bayes.SplitBayesInput.SplitBayesInput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.classifier.bayes.SplitBayesInput.parseArgs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.mahout.classifier.bayes.SplitBayesInput.splitDirectory','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.classifier.bayes.SplitBayesInput.splitFile','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.getName'
'org.apache.mahout.classifier.bayes.SplitBayesInput.validate','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir'
'org.apache.mahout.classifier.bayes.SplitBayesInput.countLines','org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.classifier.bayes.SplitBayesInputTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.classifier.bayes.SplitBayesInputTest.writeMultipleInputFiles','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.classifier.bayes.SplitBayesInputTest.writeSingleInputFile','org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.classifier.bayes.SplitBayesInputTest.splitComplete','org.apache.hadoop.fs.Path.getName'
'org.apache.mahout.classifier.bayes.SplitBayesInputTest.assertSplit','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init>'
'fr.insarennes.fafdti.hadoop.SplitExampleMultipleOutputFormat.generateFileNameForKeyValue','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.utils.SplitInput.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.utils.SplitInput.parseArgs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.mahout.utils.SplitInput.splitDirectory','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.utils.SplitInput.splitFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.Path.getName'
'org.apache.mahout.utils.SplitInput.validate','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir'
'org.apache.mahout.utils.SplitInput.countLines','org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.utils.SplitInputJob.run','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.utils.SplitInputJob.SplitInputReducer.setup','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.<init> org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector'
'org.apache.mahout.utils.SplitInputJob.SplitInputReducer.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.io.Writable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable<?>,org.apache.hadoop.io.Writable>.collect'
'org.apache.mahout.utils.SplitInputJob.SplitInputReducer.cleanup','org.apache.hadoop.mapred.lib.MultipleOutputs.close'
'org.apache.accumulo.server.test.functional.SplitRecoveryTest.nke','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.functional.SplitRecoveryTest.runSplitRecoveryTest','org.apache.hadoop.io.Text.<init>'
'.SplitTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem'
'.SplitTest.tearDown','org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'.SplitTest.recordsCoincideWithBlocks','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.getInputFormat org.apache.hadoop.mapred.JobConf.getNumMapTasks org.apache.hadoop.mapred.InputFormat<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getSplits'
'.SplitTest.recordsDontCoincideWithBlocks','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.getInputFormat org.apache.hadoop.mapred.JobConf.getNumMapTasks org.apache.hadoop.mapred.InputFormat<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getSplits'
'.SplitTest.compression','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLength org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.getInputFormat org.apache.hadoop.mapred.JobConf.getNumMapTasks org.apache.hadoop.mapred.InputFormat<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getSplits'
'.SplitTest.checkSplit','org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength'
'.SplitTest.checkRecord','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.next org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get'
'.SplitTest.checkRecordReader','org.apache.hadoop.mapred.InputFormat<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getRecordReader org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.next'
'.SplitTest.createFile','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'org.gora.sql.store.SqlTypeInterface.getSqlType','org.apache.hadoop.io.DoubleWritable.isAssignableFrom org.apache.hadoop.io.FloatWritable.isAssignableFrom org.apache.hadoop.io.IntWritable.isAssignableFrom org.apache.hadoop.io.LongWritable.isAssignableFrom org.apache.hadoop.io.Text.isAssignableFrom org.apache.hadoop.io.VIntWritable.isAssignableFrom org.apache.hadoop.io.VLongWritable.isAssignableFrom org.apache.hadoop.io.Writable.isAssignableFrom'
'org.apache.gora.sql.store.SqlTypeInterface.getSqlType','org.apache.hadoop.io.DoubleWritable.isAssignableFrom org.apache.hadoop.io.FloatWritable.isAssignableFrom org.apache.hadoop.io.IntWritable.isAssignableFrom org.apache.hadoop.io.LongWritable.isAssignableFrom org.apache.hadoop.io.Text.isAssignableFrom org.apache.hadoop.io.VIntWritable.isAssignableFrom org.apache.hadoop.io.VLongWritable.isAssignableFrom org.apache.hadoop.io.Writable.isAssignableFrom'
'com.manning.hip.ch2.SqoopSequenceFileReader.main','org.apache.hadoop.fs.Path.<init>'
'com.manning.hip.ch2.SqoopSequenceFileReader.read','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.sqoop.tool.SqoopTool.loadPluginsFromConfDir','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.setClassLoader'
'org.apache.sqoop.tool.SqoopTool.loadPluginsFromFile','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.sqoop.tool.SqoopTool.addPlugin','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.sqoop.tool.SqoopTool.getTool','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.sqoop.tool.SqoopTool.printHelp','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'org.apache.sqoop.tool.SqoopTool.parseArguments','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.sqoop.Sqoop.runSqoop','org.apache.hadoop.util.ToolRunner.run'
'org.apache.sqoop.Sqoop.runTool','org.apache.hadoop.conf.Configuration.<init>'
'com.asakusafw.windgate.hadoopfs.ssh.SshProfile.extractCompressionCodec','org.apache.hadoop.conf.Configuration.getClassByName org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.commoncrawl.mapred.ec2.postprocess.deduper.Stage2Reducer.reduce','org.apache.hadoop.mapred.Reporter.incrCounter'
'com.asakusafw.runtime.stage.input.StageInputFormat.computeSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.InputFormat<?,?>.getSplits org.apache.hadoop.mapreduce.InputFormat<?,?>.getSplits org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.asakusafw.runtime.stage.input.StageInputFormat.getSplitCombiner','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.asakusafw.runtime.stage.input.StageInputFormat.getSplitCombinerClass','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getClassByName'
'com.asakusafw.runtime.stage.input.StageInputFormat.isLocalMode','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.asakusafw.runtime.stage.input.StageInputFormat.toPathArray','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.stage.input.StageInputFormat.getPaths','org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'com.asakusafw.runtime.stage.input.StageInputFormat.instantiateFormats','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.asakusafw.runtime.stage.input.StageInputMapper.setup','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.asakusafw.runtime.stage.input.StageInputMapper.run','org.apache.hadoop.mapreduce.Mapper.run'
'com.asakusafw.runtime.stage.output.StageOutputDriver.prepareSinks','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getConfiguration org.apache.hadoop.conf.Configuration.getStringCollection'
'com.asakusafw.runtime.stage.output.StageOutputDriver.setOutputFilePrefix','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getDeclaredMethod'
'com.asakusafw.runtime.stage.output.StageOutputDriver.buildSink','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getConfiguration org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.conf.Configuration.getClass'
'com.asakusafw.runtime.stage.output.StageOutputDriver.getCounters','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getTaskAttemptID org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getCounter org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getCounter org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getCounter'
'com.asakusafw.runtime.stage.output.StageOutputDriver.buildNormalSink','org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getConfiguration org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskInputOutputContext<?,?,?,?>.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.isAssignableFrom org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.OutputFormat<?,?>.getRecordWriter'
'com.asakusafw.runtime.stage.output.StageOutputDriver.set','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.isAssignableFrom org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.stage.output.StageOutputDriver.addOutput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.getStringCollection org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass'
'com.asakusafw.runtime.stage.output.StageOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat<java.lang.Object,java.lang.Object>.checkOutputSpecs'
'com.asakusafw.runtime.stage.output.StageOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat<java.lang.Object,java.lang.Object>.getRecordWriter'
'com.asakusafw.runtime.stage.output.StageOutputFormat.createOutputCommitter','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat<java.lang.Object,java.lang.Object>.getOutputCommitter'
'com.asakusafw.runtime.stage.output.StageOutputFormat.isFileOutputEnabled','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath'
'com.asakusafw.runtime.stage.output.StageOutputFormat.CombinedOutputCommitter.setupJob','org.apache.hadoop.mapreduce.OutputCommitter.setupJob org.apache.hadoop.mapreduce.OutputCommitter.getClass org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.StageOutputFormat.CombinedOutputCommitter.commitJob','org.apache.hadoop.mapreduce.OutputCommitter.commitJob org.apache.hadoop.mapreduce.OutputCommitter.getClass org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.StageOutputFormat.CombinedOutputCommitter.abortJob','org.apache.hadoop.mapreduce.OutputCommitter.abortJob org.apache.hadoop.mapreduce.OutputCommitter.getClass org.apache.hadoop.mapreduce.JobContext.getJobID'
'com.asakusafw.runtime.stage.output.StageOutputFormat.CombinedOutputCommitter.setupTask','org.apache.hadoop.mapreduce.OutputCommitter.setupTask org.apache.hadoop.mapreduce.OutputCommitter.getClass org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'com.asakusafw.runtime.stage.output.StageOutputFormat.CombinedOutputCommitter.needsTaskCommit','org.apache.hadoop.mapreduce.OutputCommitter.needsTaskCommit org.apache.hadoop.mapreduce.OutputCommitter.getClass org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'com.asakusafw.runtime.stage.output.StageOutputFormat.CombinedOutputCommitter.commitTask','org.apache.hadoop.mapreduce.OutputCommitter.commitTask org.apache.hadoop.mapreduce.OutputCommitter.getClass org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'com.asakusafw.runtime.stage.output.StageOutputFormat.CombinedOutputCommitter.abortTask','org.apache.hadoop.mapreduce.OutputCommitter.abortTask org.apache.hadoop.mapreduce.OutputCommitter.getClass org.apache.hadoop.mapreduce.TaskAttemptContext.getJobID org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID'
'com.linkedin.mr_kluj.StagedOutputJob.waitForCompletion','org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.rename'
'com.urbanairship.hbackup.StalenessCheckerTest.oneStaleFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.urbanairship.hbackup.StalenessCheckerTest.oneStaleOneNot','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.flow.StandardDeviationFlow.reduce','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.NullWritable.get'
'com.wibidata.maven.plugins.hbase.StartMojo.execute','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'com.wibidata.maven.plugins.hbase.StartMojo.removeHadoopTmpDir','org.apache.hadoop.conf.Configuration.get'
'com.wibidata.maven.plugins.hbase.StartMojo.writeSiteFile','org.apache.hadoop.conf.Configuration.writeXml'
'com.wibidata.maven.plugins.hbase.StartMojo.getFilteredConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'com.smartitengineering.cms.maven.tools.plugin.StartMojo.ToolsSuite.ToolsSuite','org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setClientPort org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.HBaseTestingUtility.setZkCluster org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown'
'com.smartitengineering.cms.maven.tools.plugin.StartMojo.ToolsSuite.stopAll','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown'
'com.taobao.adfs.state.StateManager.adfsBlockEntryToBlockInfo','org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.<init>'
'com.taobao.adfs.state.StateManager.reloadDatanodeDescriptorMaps','org.apache.hadoop.net.NetworkTopology.<init> org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getStorageID org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getAdminState org.apache.hadoop.net.NetworkTopology.add org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getXceiverCount org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getCapacity org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getRemaining org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getDfsUsed'
'com.taobao.adfs.state.StateManager.adfsDatanodeToDatanodeDescriptor','org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.<init> org.apache.hadoop.hdfs.protocol.DatanodeInfo.AdminStates.valueOf org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.setAdminState org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getAdminState org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getAdminState'
'com.taobao.adfs.state.StateManager.datanodeDescriptorToAdfsDatanode','org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getId org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getName org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getStorageID org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getInfoPort org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getIpcPort org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getCapacity org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getDfsUsed org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getRemaining org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getLastUpdate org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getXceiverCount org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getNetworkLocation org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getAdminState'
'com.taobao.adfs.state.StateManager.updateDatanodeByDatanodeDescriptor','org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getId org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getId org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getId org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getId org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getStorageID org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getId org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getId org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getStorageID org.apache.hadoop.net.NetworkTopology.remove org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getStorageID org.apache.hadoop.net.NetworkTopology.remove org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getAdminState org.apache.hadoop.net.NetworkTopology.add org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getAdminState org.apache.hadoop.net.NetworkTopology.contains org.apache.hadoop.net.NetworkTopology.add org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getAdminState org.apache.hadoop.net.NetworkTopology.contains org.apache.hadoop.net.NetworkTopology.remove org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getStorageID org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getStorageID org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getStorageID'
'com.taobao.adfs.state.StateManager.updateClusterStatistics','org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getXceiverCount org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getCapacity org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getRemaining org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getDfsUsed'
'com.taobao.adfs.state.StateManager.fileToFileStatus','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.<init>'
'com.taobao.adfs.state.StateManager.adfsFileToHdfsFileStatus','org.apache.hadoop.hdfs.protocol.HdfsFileStatus.<init>'
'com.taobao.adfs.state.StateManager.LeaseMonitor.run','org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSNamesystem org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSStateManager org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSStateManager org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSNamesystem org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSStateManager'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.StatsAggregationMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.StatsAggregationMapper.map','org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.util.StringUtils.stringifyException'
'com.urbanairship.statshtable.StatsHTableFactory.createHTableInterface','org.apache.hadoop.hbase.client.HTableInterfaceFactory.createHTableInterface'
'com.urbanairship.statshtable.StatsHTableFactory.releaseHTableInterface','org.apache.hadoop.hbase.client.HTableInterfaceFactory.releaseHTableInterface'
'org.commoncrawl.service.stats.StatsServiceServer.taskComplete','org.apache.hadoop.record.Buffer.<init>'
'org.commoncrawl.service.stats.StatsServiceServer.call','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.setSize'
'org.apache.hcatalog.templeton.StatusDelegator.run','org.apache.hadoop.security.UserGroupInformation.createRemoteUser org.apache.hadoop.mapred.JobTracker.getAddress org.apache.hadoop.mapred.TempletonJobTracker.<init> org.apache.hadoop.mapred.TempletonJobTracker.close'
'org.apache.hcatalog.templeton.StatusDelegator.makeStatus','org.apache.hadoop.mapred.TempletonJobTracker.getJobStatus org.apache.hadoop.mapred.TempletonJobTracker.getJobProfile org.apache.hadoop.mapred.TempletonJobTracker.getJobStatus org.apache.hadoop.mapred.TempletonJobTracker.getJobProfile'
'org.apache.hcatalog.templeton.StatusDelegator.StringToJobID','org.apache.hadoop.mapred.JobID.forName'
'org.apache.oozie.service.StatusTransitService.StatusTransitRunnable.aggregateCoordJobsStatus','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.service.StatusTransitService.init','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.StatusTransitService.StatusTransitRunnable.aggregateCoordJobsStatus','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.service.StatusTransitService.init','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.util.StatusUtils.getStatus','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.util.StatusUtils.getStatusForCoordRerun','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.util.StatusUtils.getStatusForCoordActionInputCheck','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.util.StatusUtils.isV1CoordjobKillable','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.util.StatusUtils.getStatusIfBackwardSupportTrue','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.util.StatusUtils.getStatus','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.util.StatusUtils.getStatusForCoordRerun','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.util.StatusUtils.getStatusForCoordActionInputCheck','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.oozie.util.StatusUtils.isV1CoordjobKillable','org.apache.hadoop.conf.Configuration.getBoolean'
'com.datasalt.pangool.flow.Step.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings'
'com.datasalt.pangool.flow.Step.executeCoGrouper','org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.insarennes.fafdti.hadoop.furious.Step0Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'fr.insarennes.fafdti.hadoop.furious.Step0Red.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.<init>'
'fr.insarennes.fafdti.hadoop.veryfurious.Step11Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'fr.insarennes.fafdti.hadoop.veryfurious.Step11Map.generateNFGram','org.apache.hadoop.mapreduce.Mapper.Context.write'
'fr.insarennes.fafdti.hadoop.veryfurious.Step11Map.generateSGram','org.apache.hadoop.mapreduce.Mapper.Context.write'
'fr.insarennes.fafdti.hadoop.veryfurious.Step11Red.setup','org.apache.hadoop.mapreduce.Reducer.Context.getConfiguration'
'fr.insarennes.fafdti.hadoop.veryfurious.Step11Red.reduce','org.apache.hadoop.io.IntWritable.get'
'fr.insarennes.fafdti.hadoop.veryfurious.Step11Red.writeIfBestQuestion','org.apache.hadoop.mapreduce.Reducer.Context.write'
'fr.insarennes.fafdti.hadoop.veryfurious.Step12Red.reduce','org.apache.hadoop.io.IntWritable.get'
'fr.insarennes.fafdti.hadoop.furious.Step1Map.setup','org.apache.hadoop.io.IntWritable.<init>'
'fr.insarennes.fafdti.hadoop.furious.Step1Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.apache.mahout.classifier.df.mapreduce.partial.Step1Mapper.setup','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.mahout.classifier.df.mapreduce.partial.Step1Mapper.map','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.classifier.df.mapreduce.partial.Step1MapperTest.testMapper','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.set'
'step2.Step2.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.df.mapreduce.partial.Step2Job.Step2Job','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapreduce.partial.Step2Job.run','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.setCacheFiles org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.df.mapreduce.partial.Step2Job.parseOutput','org.apache.hadoop.mapreduce.Job.getConfiguration'
'fr.insarennes.fafdti.hadoop.furious.Step2Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.apache.mahout.df.mapreduce.partial.Step2Mapper.setup','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.mahout.df.mapreduce.partial.Step2Mapper.map','org.apache.hadoop.io.Text.toString'
'fr.insarennes.fafdti.hadoop.furious.Step2Red.reduce','org.apache.hadoop.io.Text.<init>'
'step3.Step3.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'step5.Step5.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch3.proto.StockAvgProtocolBuffersFileReader.readFromProtoBuf','org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.proto.StockAvgProtocolBuffersFileReader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.hackreduce.models.StockExchangeDividend.StockExchangeDividend','org.apache.hadoop.io.Text.toString'
'org.hackreduce.models.StockExchangeRecord.StockExchangeRecord','org.apache.hadoop.io.Text.toString'
'com.manning.hip.ch3.proto.StockProtocolBuffersFileReader.readFromProtoBuf','org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.proto.StockProtocolBuffersFileReader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.manning.hip.ch3.proto.StockProtocolBuffersMapReduce.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.manning.hip.ch3.proto.StockProtocolBuffersMapReduce.generateInput','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.closeStream'
'com.manning.hip.ch3.proto.StockProtocolBuffersMapReduce.PBMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.manning.hip.ch3.proto.StockProtocolBuffersMapReduce.PBReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapreduce.Mapper.Context.write'
'org.apache.pig.impl.util.StorageUtil.textToTuple','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.hcatalog.utils.StoreDemo.SumMapper.map','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.hcatalog.utils.StoreDemo.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.OutputFormat<org.apache.hadoop.io.BytesWritable,org.apache.pig.data.Tuple>.checkOutputSpecs'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.OutputFormat<org.apache.hadoop.io.BytesWritable,org.apache.pig.data.Tuple>.getOutputCommitter'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedRecordWriter.close','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.BytesWritable,org.apache.pig.data.Tuple>.close'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedOutputCommitter.abortTask','org.apache.hadoop.mapreduce.OutputCommitter.abortTask'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedOutputCommitter.commitTask','org.apache.hadoop.mapreduce.OutputCommitter.commitTask'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedOutputCommitter.needsTaskCommit','org.apache.hadoop.mapreduce.OutputCommitter.needsTaskCommit'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedOutputCommitter.setupJob','org.apache.hadoop.mapreduce.OutputCommitter.setupJob'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedOutputCommitter.setupTask','org.apache.hadoop.mapreduce.OutputCommitter.setupTask'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedOutputCommitter.commitJob','org.apache.hadoop.mapreduce.OutputCommitter.commitJob org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedOutputCommitter.cleanupJob','org.apache.hadoop.mapreduce.OutputCommitter.cleanupJob org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.hcatalog.pig.drivers.StoreFuncBasedOutputFormat.StoreFuncBasedOutputCommitter.abortJob','org.apache.hadoop.mapreduce.OutputCommitter.abortJob'
'com.talis.hbase.rdf.store.StoreInformationHolder.StoreInformationHolder','org.apache.hadoop.hbase.HTableDescriptor.getNameAsString'
'com.talis.hbase.rdf.store.StoreInformationHolder.addPredicateMapping','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut org.apache.hadoop.hbase.client.HTable.flushCommits'
'com.talis.hbase.rdf.store.StoreInformationHolder.getPredicateMapping','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get'
'com.talis.hbase.rdf.store.StoreInformationHolder.removePredicateMapping','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.client.HTable.flushCommits'
'com.talis.hbase.rdf.store.StoreInformationHolder.createPrefixTbl','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.client.HBaseAdmin.addColumn org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.client.HTable.<init>'
'.StreamCompressor.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.createOutputStream org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.compress.CompressionOutputStream.finish'
'org.springframework.data.hadoop.mapreduce.StreamJobFactoryBean.getObjectType','org.apache.hadoop.mapreduce.Job.getClass'
'org.springframework.data.hadoop.mapreduce.StreamJobFactoryBean.afterPropertiesSet','org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.createProxyUser org.apache.hadoop.security.UserGroupInformation.doAs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName'
'org.springframework.data.hadoop.mapreduce.StreamJobFactoryBean.run','org.apache.hadoop.mapreduce.Job.<init>'
'org.springframework.data.hadoop.mapreduce.StreamJobFactoryBean.createStreamJob','org.apache.hadoop.streaming.StreamJob.<init> org.apache.hadoop.streaming.StreamJob.setConf org.apache.hadoop.streaming.StreamJob.getClass org.apache.hadoop.streaming.StreamJob.getClass'
'com.manning.hip.common.StreamToHdfs.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.configure','org.apache.hadoop.io.compress.CompressionCodecFactory.<init>'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.isSplitable','org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.getSplits','org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.JobConf.getLong org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getBlockSize org.apache.hadoop.net.NetworkTopology.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.getFileBlockLocations org.apache.hadoop.io.compress.SplitCompressionInputStream.getAdjustedStart org.apache.hadoop.io.compress.SplitCompressionInputStream.getAdjustedEnd org.apache.hadoop.io.compress.SplitCompressionInputStream.close org.apache.hadoop.mapred.JobConf.getFloat org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.fs.BlockLocation.getHosts'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.getRecordReader','org.apache.hadoop.mapred.FileSplit.toString org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getBoolean'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.MyRecordReader.MyRecordReader','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.io.compress.SplitCompressionInputStream.getAdjustedStart org.apache.hadoop.io.compress.SplitCompressionInputStream.getAdjustedEnd org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.MyRecordReader.createKey','org.apache.hadoop.io.Text.<init>'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.MyRecordReader.createValue','org.apache.hadoop.io.Text.<init>'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.MyRecordReader.next','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.MyRecordReader.readUntilMatch','org.apache.hadoop.io.DataOutputBuffer.reset'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.writeInSequence','org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.getBuffer','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.write'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.getPageBytes','org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.io.compress.SplitCompressionInputStream.getAdjustedStart org.apache.hadoop.io.compress.SplitCompressionInputStream.getAdjustedEnd org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.incrCounter'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.offsetWrite','org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.write'
'org.wikimedia.wikihadoop.StreamWikiDumpInputFormat.findIndex','org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength'
'cascading.tap.hadoop.io.StreamedFileSystem.getWorkingDirectory','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified'
'cascading.tap.hadoop.io.StreamedFileSystem.getMD5SumFor','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.get'
'cascading.tap.hadoop.io.StreamedFileSystem.setMD5SumFor','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.hadoop.StreamingMain.submitJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.JobClient.close'
'org.apache.oozie.action.hadoop.StreamingMain.setStreaming','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.springframework.data.hadoop.mapreduce.StreamingTest.testStreaming','org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.springframework.data.hadoop.mapreduce.StreamingTest.testStreamingNS','org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.springframework.data.hadoop.mapreduce.StreamingTest.cleanOutput','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.springframework.data.hadoop.mapreduce.StreamingTest.testJobProperties','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.expreval.expr.betweenstmt.StringBetweenStmt.StringBetweenComparable.compareTo','org.apache.hadoop.hbase.hbql.io.IO.getSerialization org.apache.hadoop.hbase.hbql.client.HBqlException.printStackTrace org.apache.hadoop.hbase.hbql.impl.Utils.logException'
'org.apache.expreval.expr.calculation.StringCalculation.getValue','org.apache.hadoop.hbase.hbql.client.HBqlException.<init>'
'com.nearinfinity.hbase.dsl.types.StringConverter.fromBytes','org.apache.hadoop.hbase.util.Bytes.toString'
'com.nearinfinity.hbase.dsl.types.StringConverter.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'nl.vu.datalayer.hbase.bulkload.StringIdAssocTest.testId2String','org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add'
'nl.vu.datalayer.hbase.bulkload.StringIdAssocTest.testString2Id','org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'org.apache.mahout.fpm.pfpgrowth.convertors.string.StringOutputConverter.collect','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns>.collect'
'org.apache.gora.mapreduce.StringSerialization.deserialize','org.apache.hadoop.io.Text.readString'
'org.apache.gora.mapreduce.StringSerialization.getDeserializer','org.apache.hadoop.io.serializer.Deserializer<java.lang.String>.<init>'
'org.apache.gora.mapreduce.StringSerialization.serialize','org.apache.hadoop.io.Text.writeString'
'org.apache.gora.mapreduce.StringSerialization.getSerializer','org.apache.hadoop.io.serializer.Serializer<java.lang.String>.<init>'
'org.apache.pig.test.udf.storefunc.StringStore.getNext','org.apache.hadoop.io.Text.toString'
'com.asakusafw.runtime.stage.directio.StringTemplate.apply','org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.toString'
'com.asakusafw.runtime.stage.directio.StringTemplate.getSizeInBytes','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableComparator.readVInt'
'com.asakusafw.runtime.stage.directio.StringTemplate.compareInBytes','org.apache.hadoop.io.WritableComparator.compareBytes'
'com.asakusafw.runtime.stage.directio.StringTemplate.PropertyFormatter.PropertyFormatter','org.apache.hadoop.io.Text.<init>'
'com.asakusafw.runtime.stage.directio.StringTemplate.PropertyFormatter.hashCode','org.apache.hadoop.io.Text.hashCode'
'com.asakusafw.runtime.stage.directio.StringTemplate.PropertyFormatter.equals','org.apache.hadoop.io.Text.equals'
'com.asakusafw.runtime.stage.directio.StringTemplate.Constant.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'com.asakusafw.runtime.stage.directio.StringTemplate.Constant.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'com.asakusafw.runtime.stage.directio.StringTemplateTest.ser','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'com.asakusafw.runtime.stage.directio.StringTemplateTest.des','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset'
'.StringTextComparisonTest.text','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.charAt org.apache.hadoop.io.Text.charAt org.apache.hadoop.io.Text.charAt org.apache.hadoop.io.Text.charAt'
'org.apache.mahout.common.StringTuple.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'org.apache.mahout.common.StringTuple.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.write'
'org.lilyproject.repository.impl.primitivevaluetype.StringValueType.fromBytes','org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.repository.impl.primitivevaluetype.StringValueType.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'fr.eurecom.dsg.mapreduce.Stripes.run','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.eurecom.dsg.mapreduce.Stripes.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.StructuredWordSpaceMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.StructuredWordSpaceMR.setupConfiguration','org.apache.hadoop.conf.Configuration.set'
'gov.llnl.ontology.mapreduce.stats.StructuredWordSpaceMR.setupReducer','org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath'
'gov.llnl.ontology.mapreduce.stats.StructuredWordSpaceMR.StructuredWordSpaceMapper.setup','org.apache.hadoop.conf.Configuration.get'
'gov.llnl.ontology.mapreduce.stats.StructuredWordSpaceMR.StructuredWordSpaceMapper.map','org.apache.hadoop.io.IntWritable.<init>'
'gov.llnl.ontology.mapreduce.stats.StructuredWordSpaceMR.RelationTupleReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'co.nubetech.hiho.testdata.Student.readFields','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.readFields org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.readFields'
'co.nubetech.hiho.testdata.Student.write','org.apache.hadoop.io.IntWritable.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.LongWritable.write org.apache.hadoop.io.DoubleWritable.write'
'co.nubetech.hiho.testdata.Student.hashCode','org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.IntWritable.hashCode org.apache.hadoop.io.LongWritable.hashCode org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.DoubleWritable.hashCode'
'co.nubetech.hiho.testdata.Student.equals','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.IntWritable.equals org.apache.hadoop.io.LongWritable.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.DoubleWritable.equals'
'co.nubetech.hiho.testdata.Student.compareTo','org.apache.hadoop.io.Text.compareTo'
'org.commoncrawl.mapred.pipelineV3.domainmeta.subdomaincounts.SubDomainCountsMapper.map','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.subdomaincounts.SubDomainCountsReducer.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.apache.hadoop.io.IntWritable>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.subdomaincounts.SubDomainCountsStep.runStep','org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.JobClient.runJob'
'org.commoncrawl.mapred.pipelineV3.domainmeta.subdomaincounts.SubDomainToQuantcastJoinStep.runStep','org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.oozie.action.oozie.SubWorkflowActionExecutor.injectCallback','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.oozie.SubWorkflowActionExecutor.injectRecovery','org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.filter','org.apache.hadoop.io.Text.toString'
'org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.setConf','org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.filter','org.apache.hadoop.io.Text.toString'
'org.apache.oozie.command.wf.SubmitCommand.call','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitCommand.call','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitHttpXCommand.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitHttpXCommand.addFileSection','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitHttpXCommand.addArchiveSection','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitHttpXCommand.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitHttpXCommand.addFileSection','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitHttpXCommand.addArchiveSection','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitMRCommand.generateConfigurationSection','org.apache.hadoop.conf.Configuration.iterator'
'org.apache.oozie.command.wf.SubmitMRCommand.generateMRSection','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.size'
'org.apache.oozie.command.wf.SubmitMRCommand.getWorkflowXml','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitMRXCommand.generateConfigurationSection','org.apache.hadoop.conf.Configuration.iterator'
'org.apache.oozie.command.wf.SubmitMRXCommand.generateMRSection','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.size'
'org.apache.oozie.command.wf.SubmitMRXCommand.getWorkflowXml','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitPigCommand.generatePigSection','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitPigCommand.getWorkflowXml','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitPigXCommand.generatePigSection','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitPigXCommand.getWorkflowXml','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitXCommand.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.SubmitXCommand.execute','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.graph.SumAggregator.aggregate','org.apache.hadoop.io.DoubleWritable.get'
'org.apache.hama.graph.SumAggregator.getValue','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.nutch.searcher.Summary.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Text.writeString'
'org.apache.nutch.searcher.Summary.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Text.readString'
'org.archive.hadoop.cdx.SummaryGenerator.createSummary','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.open'
'org.archive.hadoop.cdx.SummaryGenerator.createSummaryOld','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.open'
'org.archive.hadoop.cdx.SummaryGenerator.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.create'
'org.archive.hadoop.cdx.SummaryGenerator.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.monitor.servlets.trace.Summary.pageBody','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.iterators.user.SummingArrayCombiner.DOSArrayEncoder.encode','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.apache.accumulo.core.iterators.user.SummingArrayCombiner.DOSArrayEncoder.decode','org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.accumulo.core.iterators.user.SummingArrayCombiner.VarLongArrayEncoder.write','org.apache.hadoop.io.WritableUtils.writeVLong'
'org.apache.accumulo.core.iterators.user.SummingArrayCombiner.VarLongArrayEncoder.read','org.apache.hadoop.io.WritableUtils.readVLong'
'org.commoncrawl.util.SuperDomainList.loadSuperDomainIdList','org.apache.hadoop.fs.FileSystem.get'
'com.ning.sweeper.Sweeper.configureHDFSAccess','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.ning.sweeper.Sweeper.drawBrowser','org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.vectorizer.tfidf.TFIDFPartialVectorReducer.reduce','org.apache.hadoop.io.WritableComparable<?>.toString'
'org.apache.mahout.vectorizer.tfidf.TFIDFPartialVectorReducer.setup','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.vectorizer.tfidf.TFIDFPartialVectorReducer.reduce','org.apache.hadoop.io.WritableComparable<?>.toString'
'org.apache.mahout.vectorizer.tfidf.TFIDFPartialVectorReducer.setup','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init>'
'edu.isi.mavuno.score.TFIDFScorer.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.crunch.TFIDFTest.run','org.apache.hadoop.fs.Path.<init>'
'com.mozilla.grouperfish.pig.eval.ml.TFIDFVectorizer.loadFeatureIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.vectorizer.term.TFPartialVectorReducer.reduce','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.vectorizer.term.TFPartialVectorReducer.setup','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.vectorizer.term.TFPartialVectorReducer.reduce','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.vectorizer.term.TFPartialVectorReducer.setup','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.<init>'
'com.mozilla.grouperfish.pig.eval.ml.TFVectorizer.loadFeatureIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'.TF_IDFFetcher.TF_IDFFetcherMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'.TF_IDFFetcher.getTFIDF','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.Job.submit org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.pig.impl.io.TFileRecordWriter.TFileRecordWriter','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.file.tfile.TFile.Writer.<init>'
'org.apache.pig.impl.io.TFileRecordWriter.close','org.apache.hadoop.io.file.tfile.TFile.Writer.close org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.pig.impl.io.TFileRecordWriter.write','org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.file.tfile.TFile.Writer.prepareAppendKey org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.file.tfile.TFile.Writer.prepareAppendValue'
'org.apache.pig.impl.io.TFileStorage.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'org.apache.pig.impl.io.TFileStorage.TFileOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.impl.io.TFileStorage.setStoreLocation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.apache.nutch.indexer.tld.TLDIndexingFilter.filter','org.apache.hadoop.io.Text.toString'
'com.yahoo.omid.tso.TSOTestBase.setupClient','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt'
'edu.umd.hooka.ttables.TTable_monolithic.TTable_monolithic','org.apache.hadoop.fs.FileSystem.open'
'edu.umd.hooka.ttables.TTable_monolithic.write','org.apache.hadoop.fs.FileSystem.create'
'edu.umd.hooka.ttables.TTable_monolithic_IFAs.TTable_monolithic_IFAs','org.apache.hadoop.fs.FileSystem.open'
'edu.umd.hooka.ttables.TTable_monolithic_IFAs.write','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.accumulo.core.util.TTimeoutTransport.create','org.apache.hadoop.net.SocketInputStream.<init> org.apache.hadoop.net.SocketOutputStream.<init>'
'com.splunk.shuttl.testutil.TUtilsFileSystem.getLocalFileSystem','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'com.splunk.shuttl.testutil.TUtilsFileSystem.getFileFromFileSystem','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile'
'extramuros.java.formats.Table.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'extramuros.java.formats.Table.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'extramuros.java.formats.Table.getConfiguration','org.apache.hadoop.conf.Configuration.<init>'
'extramuros.java.formats.Table.rowsReader','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.formats.Table.iterator','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.formats.Table.clone','org.apache.hadoop.conf.Configuration.<init>'
'extramuros.java.formats.Table.save','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init>'
'com.bah.culvert.test.TableAdapterTestingUtility.testRemoteExec','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.lightboxtechnologies.nsrl.TableDumper.TableDumperMapper.map','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.lightboxtechnologies.nsrl.TableDumper.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.giraph.io.hbase.edgemarker.TableEdgeInputFormat.TableEdgeVertexReader.getCurrentVertex','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.hbase.edgemarker.TableEdgeOutputFormat.TableEdgeVertexWriter.writeVertex','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.getBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.io.Writable>.write'
'extramuros.java.formats.TableHeader.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.write org.apache.hadoop.io.IntWritable.write org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'extramuros.java.formats.TableHeader.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'bixi.hbase.upload.TableInsertLocationS1.insert','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'bixi.hbase.upload.TableInsertLocationS1.endElement','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'bixi.hbase.upload.TableInsertPrev.TableInsertPrev','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush'
'bixi.hbase.upload.TableInsertPrev.processDoc','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.HTable.put'
'bixi.hbase.upload.TableInsertPrev.readData','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'nl.vu.datalayer.hbase.coprocessor.TableInsertResourceToTriple.cleanup','org.apache.hadoop.hbase.client.HTable.close'
'nl.vu.datalayer.hbase.coprocessor.TableInsertResourceToTriple.setup','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush org.apache.hadoop.hbase.client.HTable.setWriteBufferSize org.apache.hadoop.hbase.client.HTable.getRegionsInfo org.apache.hadoop.hbase.client.HTable.prewarmRegionCache'
'nl.vu.datalayer.hbase.coprocessor.TableInsertResourceToTriple.reduce','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'bixi.hbase.upload.TableInsertSchema4.TableInsertSchema4','org.apache.hadoop.hbase.HBaseConfiguration.create'
'bixi.hbase.upload.TableInsertStatistics.TableInsertStatistics','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.setAutoFlush'
'bixi.hbase.upload.TableInsertStatistics.insertRow','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'bixi.hbase.upload.TableInsertStatistics.batchInsertRow','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'bixi.hbase.upload.TableInsertStatistics.batchInsertRow4OneDay','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'admin.TableOperationsExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.isTableDisabled org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.isTableAvailable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.isTableAvailable org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled'
'org.apache.accumulo.core.client.admin.TableOperationsImpl.addSplits','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.admin.TableOperationsImpl.getLocalityGroups','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.admin.TableOperationsImpl.splitRangeByTablets','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.admin.TableOperationsImpl.importDirectory','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir'
'org.apache.accumulo.core.client.admin.TableOperationsImpl.clearLocatorCache','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.admin.TableOperationsImpl.getExportedProps','org.apache.hadoop.fs.FileSystem.open'
'org.apache.accumulo.core.client.admin.TableOperationsImpl.importTable','org.apache.hadoop.fs.Path.<init>'
'org.apache.accumulo.server.master.tableOps.MakeDeleteEntries.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.accumulo.server.master.tableOps.TableRangeOpWait.isReady','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.tableOps.TableRangeOpWait.call','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.master.tableOps.TableRangeOp.call','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.server.master.tableOps.TableRangeOp.undo','org.apache.hadoop.io.Text.<init>'
'org.cloudata.core.parallel.hadoop.TableScanCellReader.TableScanCellReader','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.incrCounter'
'org.cloudata.core.parallel.hadoop.TableScanCellReader.next','org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.mapred.Reporter.incrCounter'
'com.hphoto.server.TableServer.TableServer','org.apache.hadoop.hbase.HBaseAdmin.<init> org.apache.hadoop.hbase.HTable.<init> org.apache.hadoop.hbase.HTable.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.ipc.RPC.waitForProxy org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException'
'com.hphoto.server.TableServer.getUser','org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.io.Text.<init>'
'com.hphoto.server.TableServer.setUser','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit org.apache.hadoop.io.DataOutputBuffer.close'
'com.hphoto.server.TableServer.getCategory','org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.hphoto.server.TableServer.getCategories','org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.hphoto.server.TableServer.setCategory','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.hbase.HTable.put org.apache.hadoop.io.DataOutputBuffer.close org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TableServer.deleteCategory','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.delete org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TableServer.getImage','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.filter.RegExpRowFilter.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.hphoto.server.TableServer.getImages','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.io.Text.<init>'
'com.hphoto.server.TableServer.setImages','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.hbase.HTable.put org.apache.hadoop.io.DataOutputBuffer.close org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TableServer.deleteImages','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.delete org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TableServer.getExif','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.io.Text.<init>'
'com.hphoto.server.TableServer.setExif','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit org.apache.hadoop.io.DataOutputBuffer.close'
'com.hphoto.server.TableServer.deleteExif','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.delete org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TableServer.getTags','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.get'
'com.hphoto.server.TableServer.setTags','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TableServer.getComment','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.get'
'com.hphoto.server.TableServer.SetComment','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TableServer.getAlbum','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.get'
'com.hphoto.server.TableServer.setAlbum','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TableServer.getByteData','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.ArrayWritable.<init> org.apache.hadoop.io.ArrayWritable.write org.apache.hadoop.io.DataOutputBuffer.flush org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.close'
'com.hphoto.server.TableServer.getWritableClass','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.ArrayWritable.<init> org.apache.hadoop.io.ArrayWritable.readFields org.apache.hadoop.io.DataInputBuffer.close org.apache.hadoop.io.ArrayWritable.toArray org.apache.hadoop.io.WritableComparable.isAssignableFrom'
'com.hphoto.server.TableServer.getImageData','org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.hbase.HTable.obtainScanner org.apache.hadoop.hbase.HStoreKey.<init> org.apache.hadoop.hbase.HScannerInterface.next org.apache.hadoop.io.Text.toString org.apache.hadoop.io.WritableFactories.newInstance org.apache.hadoop.io.Text.toString org.apache.hadoop.io.WritableFactories.newInstance org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.DataInputBuffer.close org.apache.hadoop.hbase.HScannerInterface.close'
'com.hphoto.server.TableServer.scanTable','org.apache.hadoop.hbase.HTable.obtainScanner org.apache.hadoop.io.WritableFactories.newInstance org.apache.hadoop.hbase.HStoreKey.<init> org.apache.hadoop.hbase.HScannerInterface.next org.apache.hadoop.io.WritableFactories.newInstance org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.DataInputBuffer.close org.apache.hadoop.hbase.HScannerInterface.next org.apache.hadoop.hbase.HScannerInterface.close org.apache.hadoop.io.WritableComparable.isAssignableFrom'
'org.apache.accumulo.examples.simple.mapreduce.TableToFile.TTFMapper.map','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.examples.simple.mapreduce.TableToFile.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'org.apache.accumulo.examples.simple.mapreduce.TableToFile.main','org.apache.hadoop.util.ToolRunner.run'
'extramuros.java.jobs.utils.SeqDirectoryIterator.SeqDirectoryIterator','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileStatus.getPath'
'extramuros.java.jobs.utils.TableUtils.readAbstractTable','org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.utils.TableUtils.readFirstWritable','org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.utils.TableUtils.writeSingleWritable','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.Writable.getClass'
'extramuros.java.jobs.utils.TableUtils.fileSeqIterator','org.apache.hadoop.fs.FileSystem.get'
'extramuros.java.jobs.utils.TableUtils.parseWritableKeyId','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.FloatWritable.get'
'org.apache.accumulo.server.monitor.servlets.TablesServlet.doTableDetails','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.impl.TabletLocator.LocatorKey.hashCode','org.apache.hadoop.io.Text.hashCode'
'org.apache.accumulo.core.client.impl.TabletLocator.LocatorKey.equals','org.apache.hadoop.io.Text.equals'
'org.apache.accumulo.core.client.impl.TabletLocator.getInstance','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.impl.TabletLocator._locateTablet','org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.core.client.impl.TabletLocatorImpl.EndRowComparator.compare','org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.core.client.impl.TabletLocatorImpl.TabletLocatorImpl','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.client.impl.TabletLocatorImpl.binMutations','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'org.apache.accumulo.core.client.impl.TabletLocatorImpl.compare','org.apache.hadoop.io.WritableComparator.compareBytes'
'org.apache.accumulo.core.client.impl.TabletLocatorImpl.binRanges','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.client.impl.TabletLocatorImpl.lookupTabletLocation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.compareTo'
'org.apache.accumulo.core.client.impl.TabletLocatorImpl.rowAfterPrevRow','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append'
'org.apache.accumulo.core.client.impl.TabletLocatorImpl._locateTablet','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append'
'org.apache.accumulo.server.tabletserver.TabletServer.ThriftClientHandler.getTabletStats','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.tabletserver.TabletServer.ThriftClientHandler.flush','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.tabletserver.TabletServer.ThriftClientHandler.compact','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.tabletserver.TabletServer.ThriftClientHandler.removeLogs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.accumulo.server.tabletserver.TabletServer.splitTablet','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.tabletserver.TabletServer.run','org.apache.hadoop.fs.FileSystem.close'
'org.apache.accumulo.server.tabletserver.TabletServer.verifyTabletInformation','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.tabletserver.TabletServer.ensureHdfsSyncIsEnabled','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getConf'
'org.apache.accumulo.server.tabletserver.TabletServer.recoverLocalWriteAheadLogs','org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.SequenceFile.Reader.next org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.io.SequenceFile.Reader.close org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.accumulo.server.tabletserver.TabletServer.recover','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.accumulo.core.client.impl.TabletServerBatchWriter.MutationWriter.getLocator','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.impl.TabletServerBatchWriter.MutationWriter.binMutations','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.impl.TabletServerBatchWriter.MutationWriter.SendTask.send','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.impl.TabletServerBatchWriter.MutationWriter.sendMutationsToTabletServer','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'gov.llnl.ontology.mapreduce.stats.TagDocumentMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.TagDocumentMR.setupReducer','org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'gov.llnl.ontology.mapreduce.stats.TagDocumentMR.TagDocumentMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.input.TagTokenizer.addToken','org.apache.hadoop.io.Text.<init>'
'tap.formats.avro.TapAvroSerialization.getMapOutClass','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getClassByName'
'tap.formats.avro.TapAvroSerialization.getSerializer','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get'
'cascading.tap.hadoop.io.TapOutputCollector.TapOutputCollector','org.apache.hadoop.mapred.JobConf.get'
'cascading.tap.hadoop.io.TapOutputCollector.initialize','org.apache.hadoop.mapred.JobConf.getOutputFormat org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.OutputFormat.getRecordWriter'
'cascading.tap.hadoop.io.TapOutputCollector.collect','org.apache.hadoop.mapred.RecordWriter.write'
'cascading.tap.hadoop.io.TapOutputCollector.close','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.RecordWriter.close'
'tap.core.mapreduce.input.TapfileRecordReader.TapfileRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getLen'
'tap.core.mapreduce.input.TapfileRecordReader.readMessageClass','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.seek'
'tap.core.mapreduce.input.TapfileRecordReader.sniffFileFormat','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FSDataInputStream.close'
'tap.core.mapreduce.input.TapfileRecordReader.moveToNextDataBlock','org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.seek'
'tap.core.mapreduce.input.TapfileRecordReader.readTrailer','org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.fs.FSDataInputStream.seek'
'tap.core.mapreduce.input.TapfileRecordReader.readIndexEntries','org.apache.hadoop.fs.FSDataInputStream.seek'
'tap.core.mapreduce.input.TapfileRecordReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'tap.core.mapreduce.input.TapfileRecordReader.getPos','org.apache.hadoop.fs.FSDataInputStream.getPos'
'tap.core.mapreduce.input.TapfileRecordReaderTests.testCanReadFile','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.RawLocalFileSystem.<init> org.apache.hadoop.fs.RawLocalFileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init>'
'edu.jhu.thrax.hadoop.features.TargetWordCounterFeature.score','org.apache.hadoop.io.IntWritable.<init>'
'edu.jhu.thrax.hadoop.tools.TargetWordGivenSourceWordProbabilityTool.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.submit'
'edu.jhu.thrax.hadoop.tools.TargetWordGivenSourceWordProbabilityTool.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hama.bsp.TaskCompletionEvent.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeEnum org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVInt'
'org.apache.hama.bsp.TaskCompletionEvent.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readEnum org.apache.hadoop.io.WritableUtils.readString org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.util.TaskDataUtils.initializeTaskDataJobConfig','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.commoncrawl.util.TaskDataUtils.getTaskDataJobIdFromJobConfig','org.apache.hadoop.mapred.JobConf.get'
'org.commoncrawl.util.TaskDataUtils.TaskDataClient.TaskDataClient','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.TaskAttemptID.forName org.apache.hadoop.mapred.TaskAttemptID.getTaskID'
'org.apache.sqoop.util.TaskId.get','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.sqoop.util.TaskId.getLocalWorkPath','org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.bsp.TaskLog.cleanup','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.hama.bsp.TaskLog.captureOutAndError','org.apache.hadoop.fs.FileUtil.makeShellPath org.apache.hadoop.fs.FileUtil.makeShellPath'
'org.apache.hama.bsp.TaskLog.addCommand','org.apache.hadoop.fs.FileUtil.makeShellPath'
'org.apache.hama.bsp.TaskLog.captureDebugOut','org.apache.hadoop.fs.FileUtil.makeShellPath org.apache.hadoop.fs.FileUtil.makeShellPath'
'edu.duke.starfish.whatif.oracle.TaskProfileOracle.calcVirtualTaskStatistics','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'edu.duke.starfish.whatif.oracle.TaskProfileOracle.calcVirtualTaskCosts','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'com.manning.hip.ch13.TaskSplitReader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'com.manning.hip.ch13.TaskSplitReader.getSplitDetails','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.Text.readString org.apache.hadoop.conf.Configuration.getClassByName org.apache.hadoop.io.serializer.SerializationFactory.<init> org.apache.hadoop.io.serializer.SerializationFactory.getDeserializer org.apache.hadoop.io.serializer.Deserializer<com.manning.hip.ch13.T>.open org.apache.hadoop.io.serializer.Deserializer<com.manning.hip.ch13.T>.deserialize org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.hama.bsp.TaskStatus.readFields','org.apache.hadoop.io.WritableUtils.readEnum org.apache.hadoop.io.Text.readString org.apache.hadoop.io.WritableUtils.readEnum'
'org.apache.hama.bsp.TaskStatus.write','org.apache.hadoop.io.WritableUtils.writeEnum org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.WritableUtils.writeEnum'
'org.fusesource.fabric.hadoop.mapred.TaskTrackerFactory.doCreate','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.TaskTracker.<init> org.apache.hadoop.util.Daemon.<init> org.apache.hadoop.util.Daemon.start'
'org.fusesource.fabric.hadoop.mapred.TaskTrackerFactory.doDelete','org.apache.hadoop.mapred.TaskTracker.shutdown'
'org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils.readItemIDIndexMap','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils.readItemIDIndexMap','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils.readIntFromFile','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream'
'com.mozilla.telemetry.pig.eval.json.TelemetryInvalidCounts.readLookupFile','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'com.mozilla.telemetry.pig.eval.json.TelemetryInvalidCounts.readReferenceJson','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'cascading.tap.hadoop.util.TempHfs.NullScheme.sinkConfInit','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat'
'cascading.tap.hadoop.util.TempHfs.initTemporaryPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'test.modelgen.table.model.TempImportTarget1.setTextdata1','org.apache.hadoop.io.Text.modify'
'test.modelgen.table.model.TempImportTarget1.setDuplicateFlg','org.apache.hadoop.io.Text.modify'
'org.apache.hcatalog.templeton.tool.TempletonUtils.encodeArray','org.apache.hadoop.util.StringUtils.escapeString org.apache.hadoop.util.StringUtils.arrayToString'
'org.apache.hcatalog.templeton.tool.TempletonUtils.decodeArray','org.apache.hadoop.util.StringUtils.split org.apache.hadoop.util.StringUtils.unEscapeString'
'org.apache.hcatalog.templeton.tool.TempletonUtils.hadoopFsListAsString','org.apache.hadoop.util.StringUtils.arrayToString'
'org.apache.hcatalog.templeton.tool.TempletonUtils.hadoopFsFilename','org.apache.hadoop.fs.Path.toString'
'org.apache.hcatalog.templeton.tool.TempletonUtils.hadoopFsIsMissing','org.apache.hadoop.fs.FileSystem.exists'
'org.apache.hcatalog.templeton.tool.TempletonUtils.hadoopFsPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.security.UserGroupInformation.getLoginUser org.apache.hadoop.security.UserGroupInformation.doAs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified'
'org.apache.hcatalog.templeton.tool.TempletonUtils.run','org.apache.hadoop.fs.FileSystem.get'
'org.apache.hcatalog.templeton.test.tool.TempletonUtilsTest.testHadoopFsPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.templeton.test.tool.TempletonUtilsTest.testHadoopFsFilename','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.templeton.test.tool.TempletonUtilsTest.testHadoopFsListAsArray','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.templeton.test.tool.TempletonUtilsTest.testHadoopFsListAsString','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'com.asakusafw.testdriver.temporary.TemporaryDataModelSource.TemporaryDataModelSource','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.testdriver.temporary.TemporaryImporterPreparatorTest.setUp','org.apache.hadoop.fs.FileSystem.get'
'com.asakusafw.testdriver.temporary.TemporaryImporterPreparatorTest.tearDown','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.testdriver.temporary.TemporaryImporterPreparatorTest.simple','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.asakusafw.testdriver.temporary.TemporaryInputPreparator.truncate','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.testdriver.temporary.TemporaryInputPreparator.createOutput','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.testdriver.temporary.TemporaryOutputRetriever.truncate','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.delete'
'com.asakusafw.testdriver.temporary.TemporaryOutputRetriever.createOutput','org.apache.hadoop.fs.Path.<init>'
'com.asakusafw.runtime.stage.temporary.TemporaryStorage.list','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.asakusafw.runtime.stage.temporary.TemporaryStorage.openInput','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getUri'
'com.asakusafw.runtime.stage.temporary.TemporaryStorage.openOutput','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.SequenceFile.createWriter'
'com.hadoopilluminated.examples.terasort.TeraGen.RangeInputFormat.RangeInputSplit.readFields','org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong'
'com.hadoopilluminated.examples.terasort.TeraGen.RangeInputFormat.RangeInputSplit.write','org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong'
'com.hadoopilluminated.examples.terasort.TeraGen.RangeInputFormat.RangeRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'com.hadoopilluminated.examples.terasort.TeraGen.RangeInputFormat.RangeRecordReader.createValue','org.apache.hadoop.io.NullWritable.get'
'com.hadoopilluminated.examples.terasort.TeraGen.RangeInputFormat.RangeRecordReader.next','org.apache.hadoop.io.LongWritable.set'
'com.hadoopilluminated.examples.terasort.TeraGen.getNumberOfRows','org.apache.hadoop.mapred.JobConf.getLong'
'com.hadoopilluminated.examples.terasort.TeraGen.setNumberOfRows','org.apache.hadoop.mapred.JobConf.setLong'
'com.hadoopilluminated.examples.terasort.TeraGen.SortGenMapper.addKey','org.apache.hadoop.io.Text.set'
'com.hadoopilluminated.examples.terasort.TeraGen.SortGenMapper.addRowId','org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'com.hadoopilluminated.examples.terasort.TeraGen.SortGenMapper.addFiller','org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'com.hadoopilluminated.examples.terasort.TeraGen.SortGenMapper.map','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.clear org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'com.hadoopilluminated.examples.terasort.TeraGen.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'com.hadoopilluminated.examples.terasort.TeraGen.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.util.ToolRunner.run'
'com.hadoopilluminated.examples.terasort.TeraOutputFormat.setFinalSync','org.apache.hadoop.mapred.JobConf.setBoolean'
'com.hadoopilluminated.examples.terasort.TeraOutputFormat.getFinalSync','org.apache.hadoop.mapred.JobConf.getBoolean'
'com.hadoopilluminated.examples.terasort.TeraOutputFormat.TeraRecordWriter.write','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'com.hadoopilluminated.examples.terasort.TeraOutputFormat.TeraRecordWriter.close','org.apache.hadoop.fs.FSDataOutputStream.sync'
'com.hadoopilluminated.examples.terasort.TeraOutputFormat.getRecordWriter','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.hadoopilluminated.examples.terasort.TeraSort.TotalOrderPartitioner.InnerTrieNode.findPartition','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes'
'com.hadoopilluminated.examples.terasort.TeraSort.TotalOrderPartitioner.LeafTrieNode.findPartition','org.apache.hadoop.io.Text.compareTo'
'com.hadoopilluminated.examples.terasort.TeraSort.TotalOrderPartitioner.readPartitions','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'com.hadoopilluminated.examples.terasort.TeraSort.TotalOrderPartitioner.buildTrie','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getBytes'
'com.hadoopilluminated.examples.terasort.TeraSort.TotalOrderPartitioner.configure','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'com.hadoopilluminated.examples.terasort.TeraSort.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.filecache.DistributedCache.createSymlink org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobClient.runJob'
'com.hadoopilluminated.examples.terasort.TeraSort.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.RangeInputFormat.RangeInputSplit.readFields','org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.WritableUtils.readVLong'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.RangeInputFormat.RangeInputSplit.write','org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.WritableUtils.writeVLong'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.RangeInputFormat.RangeRecordReader.getCurrentKey','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.RangeInputFormat.RangeRecordReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.RangeInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.JobContext.getConfiguration'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.SortGenMapper.addKey','org.apache.hadoop.io.Text.set'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.SortGenMapper.getRowIdString','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.SortGenMapper.addFiller','org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.SortGenMapper.map','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.SortGenMapper.setup','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.examples.simple.mapreduce.TeraSortIngest.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'com.bah.culvert.data.index.TermBasedIndex.setTokenRegex','org.apache.hadoop.conf.Configuration.set'
'com.bah.culvert.data.index.TermBasedIndex.setSplitable','org.apache.hadoop.conf.Configuration.setBoolean'
'com.bah.culvert.data.index.TermBasedIndex.setToLower','org.apache.hadoop.conf.Configuration.setBoolean'
'com.bah.culvert.data.index.TermBasedIndexTest.setup','org.apache.hadoop.conf.Configuration.<init>'
'cc.mrlda.TermCombiner.reduce','org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfInts,org.apache.hadoop.io.DoubleWritable>.collect'
'org.apache.mahout.vectorizer.term.TermCountCombiner.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.vectorizer.term.TermCountMapper.apply','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.vectorizer.term.TermCountReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'io.TermDocIdWritable.TermDocIdWritable','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'io.TermDocIdWritable.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.LongWritable.write'
'io.TermDocIdWritable.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.LongWritable.readFields'
'io.TermDocIdWritable.hashCode','org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.LongWritable.hashCode'
'io.TermDocIdWritable.equals','org.apache.hadoop.io.LongWritable.equals org.apache.hadoop.io.Text.equals'
'io.TermDocIdWritable.toString','org.apache.hadoop.io.LongWritable.get'
'io.TermDocIdWritable.compareTo','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.LongWritable.compareTo'
'org.solbase.lucenehbase.TermDocMetadata.TermDocMetadata','org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.mahout.vectorizer.term.TermDocumentCountReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init>'
'cc.mrlda.TermReducer.configure','org.apache.hadoop.mapred.lib.MultipleOutputs.<init> org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.IOUtils.closeStream'
'cc.mrlda.TermReducer.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.DoubleWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.DoubleWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfIntFloat,edu.umd.cloud9.io.map.HMapIFW>.collect'
'cc.mrlda.TermReducer.close','org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfIntFloat,edu.umd.cloud9.io.map.HMapIFW>.collect org.apache.hadoop.mapred.OutputCollector<edu.umd.cloud9.io.pair.PairOfIntFloat,edu.umd.cloud9.io.map.HMapIFW>.collect org.apache.hadoop.mapred.lib.MultipleOutputs.close'
'org.cloudata.examples.web.TermUploadJob.exec','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setMaxMapAttempts org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMaxReduceAttempts org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.delete'
'org.hbasene.index.TermVectorPutTask.generatePut','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.putInt org.apache.hadoop.hbase.util.Bytes.putInt org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.setWriteToWAL'
'org.cloudata.examples.web.TermWeightJobOnline.exec','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.cloudata.examples.web.TermWeightMap.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.collect'
'org.cloudata.examples.web.TermWeightReduce.reduce','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.collect org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.collect org.apache.hadoop.io.Text.getBytes'
'org.cloudata.examples.web.TermWeightReduce.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.create'
'com.yahoo.omid.TestAbortTransaction.runTestInterleaveScan','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.client.ResultScanner.next'
'org.apache.giraph.io.accumulo.TestAccumuloVertexFormat.testAccumuloInputOutput','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.toString'
'org.apache.oozie.command.wf.TestActionCheckXCommand.testActionCheck','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.apache.oozie.command.wf.TestActionCheckXCommand.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.command.wf.TestActionCheckXCommand.createWorkflowActionSetPending','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.wf.TestActionErrors.testKillNodeErrorMessage','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestActionErrors._testNonTransient','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestActionErrors._testNonTransientWithCoordActionUpdate','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestActionErrors._testTransient','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'org.apache.oozie.command.wf.TestActionErrors._testError','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestActionErrors._testErrorWithUserRetry','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestActionErrors._testDataNotSet','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.action.TestActionFailover.testFsFailover','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.TestActionFailover.testFsFailover','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.TestActionFailover.testFsFailover','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.command.wf.TestActionStartXCommand.testActionStart','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.apache.oozie.command.wf.TestActionStartXCommand.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.command.wf.TestActionStartXCommand.testActionWithEscapedStringAndCDATA','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.apache.oozie.command.wf.TestActionStartXCommand.addRecordToWfJobTableWithCustomAppPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestActionStartXCommand.createWorkflowActionWithAppPathConfig','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.wf.TestActionStartXCommand.createWorkflowActionSetPending','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.wf.TestActionStartXCommand.addRecordToWfJobTableWithEscapedStringAndCDATA','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestActionStartXCommand.createWorkflowActionSetPendingWithEscapedStringAndCDATA','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.wf.TestActionStartXCommand.testActionStart','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.apache.oozie.command.wf.TestActionStartXCommand.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.command.wf.TestActionStartXCommand.addRecordToWfJobTableWithCustomAppPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestActionStartXCommand.createWorkflowActionWithAppPathConfig','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.wf.TestActionStartXCommand.createWorkflowActionSetPending','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.giraph.io.TestAdjacencyListTextVertexOutputFormat.testVertexWithNoEdges','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.TestAdjacencyListTextVertexOutputFormat.testVertexWithEdges','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.TestAdjacencyListTextVertexOutputFormat.testWithDifferentDelimiter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.spectral.common.TestAffinityMatrixInputJob.testAffinityMatrixInputMapper','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.spectral.common.TestAffinityMatrixInputJob.testAffinitymatrixInputReducer','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.graph.TestAggregatorsHandling.testMasterAggregatorsSerialization','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.giraph.graph.TestAggregatorsHandling.testAggregatorsCheckpointing','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'com.cloudera.sqoop.TestAllTables.testMultiTableImport','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.closeStream'
'pl.edu.icm.coansys.similarity.pig.script.TestAllpairsCosineSimilarity.afterClass','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.testSink','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getFamilyMap org.apache.hadoop.hbase.client.HTable.close'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.testCreatePutWithoutSystemColumnFamily','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.get org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.get org.apache.hadoop.hbase.util.Bytes.toString'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.testDontWriteBody','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.get'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.testCreatePutWithoutExplicitRowKey','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.testAddAttributeWithSystemColumnFamSpecified','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.<init>'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.testAddAttributeWithoutSystemColumnFam','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.<init>'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.assertHasSingleKeyValue','org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.get org.apache.hadoop.hbase.util.Bytes.compareTo'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.assertEmpty','org.apache.hadoop.hbase.client.Put.getFamilyMap'
'com.cloudera.flume.hbase.TestAttr2HBaseSink.testAddAttributeWithSystemColumnFamSpecifiedAndEmptyPrefix','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.<init>'
'org.apache.oozie.service.TestAuthorizationService.setUp','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestAuthorizationService.testAuthorizationService','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.setOwner org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission'
'org.apache.oozie.service.TestAuthorizationService.testErrors','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.service.TestAuthorizationService.setUp','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestAuthorizationService.testAuthorizationService','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.setOwner org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission'
'org.apache.oozie.service.TestAuthorizationService.testErrors','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.toString'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.setup','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertBooleanWritable','org.apache.hadoop.io.BooleanWritable.<init>'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertBytesWritable','org.apache.hadoop.io.BytesWritable.<init>'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertByteWritable','org.apache.hadoop.io.ByteWritable.<init>'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertDoubleWritable','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertFloatWritable','org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertIntWritable','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertLongWritable','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertNullWritable','org.apache.hadoop.io.NullWritable.get'
'org.apache.avro.hadoop.io.TestAvroDatumConverterFactory.testConvertText','org.apache.hadoop.io.Text.<init>'
'com.linkedin.haivvreo.TestAvroDeserializer.canDeserializeVoidType','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector'
'com.linkedin.haivvreo.TestAvroDeserializer.canDeserializeMapsWithPrimitiveKeys','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldsDataAsList org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData'
'com.linkedin.haivvreo.TestAvroDeserializer.canDeserializeArrays','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListLength org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListElementObjectInspector org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListElement org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListElement org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListElement org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.getPrimitiveJavaObject'
'com.linkedin.haivvreo.TestAvroDeserializer.canDeserializeRecords','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldName org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldsDataAsList org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData'
'com.linkedin.haivvreo.TestAvroDeserializer.canDeserializeUnions','org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector.getTag org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector.getTag'
'com.linkedin.haivvreo.TestAvroDeserializer.unionTester','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldName org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector.getField'
'com.linkedin.haivvreo.TestAvroDeserializer.canDeserializeEnums','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldName org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject'
'com.linkedin.haivvreo.TestAvroDeserializer.canDeserializeFixed','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldsDataAsList org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData'
'com.linkedin.haivvreo.TestAvroDeserializer.canDeserializeBytes','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldsDataAsList org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData'
'com.linkedin.haivvreo.TestAvroDeserializer.verifyNullableType','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldsDataAsList org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef org.apache.hadoop.hive.serde2.objectinspector.StructField.getFieldObjectInspector org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject'
'com.cloudera.sqoop.TestAvroImportExportRoundtrip.runExport','org.apache.hadoop.util.StringUtils.stringifyException'
'com.wibidata.avro.mapreduce.TestAvroKeyInputFormat.testCreateRecordReader','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.Object>,org.apache.hadoop.io.NullWritable>.close'
'org.apache.avro.mapreduce.TestAvroKeyInputFormat.testCreateRecordReader','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.Object>,org.apache.hadoop.io.NullWritable>.close'
'com.wibidata.avro.mapreduce.TestAvroKeyOutputFormat.testWithNullCodec','org.apache.hadoop.conf.Configuration.<init>'
'com.wibidata.avro.mapreduce.TestAvroKeyOutputFormat.testWithDeflateCodec','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt'
'com.wibidata.avro.mapreduce.TestAvroKeyOutputFormat.testGetRecordWriter','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptID.<init>'
'org.apache.avro.mapreduce.TestAvroKeyOutputFormat.testWithNullCodec','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.avro.mapreduce.TestAvroKeyOutputFormat.testWithDeflateCodec','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt'
'org.apache.avro.mapreduce.TestAvroKeyOutputFormat.testWithSnappyCode','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set'
'org.apache.avro.mapreduce.TestAvroKeyOutputFormat.testGetRecordWriter','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskAttemptID.forName'
'com.wibidata.avro.mapreduce.TestAvroKeyRecordReader.testReadRecords','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getProgress org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getProgress org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.close'
'org.apache.avro.mapreduce.TestAvroKeyRecordReader.testReadRecords','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getProgress org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.getProgress org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.hadoop.io.NullWritable>.close'
'com.wibidata.avro.mapreduce.TestAvroKeyValueRecordReader.testReadRecords','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.getProgress org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.getProgress org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.avro.mapred.AvroKey<java.lang.CharSequence>,org.apache.avro.mapred.AvroValue<java.lang.Integer>>.close'
'com.wibidata.avro.mapreduce.TestAvroKeyValueRecordWriter.testWriteRecords','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.avro.mapred.TestAvroMultipleOutputs.testJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.avro.mapred.TestAvroMultipleOutputs.testProjection','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init> org.apache.hadoop.io.NullWritable.get'
'org.apache.avro.mapred.TestAvroMultipleOutputs.testProjection1','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init> org.apache.hadoop.io.NullWritable.get'
'org.apache.avro.mapred.TestAvroMultipleOutputs.testJob_noreducer','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.avro.mapred.TestAvroMultipleOutputs.testProjection_noreducer','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init> org.apache.hadoop.io.NullWritable.get'
'com.linkedin.haivvreo.TestAvroObjectInspectorGenerator.failOnNonRecords','org.apache.hadoop.hive.serde2.SerDeException.getMessage'
'com.linkedin.haivvreo.TestAvroObjectInspectorGenerator.primitiveTypesWorkCorrectly','org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getAllStructFieldRefs org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getTypeName'
'com.linkedin.haivvreo.TestAvroObjectInspectorGenerator.canHandleMapsWithPrimitiveValueTypes','org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapValueTypeInfo org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapKeyTypeInfo'
'com.linkedin.haivvreo.TestAvroObjectInspectorGenerator.canHandleArrays','org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo'
'com.linkedin.haivvreo.TestAvroObjectInspectorGenerator.canHandleRecords','org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getCategory org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldNames org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldTypeInfos'
'com.linkedin.haivvreo.TestAvroObjectInspectorGenerator.canHandleFixed','org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo'
'com.linkedin.haivvreo.TestAvroObjectInspectorGenerator.canHandleBytes','org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo'
'com.linkedin.haivvreo.TestAvroObjectInspectorGenerator.convertsNullableTypes','org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory'
'org.apache.avro.mapred.TestAvroOutputFormat.testSetSyncInterval','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getInt'
'org.apache.avro.hadoop.io.TestAvroSerialization.testGetSerializerForKey','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.avro.hadoop.io.TestAvroSerialization.testGetSerializerForValue','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.avro.hadoop.io.TestAvroSerialization.testGetDeserializerForKey','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.avro.hadoop.io.TestAvroSerialization.testGetDeserializerForValue','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.avro.hadoop.io.TestAvroSerialization.testClassPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClassLoader'
'org.apache.gora.avro.store.TestAvroStore.deletePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'com.datasalt.pangool.examples.avro.TestAvroTweetsJoin.test','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hama.bsp.TestBSPMasterGroomServer.testSubmitJob','org.apache.hadoop.fs.FileSystem.get'
'org.apache.hama.bsp.TestBSPMasterGroomServer.checkOutput','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.hama.bsp.message.compress.TestBSPMessageCompressor.testCompression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClass'
'org.apache.hama.bsp.TestBSPTaskFaults.TestBSPProcessRunner.main','org.apache.hadoop.ipc.RPC.getProxy'
'org.apache.hama.bsp.TestBSPTaskFaults.setUp','org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.ipc.Server.start org.apache.hadoop.ipc.RPC.getProxy'
'org.apache.hama.bsp.TestBSPTaskFaults.testPing','org.apache.hadoop.ipc.Server.getListenerAddress'
'org.apache.hama.bsp.TestBSPTaskFaults.testPingOnTaskSetupFailure','org.apache.hadoop.ipc.Server.getListenerAddress'
'org.apache.hama.bsp.TestBSPTaskFaults.testPingOnTaskExecFailure','org.apache.hadoop.ipc.Server.getListenerAddress'
'org.apache.hama.bsp.TestBSPTaskFaults.testPingOnTaskCleanupFailure','org.apache.hadoop.ipc.Server.getListenerAddress'
'org.apache.hama.bsp.TestBSPTaskFaults.testBSPTaskSelfDestroy','org.apache.hadoop.ipc.Server.getListenerAddress org.apache.hadoop.ipc.Server.stop'
'org.apache.hama.bsp.TestBSPTaskFaults.tearDown','org.apache.hadoop.ipc.Server.stop'
'com.nesscomputing.hbase.spill.TestBinaryConverter.testSimple','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.client.Put.has org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.client.Put.getFamilyMap'
'com.cloudera.sqoop.lib.TestBlobRef.testExternalSubdir','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.cloudera.sqoop.lib.TestBlobRef.doExternalTest','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.executor.jpa.TestBulkUpdateInsertJPAExecutor.testInserts','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.bundle.TestBundleJobSuspendXCommand.testBundleSuspend2','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.bundle.TestBundleJobSuspendXCommand.testBundleSuspend3','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.bundle.TestBundleKillXCommand.testBundleKill2','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.bundle.TestBundleKillXCommand.testBundleKill3','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.bundle.TestBundleStartXCommand.testBundleStart2','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.bundle.TestBundleStartXCommand.testBundleStart2','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.cdbw.TestCDbwEvaluator.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.cdbw.TestCDbwEvaluator.testCanopy','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.cdbw.TestCDbwEvaluator.testKmeans','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.cdbw.TestCDbwEvaluator.testFuzzyKmeans','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.cdbw.TestCDbwEvaluator.testMeanShift','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.cdbw.TestCDbwEvaluator.testDirichlet','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyMapperManhattan','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyMapperEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyReducerManhattan','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyReducerEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyGenManhattanMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyGenEuclideanMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringManhattanSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringEuclideanSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringEuclideanWithOutlierRemovalSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringManhattanMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringEuclideanMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringEuclideanWithOutlierRemovalMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyReducerT3T4Configuration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyMapperClusterFilter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyReducerClusterFilter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyMapperManhattan','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyMapperEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyReducerManhattan','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyReducerEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyGenManhattanMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyGenEuclideanMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusterMapperManhattan','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusterMapperEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringManhattanSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringEuclideanSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringManhattanMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testClusteringEuclideanMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testUserDefinedDistanceMeasure','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.clustering.canopy.TestCanopyCreation.testCanopyReducerT3T4Configuration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.bookkeeper.benchmark.TestClient.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getDefaultReplication org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.bookkeeper.benchmark.TestClient.HDFSClient.call','org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.hflush'
'com.cloudera.sqoop.lib.TestClobRef.testExternalSubdir','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'com.cloudera.sqoop.lib.TestClobRef.doExternalTest','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.delete'
'com.inmobi.databus.TestCluster.testBasicCluster','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.iterator.TestClusterClassifier.writeAndRead','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.iterator.TestClusterClassifier.testSeqFileClusterIteratorKMeans','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.iterator.TestClusterClassifier.testMRFileClusterIteratorKMeans','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.TestClusterDumper.finalClusterPath','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.mahout.clustering.TestClusterDumper.testCanopy','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testKmeans','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testFuzzyKmeans','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testMeanShift','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testDirichlet','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testDirichlet2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testDirichlet3','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testKmeansSVD','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testKmeansDSVD','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.TestClusterDumper.testKmeansDSVD2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.bsp.TestClusterStatus.testWriteAndReadFields','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.nutch.clustering.carrot2.TestClusterer.setUp','org.apache.hadoop.conf.Configuration.<init>'
'com.datasalt.pangool.benchmark.test.TestCoGroupers.prepare','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.io.TestCodecMap.verifyCodec','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.compress.CompressionCodec.getClass'
'com.cloudera.sqoop.io.TestCodecMap.verifyShortName','org.apache.hadoop.conf.Configuration.<init>'
'com.cloudera.sqoop.io.TestCodecMap.testUnrecognizedCodec','org.apache.hadoop.conf.Configuration.<init>'
'sizzle.aggregators.TestCollectionAggregator.testCollectionAggregatorCombine','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestCollectionAggregator.testCollectionAggregatorReduce','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'com.yahoo.omid.TestCompaction.testDeleteOld','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.flush org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Result.getColumn org.apache.hadoop.hbase.client.HBaseAdmin.compact org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Result.getColumn org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Result.getColumn org.apache.hadoop.hbase.client.Result.getColumn'
'com.yahoo.omid.TestCompaction.testLimitEqualToColumns','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.flush org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Result.getColumn org.apache.hadoop.hbase.client.HBaseAdmin.compact org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getColumn'
'com.cloudera.sqoop.TestCompression.getArgv','org.apache.hadoop.io.compress.CompressionCodec.getClass'
'com.cloudera.sqoop.TestCompression.runSequenceFileCompressionTest','org.apache.hadoop.io.compress.GzipCodec.<init> org.apache.hadoop.io.compress.CompressionCodec.getClass org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IOUtils.closeStream'
'com.cloudera.sqoop.TestCompression.runTextCompressionTest','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.compress.GzipCodec.<init> org.apache.hadoop.util.ReflectionUtils.setConf org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream'
'com.cloudera.sqoop.TestCompression.testBzip2TextCompression','org.apache.hadoop.io.compress.BZip2Codec.<init>'
'com.cloudera.sqoop.TestCompression.testBzip2SequenceFileCompression','org.apache.hadoop.io.compress.BZip2Codec.<init>'
'com.datasalt.pangool.tuplemr.TestConfigParsing.testExtended','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'com.datasalt.pangool.tuplemr.TestConfigParsing.testWithFieldAliases','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.executor.jpa.TestCoordActionGetForInfoJPAExecutor.testCoordActionGet','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.executor.jpa.TestCoordActionGetForStartJPAExecutor.testCoordActionGet','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.executor.jpa.TestCoordActionGetForTimeoutJPAExecutor.testCoordActionGet','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNonExistingNameNode','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.coord.TestCoordActionStartXCommand.getCoordConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.coord.TestCoordELEvaluator.testCreateFreqELValuator','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.coord.TestCoordELEvaluator.testCreateFreqELValuator','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.coord.TestCoordELFunctions.testLatest','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.coord.TestCoordELFunctions.testFuture','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.coord.TestCoordELFunctions.testLatest','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.coord.TestCoordELFunctions.testFuture','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.executor.jpa.TestCoordJobGetActionForNominalTimeJPAExecutor.testCoordActionGet','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.executor.jpa.TestCoordJobGetActionIdsForDatesJPAExecutor.testCoordActionGet','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.executor.jpa.TestCoordJobGetActionsForDatesJPAExecutor.testCoordActionGet','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionGet','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.executor.jpa.TestCoordJobsGetForPurgeJPAExecutor.insertJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.executor.jpa.TestCoordJobsGetForPurgeJPAExecutor.getCoordConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.executor.jpa.TestCoordJobsGetForPurgeJPAExecutor.insertJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.executor.jpa.TestCoordJobsGetForPurgeJPAExecutor.getCoordConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.executor.jpa.TestCoordJobsToBeMaterializedJPAExecutor.insertJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.executor.jpa.TestCoordJobsToBeMaterializedJPAExecutor.getCoordConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.service.TestCoordMaterializeTriggerService.createCoordJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.service.TestCoordMaterializeTriggerService.createCoordJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.coord.TestCoordRerunXCommand.testCoordRerunRefresh','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.command.coord.TestCoordRerunXCommand.testCoordRerunCleanup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.command.coord.TestCoordRerunXCommand.addRecordToJobTable','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.coord.TestCoordRerunXCommand.addRecordToActionTable','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.coord.TestCoordRerunXCommand.getCoordProp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.coord.TestCoordRerunXCommand.writeToFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmit','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithStartTimeAfterEndTime','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithMultipleInstancesInputEvent','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithMultipleStartInstancesInputEvent','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithMultipleEndInstancesInputEvent','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithMultipleInstancesOutputEvent','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithBundleId','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithWrongNamespace','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithSLA','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitFixedValues','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSchemaError','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitNoDatasets','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitNoUsername','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitNoControls','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitWithDoneFlag','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitWithVarAppName','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitReservedVars','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitDatasetInitialInstance','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand._testConfigDefaults','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmit','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithBundleId','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithWrongNamespace','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testBasicSubmitWithSLA','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitFixedValues','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSchemaError','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitNoDatasets','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitNoUsername','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitNoControls','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitWithDoneFlag','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitWithVarAppName','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand.testSubmitReservedVars','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestCoordSubmitXCommand._testConfigDefaults','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.TestCoordinatorEngine.testDoneFlag','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.TestCoordinatorEngine.testCustomDoneFlag','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.TestCoordinatorEngine.testEmptyDoneFlag','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.TestCoordinatorEngine.testDoneFlagCreation','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.TestCoordinatorEngine._testSubmitJob','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.test.TestCounters.testMapOnly','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestCounters.testMapOnlyBinStorage','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestCounters.testMapReduceOnly','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestCounters.testMapReduceOnlyBinStorage','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestCounters.testMapCombineReduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestCounters.testMapCombineReduceBinStorage','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestCounters.testMultipleMRJobs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestCounters.testMapOnlyMultiQueryStores','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestCounters.testMultiQueryStores','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.test.TestCounters.getCounter','org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter'
'org.apache.crunch.test.TestCounters.clearCounters','org.apache.hadoop.mapreduce.Counters.<init>'
'org.apache.nutch.crawl.TestCrawlDbMerger.setUp','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.nutch.crawl.TestCrawlDbMerger.tearDown','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.TestCrawlDbMerger.testMerge','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.TestCrawlDbMerger.createCrawlDb','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.<init>'
'com.inmobi.databus.local.TestCreateListing.setUP','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'com.inmobi.databus.local.TestCreateListing.cleanup','org.apache.hadoop.fs.FileSystem.delete'
'com.inmobi.databus.local.TestCreateListing.testCreateListing1','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.cloudera.flume.handlers.hdfs.TestCustomDfsSink.checkCodec','org.apache.hadoop.io.compress.CompressionCodec.getClass'
'co.nubetech.hiho.mapreduce.TestDBInputDelimMapper.testMapperValidValues','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'co.nubetech.hiho.mapreduce.TestDBInputDelimMapper.testMapperValidValuesDelmiter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set'
'co.nubetech.hiho.mapreduce.TestDBInputDelimMapper.testMapperNullValues','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.job.TestDBQueryInputJob.testPopulateConfigurations','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForDriverClass','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForUrlProperty','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForUsernameProperty','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForPasswordProperty','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForOutputPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForDelimiter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForInputTableNameAndInputQuery','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForHiveLoadToPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForHiveDriver','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForHiveUrl','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForHiveOutputTableNameInCaseOfMultiPartition','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'co.nubetech.hiho.job.TestDBQueryInputJob.testCheckMandatoryConfsForInputBoundingQuery','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.store.TestDBWorkflowStore.createWorkflow','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.store.TestDBWorkflowStore._testInsertWF','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.store.TestDBWorkflowStore._testPurge','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.parse.html.TestDOMContentUtils.setup','org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.nutch.parse.html.TestDOMContentUtils.testGetOutlinks','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean'
'org.apache.oozie.TestDagEngine.testSubmit','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.TestDagEngine.testJobDefinition','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.TestDagEngine.testGetJobs','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.hphoto.bean.TestData.readFields','org.apache.hadoop.io.ArrayWritable.<init> org.apache.hadoop.io.ArrayWritable.readFields org.apache.hadoop.io.ArrayWritable.toArray'
'com.hphoto.bean.TestData.write','org.apache.hadoop.io.ArrayWritable.<init> org.apache.hadoop.io.ArrayWritable.set org.apache.hadoop.io.ArrayWritable.write'
'com.cloudera.sqoop.mapreduce.db.TestDataDrivenDBInputFormat.shutdown','org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.mapreduce.db.TestDataDrivenDBInputFormat.ValMapper.map','org.apache.hadoop.io.NullWritable.get'
'com.cloudera.sqoop.mapreduce.db.TestDataDrivenDBInputFormat.testDateSplits','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters'
'com.asakusafw.testtools.TestDataHolderTest.testField','org.apache.hadoop.io.Writable.getClass'
'org.apache.gora.store.TestDataStoreFactory.setUp','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.dedup.TestDedupJob.runDedupJob','org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.hiho.dedup.TestDedupJob.testDedupByValueWithDelimitedTextInputFormat','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'co.nubetech.hiho.dedup.TestDedupJob.testDedupByValueWithTextInputFormat','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'co.nubetech.hiho.dedup.TestDedupJob.testDedupByIntWritableKeyWithSequenceFileInputFormat','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.dedup.TestDedupJob.testDedupByValueWithSequenceFileInputFormat','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.dedup.TestDedupJob.testDedupByLongWritableKeyWithSequenceFileInputFormat','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.dedup.TestDedupJob.testDedupByValueWithSequenceFileAsTextInputFormat','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'co.nubetech.hiho.dedup.TestDedupJob.testDedupByCustomObjectKeyWithSequenceFileInputFormat','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.dedup.TestDedupKeyMapper.testMapperValidValues','org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.dedup.TestDedupKeyMapper.testMapperForNullKeyValue','org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupKeyReducer.testReducerValidValues','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupKeyReducer.testReducerForLongWritableKey','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupKeyReducer.testReducerForBytesWritableKeyAndValue','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupKeyReducer.testReducerForIntWritableKeyAndValue','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupKeyReducer.testReducerForNullValues','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupValueReducer.testReducerValidValues','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupValueReducer.testReducerForNullValues','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupValueReducer.testReducerForLongWritableKey','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupValueReducer.testReducerForBytesWritableKeyAndValue','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'co.nubetech.hiho.dedup.TestDedupValueReducer.testReducerForIntWritableKeyAndValue','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Counters.<init> org.apache.hadoop.mapreduce.Counters.findCounter'
'org.apache.nutch.indexer.TestDeleteDuplicates.setUp','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.indexer.TestDeleteDuplicates.createIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.digest'
'org.apache.nutch.indexer.TestDeleteDuplicates.createSingleDocIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.MD5Hash.digest'
'org.apache.nutch.indexer.TestDeleteDuplicates.tearDown','org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.indexer.TestDeleteDuplicates.hashDuplicatesHelper','org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.indexer.TestDeleteDuplicates.testUrlDuplicates','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.toString'
'org.apache.nutch.indexer.TestDeleteDuplicates.testMixedDuplicates','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.MD5Hash.digest org.apache.hadoop.io.MD5Hash.toString'
'co.nubetech.hiho.dedup.TestDelimitedTextInputFormat.testSetProperties','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.hama.bsp.message.TestDiskQueue.checkQueue','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.inmobi.databus.distcp.TestDistCPBaseService.setUP','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.Path.toString'
'com.inmobi.databus.distcp.TestDistCPBaseService.cleanUP','org.apache.hadoop.fs.FileSystem.delete'
'com.inmobi.databus.distcp.TestDistCPBaseService.createInvalidData','org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.inmobi.databus.distcp.TestDistCPBaseService.createValidData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'com.inmobi.databus.distcp.TestDistCPBaseService.testPositive','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestDistCpActionExecutor.testDistCpFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.apache.oozie.action.hadoop.TestDistCpActionExecutor.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.TestDistCpActionExecutor.createContext','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestDistCpActionExecutor.submitAction','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob'
'org.apache.oozie.action.hadoop.TestDistCpActionExecutor.testSimpestdistCpSubmitOK','org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.apache.oozie.action.hadoop.TestDistCpActionExecutor.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.TestDistCpActionExecutor.createContext','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestDistCpActionExecutor.submitAction','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob'
'sizzle.aggregators.TestDistinctAggregator.testDistinctAggregatorCombineDistinct','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestDistinctAggregator.testDistinctAggregatorCombineIndistinct','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestDistinctAggregator.testDistinctAggregatorReduceDistinct','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'sizzle.aggregators.TestDistinctAggregator.testDistinctAggregatorReduceIndistinct','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI.testDistributedLanczosSolverCLI','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI.testDistributedLanczosSolverEVJCLI','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolver.doTestDistributedLanczosSolver','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTranspose','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testMatrixMultiplactionJobConfBuilder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTransposeJobConfBuilder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTimesSquaredJobConfBuilders','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTimesVectorTempDirDeletion','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTimesSquaredVectorTempDirDeletion','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.createInitialConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.deleteContentsOfPath','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.saveToFs','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTranspose','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testMatrixMultiplactionJobConfBuilder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTransposeJobConfBuilder','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTimesSquaredJobConfBuilders','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTimesVectorTempDirDeletion','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTimesSquaredVectorTempDirDeletion','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.createInitialConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.deleteContentsOfPath','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete'
'org.apache.mahout.math.hadoop.TestDistributedRowMatrix.randomDistributedMatrix','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.pig.test.TestEmptyInputDir.setUpBeforeClass','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.pig.test.TestEmptyInputDir.testSkewedJoin','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.pig.test.TestEmptyInputDir.testMergeJoin','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.pig.test.TestEmptyInputDir.testFRJoin','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.pig.test.TestEmptyInputDir.testRegularJoin','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.nutch.util.TestEncodingDetector.testGuessing','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'com.twitter.elephantbird.pig.load.TestErrorsInInput.setUp','org.apache.hadoop.conf.Configuration.<init>'
'com.twitter.elephantbird.pig.load.TestErrorsInInput.TestMultiFormatLoaderWithEmptyRecords','org.apache.hadoop.fs.FileUtil.fullyDelete'
'com.cloudera.flume.handlers.hdfs.TestEscapedCustomOutputDfs.testBZip2Codec','org.apache.hadoop.io.compress.BZip2Codec.<init> org.apache.hadoop.io.compress.BZip2Codec.createInputStream'
'com.cloudera.flume.handlers.hdfs.TestEscapedCustomOutputDfs.testDefaultCodec','org.apache.hadoop.io.compress.DefaultCodec.<init> org.apache.hadoop.io.compress.DefaultCodec.createInputStream'
'com.cloudera.flume.handlers.hdfs.TestEscapedCustomOutputDfs.testCodecs','org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses'
'com.vertica.hadoop.TestExample.Map.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'com.vertica.hadoop.TestExample.Reduce.reduce','org.apache.hadoop.io.Text.<init>'
'com.vertica.hadoop.TestExample.getJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass'
'com.vertica.hadoop.TestExample.testExample','org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.impl.streaming.TestExecutableManager.testAddJobConfToEnv','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'com.squareup.cascading2.function.TestExpandProto.testInFlow','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'co.nubetech.hiho.job.sf.TestExportSalesForceJob.testPopulateConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.job.sf.TestExportSalesForceJob.testCheckMandatoryConfsValidValues','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.sf.TestExportSalesForceJob.testCheckMandatoryConfsForInputPath','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.sf.TestExportSalesForceJob.testCheckMandatoryConfsForSfUserName','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.sf.TestExportSalesForceJob.testCheckMandatoryConfsForSfPassword','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.sf.TestExportSalesForceJob.testCheckMandatoryConfsForSfObjectType','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.sf.TestExportSalesForceJob.testCheckMandatoryConfsForSfHeaders','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToFTPServer.testPopulateConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.job.TestExportToFTPServer.testCheckMandatoryConfsValidValues','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToFTPServer.testCheckMandatoryConfsForInputPath','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToFTPServer.testCheckMandatoryConfsForOutputPath','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToFTPServer.testCheckMandatoryConfsForFtpUserName','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToFTPServer.testCheckMandatoryConfsForFtpAddress','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToFTPServer.testCheckMandatoryConfsForFtpPortNumber','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToFTPServer.testCheckMandatoryConfsForFtpPassword','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToMySQLDB.testPopulateConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'co.nubetech.hiho.job.TestExportToMySQLDB.testCheckMandatoryConfsValidValues','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToMySQLDB.testCheckMandatoryConfsForInputPath','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToMySQLDB.testCheckMandatoryConfsForJdbcUrl','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToMySQLDB.testCheckMandatoryConfsForJdbcUserName','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToMySQLDB.testCheckMandatoryConfsForJdbcPassword','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToMySQLDB.testCheckMandatoryConfsForQuerySuffix','org.apache.hadoop.conf.Configuration.<init>'
'co.nubetech.hiho.job.TestExportToMySQLDB.testMySQlBasicExport','org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.hiho.job.TestExportToMySQLDB.testMySQlBasicExportForDelimiter','org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.sqoop.TestExportUpdate.createMultiKeyUpdateFiles','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.cloudera.sqoop.TestExportUpdate.createUpdateFiles','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.cloudera.sqoop.TestExportUpdate.verifyRowCount','org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestExportUpdate.verifyMultiKeyRow','org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestExportUpdate.verifyRow','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.pig.test.TestFRJoin2.setUpBeforeClass','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'.TestFSUtils.main','org.apache.hadoop.fs.Path.<init>'
'.TestFeatureSpec.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'.TestFeatureSpec.testFeatureSpec','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.parse.feed.TestFeedParser.testParseFetchChannel','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.feed.TestFeedParser.testParseFetchChannel','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.fetcher.TestFetcher.setUp','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.fetcher.TestFetcher.tearDown','org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.fetcher.TestFetcher.testFetch','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.nutch.fetcher.TestFetcher.testAgentNameCheck','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean'
'com.twitter.twadoop.mapreduce.input.TestFileSetInputFormat.setup','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.JobID.<init> org.apache.hadoop.mapreduce.JobContext.<init>'
'com.twitter.twadoop.mapreduce.input.TestFileSetInputFormat.testSplits','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.twitter.twadoop.mapreduce.input.TestFileSetInputFormat.testUniqueSplits','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestFileSystemActions.testDelete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestFileSystemActions.testMkdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestFileSystemActions.testForInvalidScheme','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestFileSystemActions.testForNullScheme','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init>'
'org.apache.giraph.aggregators.TestFloatAggregators.testMaxAggregator','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.aggregators.TestFloatAggregators.testMinAggregator','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.aggregators.TestFloatAggregators.testOverwriteAggregator','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.aggregators.TestFloatAggregators.testProductAggregator','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.aggregators.TestFloatAggregators.testSumAggregator','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'sizzle.aggregators.TestFloatSumAggregator.testFloatSumAggregatorFloatsCombine','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.withInput'
'sizzle.aggregators.TestFloatSumAggregator.testFloatSumAggregatorFloatsReduce','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.classifier.df.mapreduce.TestForest.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.df.mapreduce.TestForest.testForest','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists'
'org.apache.mahout.classifier.df.mapreduce.TestForest.sequential','org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.mahout.classifier.df.mapreduce.TestForest.testDirectory','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix'
'org.apache.mahout.classifier.df.mapreduce.TestForest.testFile','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataOutputStream.writeChars org.apache.hadoop.fs.FSDataOutputStream.writeChar'
'org.apache.mahout.classifier.df.mapreduce.TestForest.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.df.mapreduce.TestForest.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.df.mapreduce.TestForest.testForest','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists'
'org.apache.mahout.df.mapreduce.TestForest.sequential','org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.mahout.df.mapreduce.TestForest.testDirectory','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.suffix'
'org.apache.mahout.df.mapreduce.TestForest.testFile','org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataOutputStream.writeChars org.apache.hadoop.fs.FSDataOutputStream.writeChar org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.mahout.df.mapreduce.TestForest.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testValidatePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testFileSchemeWildcard','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testResolveToFullPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testvalidateSameNN','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testMkdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testDelete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testMove','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmod','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmodRecursive','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testTouchz','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testDoOperations','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testDoOperationsWithNameNodeElement','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testSubmit','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testSubmitWithNameNode','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testRecovery','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testValidatePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testvalidateSameNN','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testMkdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testDelete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testMove','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmod','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testDoOperations','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testSubmit','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.oozie.action.hadoop.TestFsActionExecutor.testRecovery','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus'
'edu.duke.starfish.jobopt.junit.TestFullEnumJobOptimizer.testWhatIfJobConfGetTime','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'.TestFuseDFS.createFile','org.apache.hadoop.io.IOUtils.copyBytes'
'.TestFuseDFS.checkFile','org.apache.hadoop.io.IOUtils.readFully'
'.TestFuseDFS.startUp','org.apache.hadoop.hdfs.HdfsConfiguration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.FileSystem.getUri'
'.TestFuseDFS.tearDown','org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'.TestFuseDFS.run','org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.oozie.command.coord.TestFutureActionsTimeOut._testSubmitJob','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestFutureActionsTimeOut._testSubmitJob','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansSeqJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansMRJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansSeqJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansMRJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansMapper','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansCombiner','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansReducer','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansClusterMapper','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testClusterObservationsSerialization','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'co.nubetech.hiho.mapreduce.TestGenericDBLoadDataMapper.testMapperWithValidValues','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.mapreduce.TestGenericDBLoadDataMapper.testMapperWithNullValues','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.mapreduce.TestGenericDBLoadDataMapper.testMapperWithUnequalLengthOfColumnInFileAndTable','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.avro.mapred.TestGenericJob.tearDown','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.avro.mapred.TestGenericJob.AvroTestConverter.map','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.avro.mapred.AvroWrapper<org.apache.avro.mapred.Pair<java.lang.Long,org.apache.avro.mapred.GenericData.Record>>,org.apache.hadoop.io.NullWritable>.collect'
'org.apache.avro.mapred.TestGenericJob.testJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'org.gora.mapreduce.TestGoraInputSplit.testReadWrite','org.apache.hadoop.io.TestWritable.testWritable'
'org.apache.gora.mapreduce.TestGoraInputSplit.testReadWrite','org.apache.hadoop.io.TestWritable.testWritable'
'org.apache.giraph.TestGraphPartitioner.verifyOutput','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getLen'
'org.apache.giraph.TestGraphPartitioner.testPartitioners','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.examples.TestGrep.test','org.apache.hadoop.util.ToolRunner.run'
'org.apache.oozie.service.TestGroupsService.testService','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestGroupsService.testInvalidGroupsMapping','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.TestHBaseBulkOutputFormat','org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hbase.HBaseConfiguration.merge org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.MapWriteOldMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Put>.collect'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.MapWrite.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init>'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.MapHCatWrite.map','org.apache.hadoop.io.Text.toString'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.hbaseBulkOutputFormatTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setWorkingDirectory org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.TextInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputCommitter org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.waitForCompletion org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.containsColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.fs.FileSystem.get'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.importSequenceFileTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setWorkingDirectory org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.containsColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.bulkModeHCatOutputFormatTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.conf.HiveConf.getAllProperties org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setWorkingDirectory org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.containsColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getColumn'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.bulkModeHCatOutputFormatTestWithDefaultDB','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.conf.HiveConf.getAllProperties org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setWorkingDirectory org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.containsColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.bulkModeAbortTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.conf.HiveConf.getAllProperties org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.configureJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setWorkingDirectory org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.Initialize','org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hbase.HBaseConfiguration.merge org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.testTableCreateDrop','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.testTableCreateDropDifferentCase','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.testTableCreateDropCaseSensitive','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.testTableDropNonExistent','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled org.apache.hadoop.hbase.client.HBaseAdmin.disableTable org.apache.hadoop.hbase.client.HBaseAdmin.deleteTable org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.testTableCreateExternal','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.TestHBaseInputFormat','org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hbase.HBaseConfiguration.merge org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.generatePuts','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.populateHBaseTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.put'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.populateHBaseTableQualifier1','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.put'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.TestHBaseTableReadMR','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.TestHBaseTableProjectionReadMR','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.TestHBaseInputFormatProjectionReadMR','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.TextOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.mapred.RunningJob.waitForCompletion org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.TestHBaseTableIgnoreAbortedTransactions','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.TestHBaseTableIgnoreAbortedAndRunningTransactions','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.apache.hcatalog.hbase.TestHBaseInputFormat.MapReadProjectionHTable.map','org.apache.hadoop.hbase.client.Result.toString org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'com.ripariandata.timberwolf.integrated.TestHBaseIntegration.createPut','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.ripariandata.timberwolf.integrated.TestHBaseIntegration.createGet','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.addFamily'
'com.ripariandata.timberwolf.integrated.TestHBaseIntegration.testRemotePut','org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'org.apache.flume.sink.hbase.TestHBaseSink.setUp','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster'
'org.apache.flume.sink.hbase.TestHBaseSink.tearDown','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'org.apache.flume.sink.hbase.TestHBaseSink.testOneEvent','org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.HBaseTestingUtility.deleteTable'
'org.apache.flume.sink.hbase.TestHBaseSink.testThreeEvents','org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.deleteTable'
'org.apache.flume.sink.hbase.TestHBaseSink.testMultipleBatches','org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.deleteTable'
'org.apache.flume.sink.hbase.TestHBaseSink.testMissingTable','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.flume.sink.hbase.TestHBaseSink.testHBaseFailure','org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'org.apache.flume.sink.hbase.TestHBaseSink.getResults','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.setStartRow org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.client.ResultScanner.close'
'com.cloudera.flume.hbase.TestHBaseSink.shipThreeEvents','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.cloudera.flume.hbase.TestHBaseSink.testSink','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.close'
'com.cloudera.flume.hbase.TestHBaseSink.testSinkEmptyCol','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.close'
'com.cloudera.flume.hbase.TestHBaseSink.testSinkEscaping','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.close'
'com.cloudera.flume.hbase.TestHBaseSink.testOpenFailBadColFam','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'org.gora.hbase.store.TestHBaseStore.assertSchemaExists','org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'org.gora.hbase.store.TestHBaseStore.assertPutArray','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.close'
'org.gora.hbase.store.TestHBaseStore.assertPutBytes','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.close'
'org.gora.hbase.store.TestHBaseStore.assertPutMap','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.close'
'org.apache.hcatalog.mapreduce.TestHCatDynamicPartitioned.generateDataColumns','org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.metastore.api.FieldSchema.<init>'
'org.apache.hcatalog.mapreduce.TestHCatDynamicPartitioned.getPartitionKeys','org.apache.hadoop.hive.metastore.api.FieldSchema.<init>'
'org.apache.hcatalog.mapreduce.TestHCatDynamicPartitioned.getTableColumns','org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.metastore.api.FieldSchema.<init>'
'org.apache.hcatalog.mapreduce.TestHCatDynamicPartitioned._testHCatDynamicPartitionMaxPartitions','org.apache.hadoop.hive.conf.HiveConf.<init>'
'org.apache.hcatalog.mapreduce.TestHCatInputFormat.setUp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.hive.serde2.thrift.test.IntString.<init> org.apache.hadoop.hive.serde2.thrift.test.IntString.write org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.fs.Path.getParent'
'org.apache.hcatalog.mapreduce.TestHCatInputFormat.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setFloat org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.hcatalog.mapreduce.TestHCatInputFormat.MyMapper.map','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'org.apache.hcatalog.pig.TestHCatLoader.dropTable','org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.pig.TestHCatLoader.createTable','org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.pig.TestHCatLoader.guardedSetUpBeforeClass','org.apache.hadoop.fs.FileUtil.fullyDelete org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.ql.Driver.<init> org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.pig.TestHCatLoader.testReadPartitionedBasic','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.getResults'
'org.apache.hcatalog.pig.TestHCatLoader.testGetInputBytes','org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.hcatalog.pig.TestHCatLoader.testConvertBooleanToInt','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.dropTable','org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.createTable','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getErrorMessage'
'org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.setUpBeforeClass','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.ql.Driver.<init> org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.mapreduce.TestHCatOutputFormat.setUp','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>'
'org.apache.hcatalog.mapreduce.TestHCatOutputFormat.tearDown','org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close'
'org.apache.hcatalog.mapreduce.TestHCatOutputFormat.initTable','org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase org.apache.hadoop.hive.metastore.api.Database.<init> org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.metastore.api.Table.<init> org.apache.hadoop.hive.metastore.api.Table.setDbName org.apache.hadoop.hive.metastore.api.Table.setTableName org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init> org.apache.hadoop.hive.metastore.api.StorageDescriptor.setCols org.apache.hadoop.hive.metastore.api.Table.setSd org.apache.hadoop.hive.ql.io.RCFileInputFormat.getName org.apache.hadoop.hive.metastore.api.StorageDescriptor.setInputFormat org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getName org.apache.hadoop.hive.metastore.api.StorageDescriptor.setOutputFormat org.apache.hadoop.hive.metastore.api.StorageDescriptor.setParameters org.apache.hadoop.hive.metastore.api.StorageDescriptor.getParameters org.apache.hadoop.hive.metastore.api.SerDeInfo.<init> org.apache.hadoop.hive.metastore.api.StorageDescriptor.setSerdeInfo org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSerdeInfo org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSerdeInfo org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSerdeInfo org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSerdeInfo org.apache.hadoop.hive.metastore.api.Table.setPartitionKeys org.apache.hadoop.hive.metastore.api.Table.setParameters org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init>'
'org.apache.hcatalog.mapreduce.TestHCatOutputFormat.testSetOutput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.hcatalog.mapreduce.TestHCatOutputFormat.publishTest','org.apache.hadoop.mapreduce.OutputCommitter.commitJob org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartition org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.hive.metastore.api.Partition.getParameters org.apache.hadoop.hive.metastore.api.Partition.getSd'
'org.apache.hcatalog.common.TestHCatUtil.testFsPermissionOperation','org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.permission.FsPermission.toString'
'org.apache.hcatalog.common.TestHCatUtil.assertFsPermissionTransformationIsGood','org.apache.hadoop.fs.permission.FsPermission.valueOf'
'org.apache.hcatalog.common.TestHCatUtil.testGetTableSchemaWithPtnColsApi','org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.metastore.api.SerDeInfo.<init> org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init> org.apache.hadoop.hive.ql.metadata.Table.<init> org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.ql.metadata.Table.getTTable'
'org.apache.hcatalog.common.TestHCatUtil.testGetTableSchemaWithPtnColsSerDeReportedFields','org.apache.hadoop.hive.metastore.api.SerDeInfo.<init> org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init> org.apache.hadoop.hive.ql.metadata.Table.<init>'
'org.apache.oozie.service.TestHadoopAccessorService.testAccessor','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.service.TestHadoopAccessorService.testAccessor','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.hama.bsp.message.TestHadoopMessageManager.testMemoryMessaging','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.hama.bsp.message.TestHadoopMessageManager.testDiskMessaging','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'org.apache.hama.bsp.message.TestHadoopMessageManager.messagingInternal','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'com.cloudera.lib.service.hadoop.TestHadoopService.createFileSystem','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'com.cloudera.lib.service.hadoop.TestHadoopService.fileSystemExecutor','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'com.cloudera.lib.service.hadoop.TestHadoopService.execute','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.mapred.JobClient.getQueues'
'com.cloudera.lib.service.hadoop.TestHadoopService.fileSystemExecutorNoNameNode','org.apache.hadoop.mapred.JobConf.set'
'com.cloudera.lib.service.hadoop.TestHadoopService.fileSystemExecutorException','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'com.cloudera.lib.service.hadoop.TestHadoopService.jobClientExecutorNoNameNode','org.apache.hadoop.mapred.JobConf.set'
'com.cloudera.lib.service.hadoop.TestHadoopService.jobClientExecutorNoJobTracker','org.apache.hadoop.mapred.JobConf.set'
'com.cloudera.lib.service.hadoop.TestHadoopService.jobClientExecutor','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'com.cloudera.lib.service.hadoop.TestHadoopService.jobClientExecutorException','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'com.linkedin.haivvreo.TestHaivvreoUtils.determineSchemaCanReadSchemaFromHDFS','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'com.hphoto.server.TestHbase.setUp','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.HBaseAdmin.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.<init>'
'com.hphoto.server.TestHbase.testSetBean','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.hbase.util.Writables.getBytes org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TestHbase.getUesr','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.hbase.util.Writables.getBytes org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.hbase.util.Writables.getBytes org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.startUpdate org.apache.hadoop.hbase.util.Writables.getBytes org.apache.hadoop.hbase.HTable.put org.apache.hadoop.hbase.HTable.commit'
'com.hphoto.server.TestHbase.scanner','org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.obtainScanner org.apache.hadoop.hbase.filter.PageRowFilter.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.filter.StopRowFilter.<init> org.apache.hadoop.hbase.filter.RowFilterSet.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.obtainScanner org.apache.hadoop.hbase.filter.RegExpRowFilter.<init> org.apache.hadoop.hbase.filter.WhileMatchRowFilter.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.obtainScanner org.apache.hadoop.hbase.filter.RegExpRowFilter.<init> org.apache.hadoop.hbase.filter.WhileMatchRowFilter.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.obtainScanner'
'com.hphoto.server.TestHbase.printScanner','org.apache.hadoop.hbase.HStoreKey.<init> org.apache.hadoop.hbase.HScannerInterface.next org.apache.hadoop.hbase.HStoreKey.getRow org.apache.hadoop.hbase.HStoreKey.getColumn org.apache.hadoop.hbase.HStoreKey.getTimestamp org.apache.hadoop.hbase.util.Writables.getWritable org.apache.hadoop.hbase.HScannerInterface.next'
'com.hphoto.server.TestHbase.getUser','org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.get org.apache.hadoop.hbase.util.Writables.getWritable org.apache.hadoop.io.Text.<init> org.apache.hadoop.hbase.HTable.get org.apache.hadoop.hbase.util.Writables.getWritable'
'com.rapleaf.hank.storage.TestHdfsPartitionRemoteFileOps.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.rapleaf.hank.storage.TestHdfsPartitionRemoteFileOps.testAttemptDelete','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'co.nubetech.hiho.merge.TestHihoValue.testSetVal','org.apache.hadoop.io.Text.<init>'
'org.apache.pig.piggybank.test.storage.TestHiveColumnarLoader.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.LocalFileSystem.getLocal'
'org.apache.pig.piggybank.test.storage.TestHiveColumnarLoader.produceDatePartitionedData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.compress.DefaultCodec.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.pig.piggybank.test.storage.TestHiveColumnarLoader.produceYearMonthDayHourPartitionedData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.compress.DefaultCodec.<init>'
'org.apache.pig.piggybank.test.storage.TestHiveColumnarLoader.produceSimpleData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.compress.DefaultCodec.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.compress.DefaultCodec.<init>'
'org.apache.pig.piggybank.test.storage.TestHiveColumnarLoader.writeRCFileTest','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.hive.ql.io.RCFileOutputFormat.setColumnNumber org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.<init> org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.<init> org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.set'
'org.apache.pig.piggybank.test.storage.TestHiveColumnarLoader.nextRandomRow','org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.resetValid org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.get'
'org.apache.oozie.action.hadoop.TestHiveMain.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'com.cloudera.hoop.TestHoopServer.testGlobFilter','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.sematext.hbase.hut.TestHutRowKeyUtil.test','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.gora.util.TestIOUtils.testSerializeDeserialize','org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.gora.util.TestIOUtils.testWritableSerde','org.apache.hadoop.io.Text.<init>'
'org.apache.gora.util.TestIOUtils.testNullFieldsWith','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.giraph.io.TestIdWithValueTextOutputFormat.testHappyPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.TestIdWithValueTextOutputFormat.testReverseIdAndValue','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.TestIdWithValueTextOutputFormat.testWithDifferentDelimiter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.TestIdWithValueTextOutputFormat.IdWithValueTestWorker','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'com.cloudera.sqoop.mapreduce.TestImportJob.testFailedImportDueToIOException','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists'
'com.cloudera.sqoop.mapreduce.TestImportJob.DummyImportJob.configureInputFormat','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath'
'com.cloudera.sqoop.mapreduce.TestImportJob.testFailedImportDueToJobFail','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass'
'com.cloudera.sqoop.mapreduce.TestImportJob.testFailedNoColumns','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists'
'com.cloudera.sqoop.mapreduce.TestImportJob.testFailedIllegalColumns','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists'
'com.cloudera.sqoop.mapreduce.TestImportJob.testDuplicateColumns','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.exists'
'com.cloudera.sqoop.TestIncrementalImport.clearDir','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestIncrementalImport.assertDirOfNumbers','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestIncrementalImport.assertSpecificNumber','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestIncrementalImport.runImport','org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestIncrementalImport.createJob','org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestIncrementalImport.runJob','org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestIncrementalImport.testTimestampBoundary','org.apache.hadoop.conf.Configuration.set'
'com.cloudera.sqoop.TestIncrementalImport.testIncrementalAppendTimestamp','org.apache.hadoop.conf.Configuration.set'
'com.twitter.elephanttwin.retrieval.TestIndexedReader.PrintBinaryMapper.map','org.apache.hadoop.io.Text.<init>'
'com.twitter.elephanttwin.retrieval.TestIndexedReader.accept','org.apache.hadoop.fs.Path.getName'
'com.twitter.elephanttwin.retrieval.TestIndexedReader.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.twitter.elephanttwin.retrieval.TestIndexedReader.main','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getConfiguration org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.indexer.TestIndexingFilters.testNonExistingIndexingFilter','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init>'
'org.apache.jena.tdbloader4.TestInferLocal.test','org.apache.hadoop.util.ToolRunner.run'
'edu.duke.starfish.profile.utils.TestInfoManager.getHadoopConfiguration','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.accumulo.server.test.TestIngest.CreateTable.getSplitPoints','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.TestIngest.generateRow','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.test.TestIngest.main','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.crawl.TestInjector.setUp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.TestInjector.tearDown','org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.TestInjector.testInject','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.crawl.TestInjector.readCrawldb','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.nutch.crawl.TestInjector.readCrawldbRecords','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.pig.test.TestJobControlCompiler.testJarAddedToDistributedCache','org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.filecache.DistributedCache.getFileClassPaths org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapred.JobConf.get'
'org.apache.pig.test.TestJobControlCompiler.testEstimateNumberOfReducers','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init>'
'org.goldenorb.TestJobManager.setUp','org.apache.hadoop.hdfs.MiniDFSCluster.<init>'
'org.goldenorb.TestJobManager.jobManagerTest','org.apache.hadoop.hdfs.MiniDFSCluster.getNameNodePort'
'org.goldenorb.TestJobManager.HeartbeatUpdater.run','org.apache.hadoop.io.LongWritable.<init>'
'edu.duke.starfish.whatif.junit.TestJobProfileOracle.testWhatif','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.pig.test.TestJobSubmission.testDefaultParallel','org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.Job.getJobConf org.apache.hadoop.mapred.jobcontrol.Job.getJobConf'
'org.apache.pig.test.TestJobSubmission.testReducerNumEstimation','org.apache.hadoop.hbase.HBaseTestingUtility.<init> org.apache.hadoop.hbase.HBaseTestingUtility.startMiniZKCluster org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.Job.getJobConf org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.Job.getJobConf org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytesBinary org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.Job.getJobConf org.apache.hadoop.hbase.util.Bytes.toBytesBinary org.apache.hadoop.hbase.HBaseTestingUtility.deleteTable org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseCluster org.apache.hadoop.hbase.MiniHBaseCluster.shutdown org.apache.hadoop.hbase.MiniHBaseCluster.join org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniZKCluster'
'org.apache.pig.test.TestJobSubmission.testReducerNumEstimationForOrderBy','org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.JobControl.getReadyJobs org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.JobControl.getReadyJobs org.apache.hadoop.mapred.jobcontrol.JobControl.getReadyJobs org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs'
'org.apache.oozie.servlet.TestJobsServlet.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml'
'org.apache.hcatalog.data.TestJsonSerDe.testRW','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.data.TestJsonSerDe.testRobustRead','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.data.TestJsonSerDe.getInternalNames','org.apache.hadoop.hive.conf.HiveConf.getColumnInternalName'
'com.wibidata.avro.mapreduce.TestKeyValueInput.IndexMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.wibidata.avro.mapreduce.TestKeyValueInput.IndexReducer.reduce','org.apache.hadoop.io.IntWritable.get'
'com.wibidata.avro.mapreduce.TestKeyValueInput.testKeyValueInput','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toString'
'com.wibidata.avro.mapreduce.TestKeyValueWordCount.LineCountMapper.setup','org.apache.hadoop.io.IntWritable.<init>'
'com.wibidata.avro.mapreduce.TestKeyValueWordCount.IntSumReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'com.wibidata.avro.mapreduce.TestKeyValueWordCount.testKeyValueMapReduce','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.toString'
'com.cloudera.kitten.TestKittenDistributedShell.setup','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.yarn.server.MiniYARNCluster.<init> org.apache.hadoop.yarn.server.MiniYARNCluster.init org.apache.hadoop.yarn.server.MiniYARNCluster.start'
'com.cloudera.kitten.TestKittenDistributedShell.tearDown','org.apache.hadoop.yarn.server.MiniYARNCluster.stop'
'com.cloudera.kitten.TestKittenDistributedShell.testKittenShell','org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.kmeans.TestKmeansClustering.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.kmeans.TestKmeansClustering.testKMeansMapper','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.mahout.clustering.kmeans.TestKmeansClustering.testKMeansCombiner','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.kmeans.TestKmeansClustering.testKMeansReducer','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.kmeans.TestKmeansClustering.testKMeansSeqJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.kmeans.TestKmeansClustering.testKMeansMRJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.kmeans.TestKmeansClustering.testKMeansWithCanopyClusterInput','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.dirichlet.TestL1ModelClustering.testDocs','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.dirichlet.TestL1ModelClustering.testDMDocs','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.dirichlet.TestL1ModelClustering.testDocs2','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.clustering.dirichlet.TestL1ModelClustering.testDMDocs2','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.command.wf.TestLastModified.testWorkflowRun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.wf.TestLastModified.testWorkflowRun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.command.wf.TestLastModified.testWorkflowRun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.ivory.rerun.handler.TestLateData.storeEntity','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.TestLauncher._test','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.RunningJob.getJobID org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.TestLauncher.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.TestLauncher.testEmpty','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testExit0','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testExit1','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testException','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testOutput','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testNewId','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testSecurityManager','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testSetupLauncherInfoWithEmptyPrepareXML','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.TestLauncher.testSetupLauncherInfoWithNonEmptyPrepareXML','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.action.hadoop.TestLauncher._test','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.mapred.JobClient.submitJob org.apache.hadoop.mapred.RunningJob.getJobID'
'org.apache.oozie.action.hadoop.TestLauncher.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.TestLauncher.testEmpty','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testExit0','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testExit1','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testException','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testOutput','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testNewId','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.oozie.action.hadoop.TestLauncher.testSecurityManager','org.apache.hadoop.mapred.RunningJob.isSuccessful org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.exists'
'org.apache.hcatalog.data.TestLazyHCatRecord.getTypeInfo','org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo'
'org.apache.nutch.crawl.TestLinkDbMerger.setUp','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.nutch.crawl.TestLinkDbMerger.tearDown','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.TestLinkDbMerger.testMerge','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.nutch.crawl.TestLinkDbMerger.createLinkDb','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.<init>'
'org.apache.oozie.workflow.lite.TestLiteWorkflowAppParser.testParserGlobal','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.workflow.lite.TestLiteWorkflowAppParser.testParserGlobalJobXML','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.workflow.lite.TestLiteWorkflowAppParser.testParserGlobalLocalAlreadyExists','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.workflow.lite.TestLiteWorkflowAppParser.testParserGlobalExtensionActions','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.workflow.lite.TestLiteWorkflowAppParser.testParserGlobalExtensionActionsLocalAlreadyExists','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.workflow.lite.TestLiteWorkflowAppParser.testParserGlobalExtensionActionsNoGlobal','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.workflow.lite.TestLiteWorkflowAppParser.testParser','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.workflow.lite.TestLiteWorkflowAppParser.RCThread.run','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.service.TestLiteWorkflowStoreService.testRetry','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.piggybank.test.storage.TestLoadFuncHelper.testDetermineFirstFile','org.apache.hadoop.fs.Path.toUri'
'org.apache.pig.piggybank.test.storage.TestLoadFuncHelper.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.pig.piggybank.test.storage.TestLoadFuncHelper.tearDown','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.oozie.client.TestLocalOozie.testWorkflowRun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.client.TestLocalOozie.testWorkflowRun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.client.TestLocalOozie.testWorkflowRun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.hama.bsp.TestLocalRunner.testOutputJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.FileSystem.get'
'com.inmobi.databus.utils.TestLocalStreamDataConsistency.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'com.inmobi.databus.utils.TestLocalStreamDataConsistency.cleanup','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.delete'
'com.inmobi.databus.utils.TestLocalStreamDataConsistency.createCompletePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.utils.TestLocalStreamDataConsistency.createFiles','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.inmobi.databus.utils.TestLocalStreamDataConsistency.createFilesData','org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.exists'
'com.inmobi.databus.utils.TestLocalStreamDataConsistency.testLocalStreamData','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.piggybank.test.evaluation.string.TestLookupInFiles.testLookupInFiles','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'org.apache.nutch.parse.msexcel.TestMSExcelParser.testIt','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.mspowerpoint.TestMSPowerPointParser.setUp','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.msword.TestMSWordParser.setUp','org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.parse.msword.TestMSWordParser.getTextContent','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverIterationsSeq','org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverIterationsMR','org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverIterationsMahalanobisSeq','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverIterationsMahalanobisMR','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.Tool.setConf org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.getClusters','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.testLauncherJar','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.testSetupMethods','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.createContext','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.submitAction','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob'
'org.apache.oozie.action.hadoop.TestMapReduceActionError._testSubmit','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.testMapReduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.testLauncherJar','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.testSetupMethods','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.createContext','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.submitAction','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob'
'org.apache.oozie.action.hadoop.TestMapReduceActionError._testSubmit','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.TestMapReduceActionError.testMapReduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.hadoop.TestMapReduceMain.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestMapReduceMain.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestMapReduceMain.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testReducer','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testMRIterations','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverIterationsSeq','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverIterationsMR','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverMnRIterations','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverIterationsMahalanobisSeq','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testDriverIterationsMahalanobisMR','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.Tool.setConf org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testNormalModelWritableSerialization','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Writable.write org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testSampledNormalModelWritableSerialization','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Writable.write org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testAsymmetricSampledNormalModelWritableSerialization','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.Writable.write org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.clustering.dirichlet.TestMapReduce.testClusterWritableSerialization','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'org.apache.mahout.clustering.spectral.common.TestMatrixDiagonalizeJob.testMatrixDiagonalizeMapper','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.clustering.spectral.common.TestMatrixDiagonalizeJob.testMatrixDiagonalizeReducer','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.NullWritable.get'
'sizzle.aggregators.TestMaximumAggregator.testMaximumAggregatorTopTenCombine','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestMaximumAggregator.testMaximumAggregatorTopTenReduce','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'sizzle.aggregators.TestMaximumAggregator.testMaximumAggregatorAllEqual','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyMapperEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyReducerEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyEuclideanMRJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyEuclideanSeqJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyEuclideanMRJobNoClustering','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyEuclideanSeqJobNoClustering','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyMapperEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyReducerEuclidean','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyEuclideanMRJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.meanshift.TestMeanShift.testCanopyEuclideanSeqJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.TestMerge.newConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.cloudera.sqoop.TestMerge.testMerge','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'com.cloudera.sqoop.TestMerge.checkFileForLine','org.apache.hadoop.fs.FileSystem.open'
'com.cloudera.sqoop.TestMerge.recordStartsWith','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByCustomObjectKeyWithSequenceFileInputFormat','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByIntWritableKeyWithSequenceFileInputFormat','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByLongWritableKeyWithSequenceFileInputFormat','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByValueWithSequenceFileInputFormat','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByKeyWithDelimitedTextInputFormat','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByValueWithDelimitedTextInputFormat','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByValueWithTextInputFormat','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByKeyWithKeyValueTextInputFormat','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'co.nubetech.hiho.merge.TestMergeJob.testMergeByValueWithSequenceFileAsTextInputFormat','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDirectory org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readLine org.apache.hadoop.fs.FSDataInputStream.close'
'co.nubetech.hiho.merge.TestMergeJob.runMergeJobs','org.apache.hadoop.util.ToolRunner.run'
'org.msgpack.hadoop.mapreduce.input.TestMessagePackInputFormat.testFormat','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.msgpack.hadoop.mapreduce.input.TestMessagePackInputFormat.checkFormat','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.msgpack.hadoop.io.MessagePackWritable>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.msgpack.hadoop.io.MessagePackWritable>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.msgpack.hadoop.io.MessagePackWritable>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.msgpack.hadoop.io.MessagePackWritable>.getCurrentValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.msgpack.hadoop.io.MessagePackWritable>.close'
'org.apache.giraph.comm.TestMessageStores.prepare','org.apache.hadoop.conf.Configuration.addDefaultResource'
'org.apache.giraph.comm.TestMessageStores.createRandomMessages','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.inmobi.databus.TestMiniClusterUtil.setup','org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.waitActive org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.mapred.MiniMRCluster.createJobConf'
'com.inmobi.databus.TestMiniClusterUtil.cleanup','org.apache.hadoop.hdfs.MiniDFSCluster.shutdown org.apache.hadoop.mapred.MiniMRCluster.shutdown'
'com.inmobi.databus.TestMiniClusterUtil.GetFileSystem','org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem'
'com.inmobi.databus.TestMiniClusterUtil.RunJob','org.apache.hadoop.mapred.JobClient.runJob'
'com.wibidata.maven.plugins.hbase.TestMiniHBaseCluster.testWithMapReduceDisabled','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'com.wibidata.maven.plugins.hbase.TestMiniHBaseCluster.testWithMapReduceEnabled','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.startMiniMapReduceCluster org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniMapReduceCluster org.apache.hadoop.hbase.HBaseTestingUtility.getDataTestDir org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.indexer.more.TestMoreIndexingFilter.assertContentType','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.indexer.more.TestMoreIndexingFilter.testNoParts','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.indexer.more.TestMoreIndexingFilter.testContentDispositionTitle','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.indexer.more.TestMoreIndexingFilter.assertContentType','org.apache.hadoop.io.Text.<init>'
'com.datasalt.pangool.examples.movingaverage.TestMovingAverage.test','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hcatalog.listener.TestMsgBusConnection.setUp','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start org.apache.hadoop.hive.ql.Driver.<init>'
'org.apache.hcatalog.listener.TestMsgBusConnection.testConnection','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.NoSuchObjectException.printStackTrace org.apache.hadoop.hive.metastore.api.AlreadyExistsException.printStackTrace'
'com.datasalt.utils.mapred.joiner.TestMultiJoiner.AMapper.map','org.apache.hadoop.io.Text.toString'
'com.datasalt.utils.mapred.joiner.TestMultiJoiner.BMapper.map','org.apache.hadoop.io.Text.toString'
'com.datasalt.utils.mapred.joiner.TestMultiJoiner.TestReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.datasalt.utils.mapred.joiner.TestMultiJoiner.test','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'com.datasalt.utils.mapred.joiner.TestMultiJoinerMultiChannel.ABMapper.map','org.apache.hadoop.io.Text.toString'
'com.datasalt.utils.mapred.joiner.TestMultiJoinerMultiChannel.TestReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.datasalt.utils.mapred.joiner.TestMultiJoinerMultiChannel.test','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'com.datasalt.utils.mapred.joiner.TestMultiJoinerSecondarySort.AMapperSecondarySort.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'com.datasalt.utils.mapred.joiner.TestMultiJoinerSecondarySort.BMapperSecondarySort.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'com.datasalt.utils.mapred.joiner.TestMultiJoinerSecondarySort.TestReducerSecondarySort.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.datasalt.utils.mapred.joiner.TestMultiJoinerSecondarySort.test','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.setup','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.mapred.MiniMRCluster.createJobConf'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.createWorkDir','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.tearDown','org.apache.hadoop.mapred.MiniMRCluster.shutdown org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.testMultiOutputFormatWithoutReduce','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.getFileClassPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.get'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.testMultiOutputFormatWithReduce','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.getJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.get'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.readFully','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.available org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.MultiOutWordIndexMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.WordCountMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.MultiOutWordCountReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'org.apache.hcatalog.mapreduce.TestMultiOutputFormat.NullOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.OutputCommitter.<init>'
'org.apache.pig.test.TestMultiQueryBasic.testMultiStoreWithOutputFormat','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.pig.test.TestMultiQueryBasic.DummyStoreWithOutputFormat.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.test.TestMultiQueryBasic.DummyOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.pig.test.TestMultiQueryLocal.PigStorageWithConfig.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.pig.test.TestMultiQueryLocal.PigTextOutputFormatWithConfig.getOutputCommitter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.datasalt.pangool.examples.solr.TestMultiShakespeareIndexer.test','org.apache.hadoop.util.ToolRunner.run'
'org.apache.pig.piggybank.test.storage.TestMultiStorage.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.pig.piggybank.test.storage.TestMultiStorage.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'org.apache.pig.piggybank.test.storage.TestMultiStorage.verifyResults','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.open'
'org.apache.accumulo.server.test.TestMultiTableIngest.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.cloudata.core.testjob.performance.TestMultiThreadCTable.putData','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setLong org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setMapSpeculativeExecution org.apache.hadoop.mapred.JobConf.setMaxMapAttempts org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get'
'org.cloudata.core.testjob.performance.TestMultiThreadCTable.PutDataMap.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.collect org.apache.hadoop.mapred.Reporter.progress org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable>.collect'
'org.cloudata.core.testjob.performance.TestMultiThreadCTable.PutDataMap.configure','org.apache.hadoop.mapred.JobConf.getInt'
'co.nubetech.hiho.similarity.ngram.TestNGramJob.runNgramJob','org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.hiho.similarity.ngram.TestNGramJob.testNGramJobForValidValues','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IOUtils.closeStream'
'co.nubetech.hiho.similarity.ngram.TestNGramReducer.testReducerForNullValues','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'co.nubetech.hiho.similarity.ngram.TestNGramReducer.testReducerValidValues','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.classifier.naivebayes.test.TestNaiveBayesDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.classifier.naivebayes.test.TestNaiveBayesDriver.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.SequenceFile.Reader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Reader.next org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.SequenceFile.Reader.close org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.naivebayes.test.TestNaiveBayesDriver.runMapReduce','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.cloudera.sqoop.io.TestNamedFifo.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.mkdirs'
'com.cloudera.sqoop.io.TestNamedFifo.testNamedFifo','org.apache.hadoop.fs.Path.toString org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.hcatalog.listener.TestNotificationListener.setUp','org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start org.apache.hadoop.hive.ql.Driver.<init> org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>'
'org.apache.hcatalog.listener.TestNotificationListener.onMessage','org.apache.hadoop.hive.metastore.api.Database.getName org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.getPartitionKeysSize org.apache.hadoop.hive.metastore.api.Partition.getTableName org.apache.hadoop.hive.metastore.api.Partition.getDbName org.apache.hadoop.hive.metastore.api.Partition.getValues org.apache.hadoop.hive.metastore.api.Partition.getTableName org.apache.hadoop.hive.metastore.api.Partition.getDbName org.apache.hadoop.hive.metastore.api.Partition.getValues org.apache.hadoop.hive.metastore.api.Table.getTableName org.apache.hadoop.hive.metastore.api.Table.getDbName org.apache.hadoop.hive.metastore.api.Table.getPartitionKeysSize org.apache.hadoop.hive.metastore.api.Database.getName'
'org.apache.nutch.parse.oo.TestOOParser.testIt','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.clustering.TestOnlineClustererFactory.setUp','org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.ontology.TestOntologyFactory.setUp','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.client.TestOozieCLI.createConfigFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml'
'org.apache.oozie.client.TestOozieCLI.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.client.TestOozieCLI.createConfigFile','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml'
'org.apache.oozie.client.TestOozieCLI.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.pig.test.TestPOPartialAgg.checkInputAndOutput','org.apache.hadoop.conf.Configuration.<init>'
'com.tgam.hadoop.test.TestPageNameCount.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.tgam.hadoop.test.TestPageNameCount.Reduce.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.tgam.hadoop.test.TestPageNameCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.oozie.util.TestParameterVerifier.testVerifyParametersNull','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.size org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.TestParameterVerifier.testVerifyParametersEmpty','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.size org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.size org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.TestParameterVerifier.testVerifyParametersMissing','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.size org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.size org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.size org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.TestParameterVerifier.testVerifyParametersDefined','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.size org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.size org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.TestParameterVerifier.testVerifyParametersEmptyName','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.nutch.parse.TestParserFactory.setUp','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.parse.TestParserFactory.setUp','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.gora.query.impl.TestPartitionQueryImpl.testReadWrite','org.apache.hadoop.io.TestWritable.testWritable'
'org.apache.giraph.graph.partition.TestPartitionStores.testReadWrite','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.hama.bsp.TestPartitioning.testPartitioner','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.coord.TestPastActionsTimeOut._testSubmitJob','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestPastActionsTimeOut._testSubmitJob','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.coord.TestPastActionsTimeOut._testSubmitJob','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.piggybank.test.storage.TestPathPartitioner.setUp','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.oozie.action.hadoop.TestPigActionExecutor.testLauncherJar','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestPigActionExecutor.testSetupMethods','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.TestPigActionExecutor.createContext','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestPigActionExecutor.submitAction','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob'
'org.apache.oozie.action.hadoop.TestPigActionExecutor._testSubmit','org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.apache.oozie.action.hadoop.TestPigActionExecutor.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.TestPigActionExecutor.testPig','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.hadoop.TestPigActionExecutor.testPigError','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.action.hadoop.TestPigActionExecutor.testUdfPig','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName'
'org.apache.oozie.action.hadoop.TestPigMain.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.apache.pig.test.TestPigServerWithMacros.testRegisterRemoteMacro','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.pig.test.TestPigServerWithMacros.testRegisterRemoteScript','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.pig.test.TestPigStats.testPigScriptInConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.test.TestPigStats.testJythonScriptInConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.test.TestPigStorage.testPigStorageSchemaSearch','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.pig.test.TestPigTupleRawComparator.setUp','org.apache.hadoop.mapred.JobConf.<init>'
'org.apache.pig.test.TestPigTupleRawComparator.testSortOrder','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.pig.test.TestPigTupleRawComparator.compareHelper','org.apache.hadoop.io.RawComparator.compare'
'org.apache.oozie.action.hadoop.TestPipesMain.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'org.apache.nutch.plugin.TestPluginSystem.setUp','org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.plugin.TestPluginSystem.testRepositoryCache','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource'
'org.apache.nutch.plugin.TestPluginSystem.getPluginFolder','org.apache.hadoop.conf.Configuration.getStrings'
'meetup.beeno.TestPool.runPool','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get'
'meetup.beeno.TestPool.runNew','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.HTable.close'
'com.odiago.flumebase.plan.TestPropagateSchemas.testOneNode','org.apache.hadoop.conf.Configuration.<init>'
'com.odiago.flumebase.plan.TestPropagateSchemas.testCarryOneForward','org.apache.hadoop.conf.Configuration.<init>'
'com.odiago.flumebase.plan.TestPropagateSchemas.testMismatchedPredecessors','org.apache.hadoop.conf.Configuration.<init>'
'com.odiago.flumebase.plan.TestPropagateSchemas.testMismatchedOutputInput','org.apache.hadoop.conf.Configuration.<init>'
'com.odiago.flumebase.plan.TestPropagateSchemas.testInterruptedPropagation','org.apache.hadoop.conf.Configuration.<init>'
'com.odiago.flumebase.plan.TestPropagateSchemas.testUnchangedNode','org.apache.hadoop.conf.Configuration.<init>'
'com.odiago.flumebase.plan.TestPropagateSchemas.testUnchangedNodeWithPrior','org.apache.hadoop.conf.Configuration.<init>'
'com.sentric.hbase.coprocessor.TestProspectiveSearchRegionObserver.fillAccountTable','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close'
'com.sentric.hbase.coprocessor.TestProspectiveSearchRegionObserver.setupBeforeClass','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HBaseTestingUtility.createTable'
'com.sentric.hbase.coprocessor.TestProspectiveSearchRegionObserver.tearDownAfterClass','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'com.sentric.hbase.coprocessor.TestProspectiveSearchRegionObserver.writeArticleShouldMatchAgent','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Put.getTimeStamp'
'com.sentric.hbase.coprocessor.TestProspectiveSearchRegionObserver.writeArticleShouldNotMatch','org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.HTable.<init>'
'com.sentric.hbase.coprocessor.TestProspectiveSearchRegionObserver.assembleRowKey','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.sentric.hbase.coprocessor.TestProspectiveSearchRegionObserver.check','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator'
'com.sentric.hbase.coprocessor.TestProspectiveSearchRegionObserver.checkRowAndDelete','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.HTable.delete'
'com.squareup.cascading2.scheme.TestProtobufScheme.testRoundtrip','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init>'
'org.apache.nutch.protocol.httpclient.TestProtocolHttpClient.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource'
'org.apache.oozie.service.TestProxyUserService.testService','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testWrongConfigGroups','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testWrongHost','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testWrongConfigHosts','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testValidateAnyHostAnyUser','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testInvalidProxyUser','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testValidateHost','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.getGroup','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testValidateGroup','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testUnknownHost','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testInvalidHost','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestProxyUserService.testInvalidGroup','org.apache.hadoop.util.StringUtils.join org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.hadoop.tdg.TestPseudoHadoop.setUpClass','org.apache.hadoop.io.IOUtils.closeStream'
'org.hadoop.tdg.TestPseudoHadoop.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem'
'org.hadoop.tdg.TestPseudoHadoop.tearDown','org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'org.hadoop.tdg.TestPseudoHadoop.copyFileWithProgress','org.apache.hadoop.util.Progressable.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'org.hadoop.tdg.TestPseudoHadoop.readWithURLHandler','org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init> org.apache.hadoop.fs.FileSystem.getUri'
'org.hadoop.tdg.TestPseudoHadoop.readWithFileSystem','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.hadoop.tdg.TestPseudoHadoop.printStream','org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream'
'org.hadoop.tdg.TestPseudoHadoop.listFiles','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileUtil.stat2Paths'
'org.hadoop.tdg.TestPseudoHadoop.deleteFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.hadoop.tdg.TestPseudoHadoop.writeAndReadBzipCompressed','org.apache.hadoop.io.compress.BZip2Codec.<init> org.apache.hadoop.io.compress.BZip2Codec.getDefaultExtension org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.BZip2Codec.createOutputStream org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.BZip2Codec.createInputStream org.apache.hadoop.io.IOUtils.copyBytes'
'org.hadoop.tdg.TestPseudoHadoop.sequenceFileIO','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.IntWritable.getClass org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IOUtils.closeStream'
'org.hadoop.tdg.TestPseudoHadoop.mapFileIO','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.io.LongWritable.getClass org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.oozie.service.TestPurgeService.testPurgeServiceForWorkflow','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.service.TestPurgeService.testPurgeServiceForWorkflow','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestPurgeXCommand.addRecordToWfJobTableForNegCase','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestPurgeXCommand.createWorkflow','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.command.wf.TestPurgeXCommand.addRecordToWfJobTableForNegCase','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestPurgeXCommand.createWorkflow','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.cloudera.sqoop.TestQuery.runQueryTest','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IOUtils.closeStream'
'org.apache.hcatalog.rcfile.TestRCFileMapReduceInputFormat.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.close'
'org.apache.hcatalog.rcfile.TestRCFileMapReduceInputFormat.writeThenReadByRecordReader','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.ql.io.RCFileOutputFormat.setColumnNumber org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.<init> org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.<init> org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.set org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable>.nextKeyValue'
'com.twitter.elephantbird.pig.load.TestRCFileProtobufStorage.setUp','org.apache.hadoop.fs.FileUtil.fullyDelete'
'com.twitter.elephantbird.pig.load.TestRCFileProtobufStorage.testRCFileStorage','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable>.write org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable>.close org.apache.hadoop.fs.FileUtil.fullyDelete'
'com.twitter.elephantbird.pig.load.TestRCFileProtobufStorage.createProtoWriter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.OutputFormat.getRecordWriter'
'com.twitter.elephantbird.pig.load.TestRCFileProtobufStorage.getDefaultWorkFile','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.TestRENAMEHandler.setup','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getParent'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.TestRENAMEHandler.testOldDirIsNotDir','org.apache.hadoop.fs.Path.getParent'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.TestRENAMEHandler.testNewDirIsNotDir','org.apache.hadoop.fs.Path.getParent'
'org.apache.hama.ipc.TestRPC.TestImpl.testServerGet','org.apache.hadoop.ipc.Server.get'
'org.apache.hama.ipc.TestRPC.testCalls','org.apache.hadoop.ipc.RPC.getServer org.apache.hadoop.ipc.Server.start org.apache.hadoop.ipc.RPC.getProxy org.apache.hadoop.ipc.RPC.call org.apache.hadoop.ipc.RPC.call org.apache.hadoop.ipc.Server.stop'
'org.apache.nutch.parse.rss.TestRSSParser.testIt','org.apache.hadoop.io.Text.<init>'
'org.apache.nutch.parse.rtf.TestRTFParser.testIt','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator.testRandomSeedGenerator','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator.testRandomSeedGenerator','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.wf.TestReRunCommand.testRerun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.wf.TestReRunXCommand.testRerun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.wf.TestReRunXCommand.testRerunFromFailNodes','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.wf.TestReRunXCommand.testRerun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.command.wf.TestReRunXCommand.testRerunFromFailNodes','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init>'
'org.apache.hcatalog.hbase.snapshot.TestRevisionManagerConfiguration.testDefault','org.apache.hadoop.conf.Configuration.get'
'org.apache.cassandra.client.TestRingCache.setup','org.apache.hadoop.conf.Configuration.<init>'
'com.digitalpebble.behemoth.solr.TestSOLRWriter.testFieldMappings','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.nutch.parse.swf.TestSWFParser.testIt','org.apache.hadoop.io.Text.<init>'
'org.apache.pig.data.TestSchemaTuple.testInterStorageSerDe','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.TaskAttemptID.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init>'
'co.nubetech.hiho.similarity.ngram.TestScoreMapper.testMapperValidValues','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'co.nubetech.hiho.similarity.ngram.TestScoreMapper.testMapperForNullValues','org.apache.hadoop.io.IntWritable.<init>'
'co.nubetech.hiho.similarity.ngram.TestScoreReducer.testReducerValidValues','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'co.nubetech.hiho.similarity.ngram.TestScoreReducer.testReducerForNullValues','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.jena.tdbloader4.TestScriptedMiniMRCluster.run','org.apache.hadoop.util.ToolRunner.run'
'com.datasalt.pangool.examples.secondarysort.TestSecondarySort.testPangool','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.setUpHCatDriver','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.ql.Driver.<init> org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testDescDB','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.getResults org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testCreateTblWithLowerCasePartNames','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getErrorMessage org.apache.hadoop.hive.metastore.api.Table.getPartitionKeys'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testAlterTblFFpart','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.mapred.TextInputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.io.RCFileInputFormat.getName org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getName org.apache.hadoop.hive.metastore.api.Partition.getSd org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testUsNonExistentDB','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testDatabaseOperations','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testCreateTableIfNotExists','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.ql.io.RCFileInputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getErrorMessage org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.ql.io.RCFileInputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testAlterTblTouch','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testChangeColumns','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testAddReplaceCols','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getErrorMessage org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.metastore.api.FieldSchema.<init> org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testAlterTblClusteredBy','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testAlterTableSetFF','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.io.RCFileInputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.io.RCFileInputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testAddPartFail','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testAddPartPass','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getErrorMessage org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testCTAS','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getErrorMessage org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testStoredAs','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testAddDriverInfo','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.io.RCFileInputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getName org.apache.hadoop.hive.metastore.api.Table.getSd org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testInvalidateNonStringPartition','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getErrorMessage'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testInvalidateSeqFileStoredAs','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testInvalidateTextFileStoredAs','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testInvalidateClusteredBy','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testCTLFail','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.hcatalog.cli.TestSemanticAnalysis.testCTLPass','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.pig.piggybank.test.storage.TestSequenceFileLoader.setUp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.getClass org.apache.hadoop.io.Text.getClass org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.IOUtils.closeStream'
'com.cloudera.flume.handlers.seqfile.TestSequenceFileOutputFormat.sequenceFileWriteReadHelper','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init>'
'org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.Initialize','org.apache.hadoop.hive.conf.HiveConf.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.ql.Driver.<init> org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.testSequenceTableWriteRead','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.testTextTableWriteRead','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run'
'org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.testSequenceTableWriteReadMR','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.conf.HiveConf.getAllProperties org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.testTextTableWriteReadMR','org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.hive.ql.Driver.run org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hive.conf.HiveConf.getAllProperties org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.input.TextInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'org.apache.mahout.text.TestSequenceFilesFromDirectory.testSequenceFileFromDirectoryBasic','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.text.TestSequenceFilesFromDirectory.testSequnceFileFromDirectoryTsv','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.text.TestSequenceFilesFromDirectory.createFilesFromArrays','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.text.TestSequenceFilesFromDirectory.createTsvFilesFromArrays','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.text.TestSequenceFilesFromDirectory.checkChunkFiles','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.mahout.text.TestSequenceFilesFromDirectory.ExcludeDotFiles.accept','org.apache.hadoop.fs.Path.getName'
'cascading.tuple.hadoop.TestSerialization.TestTextDeserializer.deserialize','org.apache.hadoop.io.WritableUtils.readString'
'cascading.tuple.hadoop.TestSerialization.TestTextSerializer.serialize','org.apache.hadoop.io.WritableUtils.writeString'
'sizzle.aggregators.TestSetAggregator.testSetAggregatorCombineDistinct','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestSetAggregator.testSetAggregatorCombineIndistinct','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestSetAggregator.testSetAggregatorCombineLess','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestSetAggregator.testSetAggregatorCombineMore','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestSetAggregator.testSetAggregatorReduceDistinct','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'sizzle.aggregators.TestSetAggregator.testSetAggregatorReduceInistinct','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'sizzle.aggregators.TestSetAggregator.testSetAggregatorReduceLess','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'sizzle.aggregators.TestSetAggregator.testSetAggregatorReduceMore','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'org.apache.oozie.action.hadoop.TestShellActionExecutor.testLauncherJar','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestShellActionExecutor.testSetupMethods','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScript','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName'
'org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScriptError','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName'
'org.apache.oozie.action.hadoop.TestShellActionExecutor.testPerlScript','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName'
'org.apache.oozie.action.hadoop.TestShellActionExecutor.testEnvVar','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName'
'org.apache.oozie.action.hadoop.TestShellActionExecutor.evaluate','org.apache.hadoop.mapred.RunningJob.isComplete org.apache.hadoop.mapred.RunningJob.isComplete'
'org.apache.oozie.action.hadoop.TestShellActionExecutor._testSubmit','org.apache.hadoop.mapred.RunningJob.isSuccessful'
'org.apache.oozie.action.hadoop.TestShellActionExecutor.submitAction','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobID.forName org.apache.hadoop.mapred.JobClient.getJob'
'com.datasalt.pangool.examples.simplesecondarysort.TestSimpleSecondarySort.test','org.apache.hadoop.util.ToolRunner.run'
'edu.duke.starfish.jobopt.junit.TestSmartEnumJobOptimizer.testWhatIfJobConfGetTime','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.hbase.TestSnapshots.Initialize','org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.hbase.TestSnapshots.TestSnapshotConversion','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode'
'org.apache.avro.hadoop.file.TestSortedKeyValueFile.testWriteOutOfSortedOrder','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.avro.hadoop.file.TestSortedKeyValueFile.testWriter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.avro.hadoop.file.TestSortedKeyValueFile.testReader','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestSplitCombine.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setLong'
'org.apache.pig.test.TestSplitIndex.SplitSensitiveLoadFunc.prepareToRead','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath'
'org.apache.pig.test.TestSplitIndex.SplitSensitiveLoadFunc.getNext','org.apache.hadoop.fs.Path.toString'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.getWritePath','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.ensureEmptyWriteDir','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.exists'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.getConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.verifyFileExists','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.exists'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.verifyFileDoesNotExist','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.exists'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.testNonSplittingTextFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.testNonSplittingGzipFile','org.apache.hadoop.io.compress.GzipCodec.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.testSplittingTextFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.io.TestSplittableBufferedWriter.testSplittingGzipFile','org.apache.hadoop.io.compress.GzipCodec.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.ssh.TestSshActionExecutor.Context.getActionDir','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.ssh.TestSshActionExecutor.setUp','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.oozie.action.ssh.TestSshActionExecutor.testConnectionErrors','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'com.datasalt.pangool.flow.TestStandardDeviationFlow.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.service.TestStatusTransitService.addRecordToWfJobTable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'.TestStep0.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'.TestStep1.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.pig.test.TestStore.testBinStorageGetSchema','org.apache.hadoop.mapreduce.Job.<init>'
'org.apache.pig.test.TestStore.DummyStore.storeSchema','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.pig.test.TestStore.DummyStore.cleanupOnFailure','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.pig.test.TestStore.DummyOutputFormat.getDefaultWorkFile','org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getWorkPath org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestStore.DummyOutputCommitter.setupJob','org.apache.hadoop.mapreduce.OutputCommitter.setupJob'
'org.apache.pig.test.TestStore.DummyOutputCommitter.setupTask','org.apache.hadoop.mapreduce.OutputCommitter.setupTask'
'org.apache.pig.test.TestStore.DummyOutputCommitter.commitTask','org.apache.hadoop.mapreduce.OutputCommitter.commitTask'
'org.apache.pig.test.TestStore.DummyOutputCommitter.abortTask','org.apache.hadoop.mapreduce.OutputCommitter.abortTask'
'org.apache.pig.test.TestStore.DummyOutputCommitter.cleanupJob','org.apache.hadoop.mapreduce.OutputCommitter.cleanupJob'
'org.apache.pig.test.TestStore.DummyOutputCommitter.commitJob','org.apache.hadoop.mapreduce.OutputCommitter.commitJob'
'org.apache.pig.test.TestStore.DummyOutputCommitter.abortJob','org.apache.hadoop.mapreduce.OutputCommitter.abortJob'
'org.apache.pig.test.TestStore.DummyOutputCommitter.createFile','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.pig.test.TestStoreInstances.STFuncCheckInstances.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.set'
'org.apache.pig.test.TestStoreInstances.OutputCommitterTestInstances.OutputCommitterTestInstances','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.action.hadoop.TestStreamingMain.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.hadoop.TestStreamingMain.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.filecache.DistributedCache.addFileToClassPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowStart','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowRecovery','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testConfigPropagation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testGetGroupFromParent','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testConfigNotPropagation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubworkflowLib','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowStart','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowRecovery','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testConfigPropagation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testConfigNotPropagation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.graph.TestSubmitGraphJob.testSubmitJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.hama.graph.TestSubmitGraphJob.verifyResult','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.get'
'org.apache.oozie.command.wf.TestSubmitMRXCommand.testWFXmlGeneration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitMRXCommand.testWFXmlGenerationNegative1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitMRXCommand.testWFXmlGeneration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitMRXCommand.testWFXmlGenerationNegative1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitPigXCommand.testWFXmlGeneration1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitPigXCommand.testWFXmlGeneration2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitPigXCommand.testWFXmlGenerationNegative1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitPigXCommand.testWFXmlGeneration1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitPigXCommand.testWFXmlGeneration2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitPigXCommand.testWFXmlGenerationNegative1','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitXCommand.testSubmitAppName','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitXCommand.testSubmitReservedVars','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitXCommand.testAppPathIsDir','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitXCommand.testAppPathIsFile1','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitXCommand.testAppPathIsFile2','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.command.wf.TestSubmitXCommand.testAppPathIsFileNegative','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.searcher.TestSummarizerFactory.setUp','org.apache.hadoop.conf.Configuration.set'
'org.apache.hama.bsp.sync.TestSyncServiceFactory.testClientInstantiation','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hama.bsp.sync.TestSyncServiceFactory.testServerInstantiation','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.jena.tdbloader4.TestTDBLoader4MiniMRCluster.test','org.apache.hadoop.util.ToolRunner.run'
'pl.edu.icm.coansys.similarity.pig.script.TestTFIDF.afterClass','org.apache.hadoop.fs.Path.<init>'
'com.cloudera.sqoop.hive.TestTableDefWriter.testDifferentTableNames','org.apache.hadoop.conf.Configuration.<init>'
'com.cloudera.sqoop.hive.TestTableDefWriter.testDifferentTargetDirs','org.apache.hadoop.conf.Configuration.<init>'
'com.cloudera.sqoop.hive.TestTableDefWriter.testPartitions','org.apache.hadoop.conf.Configuration.<init>'
'com.cloudera.sqoop.hive.TestTableDefWriter.testLzoSplitting','org.apache.hadoop.conf.Configuration.<init>'
'com.cloudera.sqoop.hive.TestTableDefWriter.testUserMapping','org.apache.hadoop.conf.Configuration.<init>'
'com.cloudera.sqoop.hive.TestTableDefWriter.testUserMappingFailWhenCantBeApplied','org.apache.hadoop.conf.Configuration.<init>'
'org.cloudata.core.parallel.TestTabletInputFormat.testTabletInputFormat','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.RecordReader<org.cloudata.core.parallel.Row.Key,org.apache.hadoop.io.Writable>.createKey org.apache.hadoop.mapred.RecordReader<org.cloudata.core.parallel.Row.Key,org.apache.hadoop.io.Writable>.createValue org.apache.hadoop.mapred.RecordReader<org.cloudata.core.parallel.Row.Key,org.apache.hadoop.io.Writable>.next'
'com.cloudera.sqoop.TestTargetDir.testTargetDir','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.getContentSummary org.apache.hadoop.fs.ContentSummary.getFileCount org.apache.hadoop.util.StringUtils.stringifyException'
'com.cloudera.sqoop.TestTargetDir.testExistingTargetDir','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.hcatalog.templeton.tool.TestTempletonUtils.testHadoopFsPath','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.templeton.tool.TestTempletonUtils.testHadoopFsFilename','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.templeton.tool.TestTempletonUtils.testHadoopFsListAsArray','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.templeton.tool.TestTempletonUtils.testHadoopFsListAsString','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init>'
'sizzle.aggregators.TestTextAggregator.testTextAggregatorCombine','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestTextAggregator.testTextAggregatorReduce','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'org.apache.giraph.io.TestTextDoubleDoubleAdjacencyListVertexInputFormat.setUp','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.nextKeyValue org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.giraph.io.TestTextDoubleDoubleAdjacencyListVertexInputFormat.testIndexMustHaveValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.TestTextDoubleDoubleAdjacencyListVertexInputFormat.testEdgesMustHaveValues','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue org.apache.hadoop.io.Text.<init>'
'org.apache.giraph.io.TestTextDoubleDoubleAdjacencyListVertexInputFormat.testHappyPath','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.io.TestTextDoubleDoubleAdjacencyListVertexInputFormat.testLineSanitizer','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.io.TestTextDoubleDoubleAdjacencyListVertexInputFormat.testDifferentSeparators','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.getCurrentValue org.apache.hadoop.io.Text.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'com.twitter.elephantbird.pig.load.TestThriftMultiFormatLoader.setUp','org.apache.hadoop.conf.Configuration.<init>'
'com.twitter.elephantbird.pig.load.TestThriftMultiFormatLoader.testMultiFormatLoader','org.apache.hadoop.fs.FileUtil.fullyDelete'
'com.cloudera.sqoop.tool.TestToolPlugin.testPlugin','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'com.cloudera.sqoop.tool.TestToolPlugin.testNoOverrideTools','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.StringUtils.stringifyException'
'sizzle.aggregators.TestTopAggregator.testTopAggregatorTopTenCombine','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,sizzle.io.EmitKey,sizzle.io.EmitValue>.runTest'
'sizzle.aggregators.TestTopAggregator.testTopAggregatorTopTenReduce','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.resetOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'sizzle.aggregators.TestTopAggregator.testTopAggregatorAllEqual','org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.setInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<sizzle.io.EmitKey,sizzle.io.EmitValue,org.apache.hadoop.io.Text,org.apache.hadoop.io.NullWritable>.runTest'
'com.datasalt.pangool.flow.TestTopCountryBySimilarsFlow.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.examples.topicalwordcount.TestTopicFingerprint.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.examples.topicalwordcount.TestTopicalWordCount.test','org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.conf.Configuration.<init>'
'com.datasalt.pangool.examples.topicalwordcount.TestTopicalWordCount.assertOutput','org.apache.hadoop.fs.Path.<init>'
'com.yahoo.omid.TestTransactionConflict.runTestWriteWriteConflict','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add'
'com.yahoo.omid.TestTransactionConflict.runTestMultiTableConflict','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.client.HBaseAdmin.createTable org.apache.hadoop.hbase.client.HBaseAdmin.isTableDisabled org.apache.hadoop.hbase.client.HBaseAdmin.enableTable org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.size'
'com.yahoo.omid.TestTransactionConflict.runTestCleanupAfterConflict','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.equals org.apache.hadoop.hbase.client.Result.size org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.equals'
'com.yahoo.omid.TestTransactionConflict.testCleanupWithDeleteRow','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.ResultScanner.next'
'com.datasalt.pangool.tuplemr.serialization.TestTupleFieldSerialization.test','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.tuplemr.serialization.TestTupleFieldSerialization.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.datasalt.pangool.tuplemr.mapred.TestTupleHashPartitioner.multipleSourcesTest','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.tuplemr.mapred.TestTupleHashPartitioner.sanityTest','org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.tuplemr.mapred.lib.output.TestTupleInputOutputFormat.test','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.pig.test.TestTypedMap.testSimpleLoad','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.pig.test.TestTypedMap.testSimpleMapKeyLookup','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.pig.test.TestTypedMap.testSimpleMapCast','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.pig.test.TestTypedMap.testComplexLoad','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.pig.test.TestTypedMap.testComplexCast','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.pig.test.TestTypedMap.testComplexCast2','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.pig.test.TestTypedMap.testUnTypedMap','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.pig.test.TestTypedMap.testOrderBy','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.nutch.net.TestURLFilters.testNonExistingUrlFilter','org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.net.TestURLNormalizers.testURLNormalizers','org.apache.hadoop.conf.Configuration.set'
'org.apache.nutch.net.TestURLNormalizers.testURLNormalizers','org.apache.hadoop.conf.Configuration.set'
'com.hphoto.server.TestUidServer.TestUidServer','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.ipc.RPC.waitForProxy org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException'
'com.yahoo.omid.TestUpdateScan.testGet','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.filter.BinaryPrefixComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.filter.FilterList.<init> org.apache.hadoop.hbase.filter.BinaryPrefixComparator.<init> org.apache.hadoop.hbase.filter.RowFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.filter.WhileMatchFilter.<init> org.apache.hadoop.hbase.filter.FilterList.addFilter org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.filter.WhileMatchFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toInt'
'com.yahoo.omid.TestUpdateScan.testScan','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toInt'
'com.yahoo.omid.TestUpdateScan.testScanUncommitted','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toInt'
'com.datasalt.pangool.examples.urlresolution.TestUrlResolution.testPangool','org.apache.hadoop.util.ToolRunner.run'
'com.datasalt.pangool.examples.useractivitynormalizer.TestUserActivityNormalizer.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.flume.channel.file.TestUtils.toDataInput','org.apache.hadoop.io.Writable.write'
'backtype.support.TestUtils.getTmpPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'backtype.support.TestUtils.deletePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.springframework.data.hadoop.TestUtils.mkdir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'org.springframework.data.hadoop.TestUtils.writeToFS','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.cloudera.hadoop.hdfs.nfs.TestUtils.setupConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.TestV0JobServlet.call','org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.TestV0JobServlet.testReRun','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.TestV0JobServlet.testInvalidReRunConfigurations','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.TestV0JobServlet.call','org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.TestV0JobServlet.testReRun','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.TestV0JobServlet.testInvalidReRunConfigurations','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.TestV0JobServlet.call','org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.TestV0JobServlet.testReRun','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.TestV0JobServlet.testInvalidReRunConfigurations','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.servlet.TestV1JobServlet.call','org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.TestV1JobServlet.call','org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.TestV1JobsServlet.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.writeXml'
'org.apache.mahout.clustering.spectral.common.TestVectorCache.testSave','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.clustering.spectral.common.TestVectorCache.testLoad','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.deleteOnExit org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.setCacheFiles'
'org.apache.mahout.clustering.spectral.common.TestVectorCache.testAll','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob.testVectorDistanceMapper','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob.testVectorDistanceInvertedMapper','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob.testRun','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob.testRunInverted','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run org.apache.hadoop.fs.Path.<init>'
'com.vertica.hadoop.TestVertica.getVerticaJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.vertica.hadoop.TestVertica.getVerticaSplit','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.vertica.hadoop.TestVertica.testVerticaRecord','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init>'
'com.vertica.hadoop.TestVertica.recordTest','org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'com.vertica.hadoop.TestVertica.testVerticaSplit','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset'
'com.vertica.hadoop.TestVertica.testVerticaReader','org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.io.LongWritable.get'
'com.vertica.hadoop.TestVertica.validateInput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.JobID.<init> org.apache.hadoop.mapreduce.JobContext.<init> org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'com.vertica.hadoop.TestVertica.testVerticaOutput','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Job.getConfiguration'
'edu.duke.starfish.whatif.junit.TestWhatIfEngine.testWhatIfJobConfGetTime','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'edu.duke.starfish.whatif.junit.TestWhatIfEngine.testWhatIfJobConfGetJobInfo','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setLong org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'com.cloudera.sqoop.TestWhere.runWhereTest','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.IOUtils.closeStream'
'pignlproc.format.TestWikipediaParsing.testEnWikipediaParsingFromReader','org.apache.hadoop.io.Text.<init>'
'com.wibidata.avro.mapreduce.TestWordCount.LineCountMapper.setup','org.apache.hadoop.io.IntWritable.<init>'
'com.wibidata.avro.mapreduce.TestWordCount.StatCountMapper.setup','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init>'
'com.wibidata.avro.mapreduce.TestWordCount.StatCountMapper.map','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.Text.set'
'com.wibidata.avro.mapreduce.TestWordCount.GenericStatsReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'com.wibidata.avro.mapreduce.TestWordCount.SpecificStatsReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'com.wibidata.avro.mapreduce.TestWordCount.SortReducer.reduce','org.apache.hadoop.io.NullWritable.get'
'com.wibidata.avro.mapreduce.TestWordCount.testAvroGenericOutput','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.wibidata.avro.mapreduce.TestWordCount.testAvroSpecificOutput','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.wibidata.avro.mapreduce.TestWordCount.testAvroInput','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.wibidata.avro.mapreduce.TestWordCount.testAvroMapOutput','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.suffix org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.Job.getConfiguration'
'org.apache.avro.mapred.tether.TestWordCountTether.testJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.FileInputFormat.addInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.set'
'org.apache.trevni.avro.TestWordCount.testOutputFormat','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.trevni.avro.TestWordCount.testInputFormat','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.avro.mapred.TestWordCount.testJob','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.avro.mapred.TestWordCount.testProjection','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.mapred.FileSplit.<init> org.apache.hadoop.io.NullWritable.get'
'org.apache.oozie.client.TestWorkflowClient.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.executor.jpa.TestWorkflowIdGetForExternalIdJPAExecutor.addRecordToWfJobTable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.executor.jpa.TestWorkflowIdGetForExternalIdJPAExecutor.addRecordToWfJobTable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.executor.jpa.TestWorkflowIdGetForExternalIdJPAExecutor.addRecordToWfJobTable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.executor.jpa.TestWorkflowJobsGetForPurgeJPAExecutor.addRecordToWfJobTable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.executor.jpa.TestWorkflowJobsGetForPurgeJPAExecutor.addRecordToWfJobTable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.client.TestWorkflowXClient.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.client.TestWorkflowXClient.call','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.util.TestWritableUtils.testWritableUtils','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString'
'org.apache.oozie.util.TestXConfiguration.testCopy','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.TestXConfiguration.testInjectDefaults','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.TestXConfiguration.testCopy','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.util.TestXConfiguration.testInjectDefaults','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.berkeley.chukwa_xtrace.TestXtrAdaptor.testXtrAdaptor','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.<init> org.apache.hadoop.chukwa.datacollection.connector.ChunkCatcherConnector.<init> org.apache.hadoop.chukwa.datacollection.connector.ChunkCatcherConnector.start org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.adaptorCount org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.processAddCommand org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.adaptorCount org.apache.hadoop.chukwa.datacollection.connector.ChunkCatcherConnector.waitForAChunk org.apache.hadoop.chukwa.Chunk.getData org.apache.hadoop.chukwa.datacollection.connector.ChunkCatcherConnector.waitForAChunk org.apache.hadoop.chukwa.Chunk.getData org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.shutdown'
'edu.berkeley.chukwa_xtrace.TestXtrAdaptor.testXtrAdaptor','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.<init> org.apache.hadoop.chukwa.datacollection.connector.ChunkCatcherConnector.<init> org.apache.hadoop.chukwa.datacollection.connector.ChunkCatcherConnector.start org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.adaptorCount org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.processAddCommand org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.adaptorCount org.apache.hadoop.chukwa.datacollection.connector.ChunkCatcherConnector.waitForAChunk org.apache.hadoop.chukwa.Chunk.getData org.apache.hadoop.chukwa.datacollection.connector.ChunkCatcherConnector.waitForAChunk org.apache.hadoop.chukwa.Chunk.getData org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent.shutdown'
'edu.berkeley.chukwa_xtrace.TestXtrIndex.testIndexing','org.apache.hadoop.io.Text.toString'
'org.apache.hcatalog.hbase.snapshot.TestZNodeSetUp.Initialize','org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hbase.HBaseConfiguration.merge org.apache.hadoop.hive.conf.HiveConf.set org.apache.hadoop.hive.cli.CliSessionState.<init> org.apache.hadoop.hive.ql.session.SessionState.start'
'org.apache.hcatalog.hbase.snapshot.TestZNodeSetUp.testBasicZNodeCreation','org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'.Test.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.apache.nutch.tools.proxy.TestbedProxy.main','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.lilyproject.tools.tester.TesterMetrics.TesterMetrics','org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.<init> org.apache.hadoop.hbase.metrics.MetricsRate.<init> org.apache.hadoop.metrics.MetricsUtil.getContext org.apache.hadoop.metrics.MetricsUtil.createRecord org.apache.hadoop.metrics.MetricsContext.registerUpdater'
'org.lilyproject.tools.tester.TesterMetrics.shutdown','org.apache.hadoop.metrics.MetricsContext.unregisterUpdater'
'org.lilyproject.tools.tester.TesterMetrics.doUpdates','org.apache.hadoop.metrics.util.MetricsRegistry.getMetricsList org.apache.hadoop.metrics.util.MetricsBase.pushMetric org.apache.hadoop.metrics.MetricsRecord.update'
'org.lilyproject.tools.tester.TesterMetrics.report','org.apache.hadoop.metrics.util.MetricsLongValue.get org.apache.hadoop.metrics.util.MetricsLongValue.set'
'org.apache.avro.mapred.tether.TetherJob.getExecutable','org.apache.hadoop.mapred.JobConf.get'
'org.apache.avro.mapred.tether.TetherJob.setExecutable','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.avro.mapred.tether.TetherJob.runJob','org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.avro.mapred.tether.TetherJob.submitJob','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.submitJob'
'org.apache.avro.mapred.tether.TetherJob.setupTetherJob','org.apache.hadoop.mapred.JobConf.setMapRunnerClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.getStringCollection org.apache.hadoop.mapred.JobConf.setStrings org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'org.apache.avro.mapred.tether.TetherOutputFormat.setDeflateLevel','org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInt'
'org.apache.avro.mapred.tether.TetherOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getCompressOutput org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.RecordWriter<org.apache.avro.mapred.tether.TetherData,org.apache.hadoop.io.NullWritable>.<init>'
'org.apache.avro.mapred.tether.TetherRecordReader.TetherRecordReader','org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getStart org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.JobConf.set'
'org.apache.avro.mapred.tether.TetherRecordReader.createValue','org.apache.hadoop.io.NullWritable.get'
'sizzle.aggregators.TextAggregator.collect','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.NullWritable.get'
'org.commoncrawl.util.TextBytes.set','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.commoncrawl.util.TextBytes.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.util.TextBytes.skip','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.skipFully'
'org.commoncrawl.util.TextBytes.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.util.TextBytes.Comparator.compare','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize'
'org.commoncrawl.util.TextBytes.readString','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.util.TextBytes.writeString','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.util.TextBytes.main','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableComparator.compareBytes'
'org.commoncrawl.mapred.pipelineV3.domainmeta.TextBytesQuery.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.Path.<init>'
'org.commoncrawl.mapred.pipelineV3.domainmeta.TextBytesQuery.configure','org.apache.hadoop.mapred.JobConf.get'
'org.commoncrawl.mapred.pipelineV3.domainmeta.TextBytesQuery.map','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.apache.hadoop.io.Writable>.collect'
'org.commoncrawl.util.shared.TextBytes.set','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.commoncrawl.util.shared.TextBytes.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.util.shared.TextBytes.skip','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.skipFully'
'org.commoncrawl.util.shared.TextBytes.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.util.shared.TextBytes.Comparator.compare','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize'
'org.commoncrawl.util.shared.TextBytes.readString','org.apache.hadoop.io.WritableUtils.readVInt'
'org.commoncrawl.util.shared.TextBytes.writeString','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.util.shared.TextBytes.main','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.getPosition org.apache.hadoop.io.WritableComparator.compareBytes org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.size org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.WritableComparator.compareBytes'
'cascading.scheme.hadoop.TextDelimited.source','org.apache.hadoop.io.LongWritable.get'
'edu.umd.cloud9.collection.line.TextDocument.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'edu.umd.cloud9.collection.line.TextDocument.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'edu.jhu.thrax.hadoop.comparators.TextFieldComparator.compare','org.apache.hadoop.io.WritableComparator.compare'
'edu.jhu.thrax.hadoop.comparators.TextFieldComparator.getTextLength','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableComparator.readVInt'
'edu.isi.mavuno.input.TextFileInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'edu.isi.mavuno.input.TextFileInputFormat.TextFileRecordReader.TextFileRecordReader','org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'edu.isi.mavuno.input.TextFileInputFormat.TextFileRecordReader.initialize','org.apache.hadoop.io.DataOutputBuffer.reset'
'edu.isi.mavuno.input.TextFileInputFormat.TextFileRecordReader.nextKeyValue','org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.DataOutputBuffer.getData'
'org.apache.crunch.io.text.TextFileReaderFactory.read','org.apache.hadoop.fs.FileSystem.open'
'org.apache.crunch.io.text.TextFileSourceTarget.TextFileSourceTarget','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.formats.adapters.TextLinesRowIterator.TextLinesRowIterator','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init>'
'extramuros.java.formats.adapters.TextLinesRowIterator.readNextRow','org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.readLine org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.util.LineReader.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.util.LineReader.<init>'
'extramuros.java.formats.adapters.TextFileTableAdapter.clone','org.apache.hadoop.conf.Configuration.<init>'
'extramuros.java.formats.adapters.TextFileTableAdapter.save','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.LongWritable.<init>'
'extramuros.java.formats.adapters.TextFileTableAdapter.write','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.write'
'extramuros.java.formats.adapters.TextFileTableAdapter.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.toString'
'extramuros.java.formats.adapters.TextFileTableAdapter.iterator','org.apache.hadoop.fs.Path.<init>'
'extramuros.java.formats.adapters.TextFileTableAdapter.map','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.get'
'extramuros.java.formats.adapters.TextFileTableAdapter.split','org.apache.hadoop.io.Text.toString'
'org.apache.crunch.io.text.TextFileTarget.TextFileTarget','org.apache.hadoop.fs.Path.<init>'
'azkaban.common.web.TextFileViewer.displayFile','org.apache.hadoop.fs.FileSystem.open'
'tap.formats.text.TextFormat.setupOutput','org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass'
'tap.formats.text.TextFormat.setupInput','org.apache.hadoop.mapred.JobConf.setInputFormat'
'com.manning.hip.ch13.localjobrunner.TextIOJobBuilder.TextIOJobBuilder','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get'
'com.manning.hip.ch13.localjobrunner.TextIOJobBuilder.writeInputs','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.manning.hip.ch13.localjobrunner.TextIOJobBuilder.accept','org.apache.hadoop.fs.Path.getName'
'com.manning.hip.ch13.localjobrunner.TextIOJobBuilder.verifyResults','org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.manning.hip.ch13.localjobrunner.TextIOJobBuilder.readLines','org.apache.hadoop.fs.FileSystem.open'
'org.chombo.util.TextInt.TextInt','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.chombo.util.TextInt.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.IntWritable.readFields'
'org.chombo.util.TextInt.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.IntWritable.write'
'org.chombo.util.TextInt.compareTo','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.IntWritable.compareTo'
'org.chombo.util.TextInt.baseCompareTo','org.apache.hadoop.io.Text.compareTo'
'org.chombo.util.TextInt.hashCode','org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.IntWritable.hashCode'
'org.chombo.util.TextInt.baseHashCode','org.apache.hadoop.io.Text.hashCode'
'org.chombo.util.TextInt.equals','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.IntWritable.equals'
'org.chombo.util.TextInt.toString','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.get'
'edu.ucla.sspace.hadoop.TextIntWritable.TextIntWritable','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.ucla.sspace.hadoop.TextIntWritable.compareTo','org.apache.hadoop.io.Text.compareTo'
'edu.ucla.sspace.hadoop.TextIntWritable.hashCode','org.apache.hadoop.io.Text.hashCode'
'edu.ucla.sspace.hadoop.TextIntWritable.equals','org.apache.hadoop.io.Text.equals'
'edu.ucla.sspace.hadoop.TextIntWritable.readFields','org.apache.hadoop.io.Text.readFields'
'edu.ucla.sspace.hadoop.TextIntWritable.write','org.apache.hadoop.io.Text.write'
'.TextIterator.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.bytesToCodePoint'
'cascading.scheme.hadoop.TextLine.sourceConfInit','org.apache.hadoop.mapred.FileInputFormat.getInputPaths org.apache.hadoop.mapred.FileInputFormat.getInputPaths org.apache.hadoop.mapred.JobConf.setInputFormat'
'cascading.scheme.hadoop.TextLine.hasZippedFiles','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName'
'cascading.scheme.hadoop.TextLine.sinkConfInit','org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat'
'cascading.scheme.hadoop.TextLine.sourceHandleInput','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.toString'
'com.twitter.elephanttwin.io.TextLongPairWritable.TextLongPairWritable','org.apache.hadoop.io.LongWritable.get'
'com.twitter.elephanttwin.io.TextLongPairWritable.setText','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'com.twitter.elephanttwin.io.TextLongPairWritable.setLong','org.apache.hadoop.io.LongWritable.get'
'com.twitter.elephanttwin.io.TextLongPairWritable.write','org.apache.hadoop.io.Text.write'
'com.twitter.elephanttwin.io.TextLongPairWritable.readFields','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields'
'com.twitter.elephanttwin.io.TextLongPairWritable.compareTo','org.apache.hadoop.io.Text.compareTo'
'com.twitter.elephanttwin.io.TextLongPairWritable.PairComparator.compare','org.apache.hadoop.io.WritableComparable.compareTo'
'com.twitter.elephanttwin.io.TextLongPairWritable.Parititioner.getPartition','org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.Text,java.lang.Object>.<init> org.apache.hadoop.mapreduce.lib.partition.HashPartitioner<org.apache.hadoop.io.Text,java.lang.Object>.getPartition'
'com.twitter.elephanttwin.io.TextLongPairWritable.hashCode','org.apache.hadoop.io.Text.hashCode'
'com.twitter.elephanttwin.io.TextLongPairWritable.equals','org.apache.hadoop.io.Text.equals'
'org.apache.gora.tutorial.log.TextLong.TextLong','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init>'
'org.apache.crunch.types.writable.TextMapWritable.readFields','org.apache.hadoop.io.Text.readString org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields'
'org.apache.crunch.types.writable.TextMapWritable.write','org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.WritableUtils.writeVInt'
'edu.jhu.thrax.hadoop.comparators.TextMarginalComparator.compare','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize'
'edu.umd.cloud9.example.hits.TextOutput.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.example.hits.TextOutput.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobClient.runJob'
'edu.umd.cloud9.example.hits.TextOutput.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'uk.bl.wap.hadoop.TextOutputFormat.LineRecordWriter.writeObject','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'uk.bl.wap.hadoop.TextOutputFormat.getRecordWriter','org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'org.apache.hama.bsp.TextOutputFormat.LineRecordWriter.writeObject','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.hama.bsp.TextOutputFormat.getRecordWriter','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.compress.CompressionCodec.createOutputStream'
'com.datasalt.utils.io.TextPairWritable.TextPairWritable','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.datasalt.utils.io.TextPairWritable.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields'
'com.datasalt.utils.io.TextPairWritable.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write'
'com.datasalt.utils.io.TextPairWritable.compareTo','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo'
'com.datasalt.utils.io.TextPairWritable.Comparator.compare','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize'
'com.datasalt.utils.io.TextPairWritable.FirstStringComparator.compare','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize'
'org.chombo.util.TextPair.TextPair','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.chombo.util.TextPair.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields'
'org.chombo.util.TextPair.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write'
'org.chombo.util.TextPair.compareTo','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo'
'org.chombo.util.TextPair.baseCompareTo','org.apache.hadoop.io.Text.compareTo'
'org.chombo.util.TextPair.hashCode','org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.Text.hashCode'
'org.chombo.util.TextPair.baseHashCode','org.apache.hadoop.io.Text.hashCode'
'org.chombo.util.TextPair.equals','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals'
'.TextPair.TextPair','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'.TextPair.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write'
'.TextPair.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields'
'.TextPair.hashCode','org.apache.hadoop.io.Text.hashCode org.apache.hadoop.io.Text.hashCode'
'.TextPair.equals','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.equals'
'.TextPair.compareTo','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.compareTo'
'.TextPair.Comparator.compare','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize'
'.TextPair.FirstComparator.compare','org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.WritableUtils.decodeVIntSize org.apache.hadoop.io.Text.compareTo'
'org.apache.mahout.text.TextParagraphSplittingJob.run','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.text.TextParagraphSplittingJob.SplitMap.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.set'
'org.apache.mahout.text.TextParagraphSplittingJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.text.TextParagraphSplittingJob.run','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.text.TextParagraphSplittingJob.SplitMap.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.set'
'org.apache.mahout.text.TextParagraphSplittingJob.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.parse.text.TextParser.setConf','org.apache.hadoop.conf.Configuration.get'
'org.springframework.data.hadoop.fs.TextRecordInputStream.TextRecordInputStream','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init>'
'org.springframework.data.hadoop.fs.TextRecordInputStream.read','org.apache.hadoop.io.DataInputBuffer.read org.apache.hadoop.io.WritableComparable.toString org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.Writable.toString org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.DataInputBuffer.read'
'com.cloudera.recordbreaker.analyzer.TextRegexpSchemaDescriptor.getIterator','org.apache.hadoop.fs.FileSystem.open'
'org.childtv.hadoop.hbase.mapred.TextTableOutputFormat.TextTableOutputFormat','org.apache.hadoop.hbase.mapred.TableOutputFormat.<init>'
'org.childtv.hadoop.hbase.mapred.TextTableOutputFormat.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.get'
'org.childtv.hadoop.hbase.mapred.TextTableOutputFormat.decodeColumnName','org.apache.hadoop.hbase.util.Base64.decode'
'org.childtv.hadoop.hbase.mapred.TextTableOutputFormat.decodeValue','org.apache.hadoop.hbase.util.Base64.decode'
'org.childtv.hadoop.hbase.mapred.TextTableOutputFormat.checkOutputSpecs','org.apache.hadoop.hbase.mapred.TableOutputFormat.checkOutputSpecs'
'org.childtv.hadoop.hbase.mapred.TextTableOutputFormat.getRecordWriter','org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter'
'org.childtv.hadoop.hbase.mapred.TextTableOutputFormat.TextTableRecordWriter.close','org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.io.BatchUpdate>.close'
'org.childtv.hadoop.hbase.mapred.TextTableOutputFormat.TextTableRecordWriter.write','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.io.BatchUpdate>.write'
'com.manning.hip.ch4.joins.contribjoin.TextTaggedMapOutput.TextTaggedMapOutput','org.apache.hadoop.io.Text.<init>'
'com.manning.hip.ch4.joins.contribjoin.TextTaggedMapOutput.write','org.apache.hadoop.io.Text.write'
'com.manning.hip.ch4.joins.contribjoin.TextTaggedMapOutput.readFields','org.apache.hadoop.io.Text.readFields'
'org.pentaho.hadoop.mapreduce.converter.converters.TextToIntegerConverter.canConvert','org.apache.hadoop.io.Text.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.TextToIntegerConverter.convert','org.apache.hadoop.io.Text.toString'
'org.pentaho.hadoop.mapreduce.converter.converters.TextToStringConverter.canConvert','org.apache.hadoop.io.Text.equals'
'org.pentaho.hadoop.mapreduce.converter.converters.TextToStringConverter.convert','org.apache.hadoop.io.Text.toString'
'org.apache.accumulo.core.util.TextUtil.getBytes','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes'
'org.apache.accumulo.core.util.TextUtil.getByteBuffer','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.accumulo.core.util.TextUtil.truncate','org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'org.apache.giraph.io.TextVertexOutputFormat.checkOutputSpecs','org.apache.hadoop.mapreduce.lib.output.TextOutputFormat<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.checkOutputSpecs'
'org.apache.giraph.io.TextVertexOutputFormat.getOutputCommitter','org.apache.hadoop.mapreduce.lib.output.TextOutputFormat<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.getOutputCommitter'
'org.apache.giraph.io.TextVertexOutputFormat.TextVertexWriter.createLineRecordWriter','org.apache.hadoop.mapreduce.lib.output.TextOutputFormat<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.getRecordWriter'
'org.apache.giraph.io.TextVertexOutputFormat.TextVertexWriter.close','org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.close'
'pl.edu.icm.coansys.classification.documents.jobs.TfidfJob_Proto.run','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs'
'pl.edu.icm.coansys.classification.documents.jobs.TfidfJob_Proto.firstJobExecution','org.apache.hadoop.conf.Configuration.clear org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.setCaching org.apache.hadoop.hbase.client.Scan.setCacheBlocks org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'pl.edu.icm.coansys.classification.documents.jobs.TfidfJob_Proto.calculateDocsNum','org.apache.hadoop.mapreduce.Job.getCounters'
'pl.edu.icm.coansys.classification.documents.jobs.TfidfJob_Proto.secondJobExecution','org.apache.hadoop.conf.Configuration.clear org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'pl.edu.icm.coansys.classification.documents.jobs.TfidfJob_Proto.thirdJobExecution','org.apache.hadoop.conf.Configuration.clear org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'pl.edu.icm.coansys.classification.documents.jobs.TfidfJob_Proto.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'pl.edu.icm.coansys.classification.documents.jobs.TfidfMapper.map','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.classifier.naivebayes.training.ThetaMapper.setup','org.apache.hadoop.conf.Configuration.getFloat org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.classifier.naivebayes.training.ThetaMapper.map','org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.classifier.naivebayes.training.ThetaMapper.cleanup','org.apache.hadoop.io.Text.<init>'
'org.apache.jena.tdbloader4.ThirdMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.clear'
'edu.jhu.thrax.hadoop.jobs.ThraxJob.getJob','org.apache.hadoop.mapreduce.Job.<init>'
'com.urbanairship.statshtable.ThreadCountTest.threadCountTest','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTableInterface.close org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.close'
'com.nearinfinity.blur.thrift.ThriftBlurControllerServer.createServer','org.apache.hadoop.conf.Configuration.<init>'
'thrift.ThriftExample.main','org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.<init> org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.<init> org.apache.hadoop.hbase.thrift.generated.Mutation.<init>'
'org.honu.datacollection.collector.streaming.ThriftJettyCollector.main','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.datasalt.pangool.serialization.ThriftSerialization.TDeserializerAdapter.deserialize','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.datasalt.pangool.serialization.ThriftSerialization.enableThriftSerialization','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'backtype.hadoop.ThriftSerializer.serialize','org.apache.hadoop.io.WritableUtils.writeVInt'
'com.asakusafw.dmdl.thundergate.emitter.ThunderGateModelEmitterTest.table','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.asakusafw.dmdl.thundergate.emitter.ThunderGateModelEmitterTest.join','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'com.digitalpebble.behemoth.tika.TikaDriver.main','org.apache.hadoop.util.ToolRunner.run'
'com.digitalpebble.behemoth.tika.TikaDriver.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJarByClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'uk.bl.wap.util.solr.TikaExtractor.TikaExtractor','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.getLong'
'com.digitalpebble.behemoth.tika.TikaMapper.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,com.digitalpebble.behemoth.BehemothDocument>.collect'
'com.digitalpebble.behemoth.tika.TikaMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getClass'
'org.apache.nutch.parse.tika.TikaParser.setConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getResource org.apache.hadoop.conf.Configuration.get'
'com.mycompany.hiaex.TimeSeries.Mapper1.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'com.mycompany.hiaex.TimeSeries.Reducer1.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init>'
'com.mycompany.hiaex.TimeSeries.Partitioner1.getPartition','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.conf.Configuration.getInt'
'com.mycompany.hiaex.TimeSeries.run','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.mycompany.hiaex.TimeSeries.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.hadoop.TimesSquaredJob.createTimesSquaredJobConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.setCacheFiles org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.apache.mahout.math.hadoop.TimesSquaredJob.createTimesJobConf','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.TimesSquaredJob.retrieveTimesSquaredOutputVector','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.TimesSquaredJob.TimesSquaredMapper.configure','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.mahout.math.hadoop.TimesSquaredJob.TimesSquaredMapper.close','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.NullWritable,org.apache.mahout.math.VectorWritable>.collect'
'org.apache.mahout.math.hadoop.TimesSquaredJob.TimesMapper.map','org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.math.hadoop.TimesSquaredJob.VectorSummingReducer.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.mahout.math.hadoop.TimesSquaredJob.VectorSummingReducer.reduce','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.NullWritable,org.apache.mahout.math.VectorWritable>.collect'
'org.apache.mahout.math.hadoop.TimesSquaredJob.createTimesSquaredJobConf','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.NullWritable.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.setCacheFiles org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setInt org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass'
'org.apache.mahout.math.hadoop.TimesSquaredJob.createTimesJobConf','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.mahout.math.hadoop.TimesSquaredJob.retrieveTimesSquaredOutputVector','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.math.hadoop.TimesSquaredJob.TimesSquaredMapper.configure','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.mahout.math.hadoop.TimesSquaredJob.TimesSquaredMapper.close','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.NullWritable,org.apache.mahout.math.VectorWritable>.collect'
'org.apache.mahout.math.hadoop.TimesSquaredJob.TimesMapper.map','org.apache.hadoop.io.IntWritable.get'
'org.apache.mahout.math.hadoop.TimesSquaredJob.VectorSummingReducer.configure','org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.mahout.math.hadoop.TimesSquaredJob.VectorSummingReducer.reduce','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.NullWritable,org.apache.mahout.math.VectorWritable>.collect'
'filters.TimestampFilterExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.filter.TimestampsFilter.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.Scan.setTimeRange org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.close'
'org.apache.crunch.io.To.formattedFile','org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.To.avroFile','org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.To.sequenceFile','org.apache.hadoop.fs.Path.<init>'
'org.apache.crunch.io.To.textFile','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.cf.taste.hadoop.ToEntityPrefsMapper.setup','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.cf.taste.hadoop.ToEntityPrefsMapper.map','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.cf.taste.hadoop.ToEntityPrefsMapper.setup','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.cf.taste.hadoop.ToEntityPrefsMapper.map','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.cf.taste.hadoop.preparation.ToItemVectorsMapper.map','org.apache.hadoop.io.IntWritable.<init>'
'org.apache.sqoop.hbase.ToStringPutTransformer.getFieldNameBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.sqoop.hbase.ToStringPutTransformer.getPutCommand','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'gov.llnl.ontology.mapreduce.stats.TokenCountMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.TokenCountMR.setupReducer','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'gov.llnl.ontology.mapreduce.stats.TokenCountMR.TokenCountMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'edu.isi.mavuno.util.TokenWritable.TokenWritable','org.apache.hadoop.io.Text.clear'
'edu.isi.mavuno.util.TokenWritable.safeSet','org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.set'
'edu.isi.mavuno.util.TokenWritable.readFields','org.apache.hadoop.io.Text.readFields'
'edu.isi.mavuno.util.TokenWritable.write','org.apache.hadoop.io.Text.write'
'edu.isi.mavuno.util.TokenWritable.toString','org.apache.hadoop.io.Text.toString'
'cc.TokeniseSentences.main','org.apache.hadoop.util.ToolRunner.run'
'cc.TokeniseSentences.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMaxMapTaskFailuresPercent org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'cc.TokeniseSentences.TokeniseSentencesMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter org.apache.hadoop.mapred.Reporter.getCounter'
'com.mozilla.grouperfish.transforms.coclustering.pig.eval.text.Tokenize.loadDictionary','org.apache.hadoop.fs.Path.<init>'
'org.sleuthkit.hadoop.TokenizeAndVectorizeDocuments.runPipeline','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.mozilla.grouperfish.pig.eval.text.Tokenize.loadDictionary','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.ga.watchmaker.cd.tool.ToolCombiner.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.<init>'
'org.springframework.data.hadoop.mapreduce.ToolExecutor.invokeTargetObject','org.apache.hadoop.util.ToolRunner.run'
'eu.scape_project.pt.pit.ToolSpecRepository.ToolSpecRepository','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.toString'
'eu.scape_project.pt.pit.ToolSpecRepository.toolspecExists','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'eu.scape_project.pt.pit.ToolSpecRepository.getToolSpec','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'eu.scape_project.pt.pit.ToolSpecRepository.getToolSpecList','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.springframework.data.hadoop.mapreduce.ToolTests.testToolArgs','org.apache.hadoop.conf.Configuration.get'
'eu.scape_project.pt.executors.ToolspecExecutor.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.Mapper.Context.getTaskAttemptID org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.datasalt.pangool.flow.TopCountryBySimilarsFlow.reduce','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.NullWritable.get'
'org.sleuthkit.hadoop.TopFeatureMapper.setup','org.apache.hadoop.fs.FileSystem.get'
'org.sleuthkit.hadoop.TopFeatureMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.fpm.pfpgrowth.convertors.TopKPatternsOutputConverter.collect','org.apache.hadoop.mapred.OutputCollector<org.apache.mahout.fpm.pfpgrowth.convertors.A,java.util.List<org.apache.mahout.common.Pair<java.util.List<org.apache.mahout.fpm.pfpgrowth.convertors.A>,java.lang.Long>>>.collect'
'.TopSchoolsDriver.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass org.apache.hadoop.mapred.JobConf.setPartitionerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'.TopSchoolsDriver.main','org.apache.hadoop.util.ToolRunner.run'
'.TopSchoolsMapper.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'.TopSchoolsMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'.TopSchoolsOutputKeyComparator.compare','org.apache.hadoop.io.WritableComparable.toString org.apache.hadoop.io.WritableComparable.toString'
'.TopSchoolsOutputValueGroupingComparator.compare','org.apache.hadoop.io.WritableComparable.toString org.apache.hadoop.io.WritableComparable.toString'
'.TopSchoolsReducer.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'.TopSchoolsReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'com.datasalt.pangool.examples.topicalwordcount.TopicFingerprint.TopNWords.reduce','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.examples.topicalwordcount.TopicFingerprint.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.examples.topicalwordcount.TopicFingerprint.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.clustering.lda.cvb.TopicModel.persist','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.delete'
'org.apache.crunch.examples.TotalBytesByIP.run','org.apache.hadoop.util.GenericOptionsParser.printGenericCommandUsage'
'org.apache.crunch.examples.TotalBytesByIP.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.jena.tdbloader4.partitioners.TotalOrderPartitioner.init','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getMapOutputKeyClass org.apache.hadoop.mapreduce.Job.getNumReduceTasks org.apache.hadoop.mapreduce.Job.getSortComparator org.apache.hadoop.io.RawComparator<org.apache.jena.tdbloader4.partitioners.K>.compare org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.io.BinaryComparable.isAssignableFrom org.apache.hadoop.conf.Configuration.getInt'
'org.apache.jena.tdbloader4.partitioners.TotalOrderPartitioner.setPartitionFile','org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.jena.tdbloader4.partitioners.TotalOrderPartitioner.getPartitionFile','org.apache.hadoop.conf.Configuration.get'
'org.apache.jena.tdbloader4.partitioners.TotalOrderPartitioner.InnerTrieNode.findPartition','org.apache.hadoop.io.BinaryComparable.getLength org.apache.hadoop.io.BinaryComparable.getBytes'
'org.apache.jena.tdbloader4.partitioners.TotalOrderPartitioner.SinglySplitTrieNode.findPartition','org.apache.hadoop.io.BinaryComparable.compareTo'
'org.apache.jena.tdbloader4.partitioners.TotalOrderPartitioner.readPartitions','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.NullWritable.get org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.jena.tdbloader4.partitioners.TotalOrderPartitioner.buildTrieRec','org.apache.hadoop.io.BinaryComparable.compareTo'
'.TotalSortByKeyOutputKeyComparator.compare','org.apache.hadoop.io.WritableComparable.toString org.apache.hadoop.io.WritableComparable.toString'
'.TotalSortByKeyReducer.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'org.apache.accumulo.server.trace.TraceFileSystem.open','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.open'
'org.apache.accumulo.server.trace.TraceFileSystem.create','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.create'
'org.apache.accumulo.server.trace.TraceFileSystem.createNewFile','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.append','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.append'
'org.apache.accumulo.server.trace.TraceFileSystem.getReplication','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.setReplication','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.exists','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.isDirectory','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.isFile','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.getLength','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.getContentSummary','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.listStatus','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.listStatus'
'org.apache.accumulo.server.trace.TraceFileSystem.globStatus','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.mkdirs','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.mkdirs'
'org.apache.accumulo.server.trace.TraceFileSystem.copyFromLocalFile','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.moveFromLocalFile','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.copyToLocalFile','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.moveToLocalFile','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.startLocalOutput','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.completeLocalOutput','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.getBlockSize','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.getFileChecksum','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.setPermission','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.setOwner','org.apache.hadoop.fs.Path.toString'
'org.apache.accumulo.server.trace.TraceFileSystem.getUri','org.apache.hadoop.fs.FileSystem.getUri'
'org.apache.accumulo.server.trace.TraceFileSystem.initialize','org.apache.hadoop.fs.FileSystem.initialize'
'org.apache.accumulo.server.trace.TraceFileSystem.rename','org.apache.hadoop.fs.FileSystem.rename'
'org.apache.accumulo.server.trace.TraceFileSystem.delete','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'org.apache.accumulo.server.trace.TraceFileSystem.setWorkingDirectory','org.apache.hadoop.fs.FileSystem.setWorkingDirectory'
'org.apache.accumulo.server.trace.TraceFileSystem.getWorkingDirectory','org.apache.hadoop.fs.FileSystem.getWorkingDirectory'
'org.apache.accumulo.server.trace.TraceFileSystem.getFileStatus','org.apache.hadoop.fs.FileSystem.getFileStatus'
'org.apache.accumulo.server.trace.TraceFileSystem.getAndWrap','org.apache.hadoop.fs.FileSystem.get'
'org.apache.accumulo.server.trace.TraceServer.put','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.server.trace.TraceServer.Receiver.span','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.cassandra.hadoop.trackers.TrackerInitializer.stopTaskTracker','org.apache.hadoop.metrics2.util.MBeans.unregister org.apache.hadoop.mapred.TaskTracker.shutdown'
'org.apache.cassandra.hadoop.trackers.TrackerInitializer.run','org.apache.hadoop.mapred.JobTracker.startTracker org.apache.hadoop.mapred.JobTracker.offerService org.apache.hadoop.mapred.JobTracker.stopTracker org.apache.hadoop.mapred.JobTracker.stopTracker org.apache.hadoop.mapred.TaskTracker.<init> org.apache.hadoop.metrics2.util.MBeans.register org.apache.hadoop.mapred.TaskTracker.run org.apache.hadoop.mapred.TaskTracker.shutdown'
'gov.llnl.ontology.mains.TrainLogisticRegression.main','org.apache.hadoop.hbase.client.Scan.<init>'
'com.yahoo.omid.tso.TransactionClient.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'com.yahoo.omid.client.TransactionManager.cleanup','org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.delete'
'org.apache.mahout.fpm.pfpgrowth.TransactionSortingMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.<init>'
'org.apache.mahout.fpm.pfpgrowth.TransactionTree.readFields','org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VLongWritable.<init> org.apache.hadoop.io.VIntWritable.readFields org.apache.hadoop.io.VIntWritable.get org.apache.hadoop.io.VLongWritable.readFields org.apache.hadoop.io.VLongWritable.get org.apache.hadoop.io.VIntWritable.readFields org.apache.hadoop.io.VIntWritable.get org.apache.hadoop.io.VIntWritable.readFields org.apache.hadoop.io.VIntWritable.get org.apache.hadoop.io.VIntWritable.readFields org.apache.hadoop.io.VIntWritable.get org.apache.hadoop.io.VIntWritable.readFields org.apache.hadoop.io.VIntWritable.get org.apache.hadoop.io.VLongWritable.readFields org.apache.hadoop.io.VLongWritable.get org.apache.hadoop.io.VIntWritable.readFields org.apache.hadoop.io.VIntWritable.get org.apache.hadoop.io.VIntWritable.readFields org.apache.hadoop.io.VIntWritable.get'
'org.apache.mahout.fpm.pfpgrowth.TransactionTree.write','org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VLongWritable.<init> org.apache.hadoop.io.VIntWritable.set org.apache.hadoop.io.VIntWritable.write org.apache.hadoop.io.VLongWritable.set org.apache.hadoop.io.VLongWritable.write org.apache.hadoop.io.VIntWritable.set org.apache.hadoop.io.VIntWritable.write org.apache.hadoop.io.VIntWritable.set org.apache.hadoop.io.VIntWritable.write org.apache.hadoop.io.VIntWritable.set org.apache.hadoop.io.VIntWritable.write org.apache.hadoop.io.VIntWritable.set org.apache.hadoop.io.VIntWritable.write org.apache.hadoop.io.VLongWritable.set org.apache.hadoop.io.VLongWritable.write org.apache.hadoop.io.VIntWritable.set org.apache.hadoop.io.VIntWritable.write org.apache.hadoop.io.VIntWritable.set org.apache.hadoop.io.VIntWritable.write'
'com.yahoo.omid.client.TransactionalTable.TransactionalTable','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.yahoo.omid.client.TransactionalTable.get','org.apache.hadoop.hbase.client.Get.getRow org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.getTimeRange org.apache.hadoop.hbase.io.TimeRange.getMin org.apache.hadoop.hbase.io.TimeRange.getMax org.apache.hadoop.hbase.client.Get.setTimeRange org.apache.hadoop.hbase.client.Get.getFamilyMap org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Result.<init>'
'com.yahoo.omid.client.TransactionalTable.delete','org.apache.hadoop.hbase.client.Delete.getRow org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Delete.getRow org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Delete.getFamilyMap org.apache.hadoop.hbase.KeyValue.getType org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.client.Get.addFamily org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Result.getMap org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Delete.getRow org.apache.hadoop.hbase.client.Put.getFamilyMap'
'com.yahoo.omid.client.TransactionalTable.put','org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.getFamilyMap org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.KeyValue.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.getRow org.apache.hadoop.hbase.client.Put.getFamilyMap'
'com.yahoo.omid.client.TransactionalTable.getScanner','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setMaxVersions org.apache.hadoop.hbase.client.Scan.setTimeRange'
'com.yahoo.omid.client.TransactionalTable.filter','org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.KeyValue.getRow org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.KeyValue.getFamily org.apache.hadoop.hbase.KeyValue.getQualifier org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.setMaxVersions org.apache.hadoop.hbase.client.Get.setTimeRange org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.KeyValue.getValueLength org.apache.hadoop.hbase.KeyValue.getTimestamp org.apache.hadoop.hbase.client.Result.list'
'com.yahoo.omid.client.TransactionalTable.ClientScanner.next','org.apache.hadoop.hbase.client.Result.list org.apache.hadoop.hbase.client.Result.<init>'
'com.nexr.rhive.util.TransformUtils.tranform','org.apache.hadoop.fs.FileStatus.getPermission org.apache.hadoop.fs.FileStatus.getOwner org.apache.hadoop.fs.FileStatus.getGroup org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getModificationTime org.apache.hadoop.fs.FileStatus.getPath'
'edu.isi.mavuno.app.nlp.TratzParse.MyMapper.map','org.apache.hadoop.io.Text.set'
'edu.isi.mavuno.app.nlp.TratzParse.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter'
'edu.isi.mavuno.app.nlp.TratzParse.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.isi.mavuno.util.TratzParsedTokenWritable.TratzParsedTokenWritable','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.clear org.apache.hadoop.io.IntWritable.set'
'edu.isi.mavuno.util.TratzParsedTokenWritable.setCharOffset','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.io.IntWritable.set'
'edu.isi.mavuno.util.TratzParsedTokenWritable.getCharOffsetBegin','org.apache.hadoop.io.IntWritable.get'
'edu.isi.mavuno.util.TratzParsedTokenWritable.getCharOffsetEnd','org.apache.hadoop.io.IntWritable.get'
'edu.isi.mavuno.util.TratzParsedTokenWritable.setDependIndex','org.apache.hadoop.io.IntWritable.set'
'edu.isi.mavuno.util.TratzParsedTokenWritable.getDependIndex','org.apache.hadoop.io.IntWritable.get'
'edu.isi.mavuno.util.TratzParsedTokenWritable.readFields','org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.IntWritable.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.IntWritable.readFields'
'edu.isi.mavuno.util.TratzParsedTokenWritable.write','org.apache.hadoop.io.IntWritable.write org.apache.hadoop.io.IntWritable.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.Text.write org.apache.hadoop.io.IntWritable.write'
'com.mongodb.hadoop.examples.treasury.TreasuryYieldReducer.reduce','org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig.main','org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.trec.TrecDocnoMappingBuilder.MyMapper.map','org.apache.hadoop.io.Text.set'
'edu.umd.cloud9.collection.trec.TrecDocnoMappingBuilder.MyReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'edu.umd.cloud9.collection.trec.TrecDocnoMappingBuilder.build','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'edu.umd.cloud9.collection.trec.TrecDocnoMappingBuilder.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'edu.umd.cloud9.collection.trec.TrecDocnoMappingBuilder.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.trec.TrecDocumentInputFormatOld.TrecDocumentRecordReader.TrecDocumentRecordReader','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'edu.umd.cloud9.collection.trec.TrecDocumentInputFormatOld.TrecDocumentRecordReader.next','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.toString'
'edu.umd.cloud9.collection.trec.TrecDocumentInputFormatOld.TrecDocumentRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'edu.umd.cloud9.webgraph.driver.TrecDriver.run','org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.conf.Configuration.setInt'
'edu.umd.cloud9.webgraph.driver.TrecDriver.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.webgraph.driver.TrecDriver.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'edu.umd.cloud9.webgraph.driver.TrecDriver.readInput','org.apache.hadoop.conf.Configuration.set'
'edu.umd.cloud9.webgraph.TrecExtractLinks.Map.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get'
'edu.umd.cloud9.webgraph.TrecExtractLinks.Map.map','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set'
'edu.umd.cloud9.webgraph.TrecExtractLinks.runTool','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.webgraph.TrecExtractLinks.recursivelyAddInputPaths','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath'
'edu.umd.cloud9.collection.trec.TrecForwardIndex.getDocument','org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.read'
'edu.umd.cloud9.collection.trec.TrecForwardIndex.loadIndex','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'edu.umd.cloud9.collection.trec.TrecForwardIndex.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'edu.isi.mavuno.input.TrecInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'edu.isi.mavuno.input.TrecInputFormat.TrecDocumentRecordReader.TrecDocumentRecordReader','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'edu.isi.mavuno.input.TrecInputFormat.TrecDocumentRecordReader.nextKeyValue','org.apache.hadoop.io.LongWritable.set'
'edu.umd.cloud9.collection.trecweb.TrecWebDocumentInputFormat.TrecWebDocumentRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'edu.umd.cloud9.collection.trecweb.TrecWebDocumentInputFormatOld.TrecWebRecordReader.TrecWebRecordReader','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'edu.umd.cloud9.collection.trecweb.TrecWebDocumentInputFormatOld.TrecWebRecordReader.next','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.toString'
'edu.umd.cloud9.collection.trecweb.TrecWebDocumentInputFormatOld.TrecWebRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'hadooptree.TreeBuilder.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete'
'hadooptree.TreeBuilder.writeTree','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'hadooptree.TreeBuilder.growSubtreesJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'hadooptree.TreeBuilder.filterInstancesJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'hadooptree.TreeBuilder.findBestFieldSplitJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'hadooptree.TreeBuilder.findBestCategorySplitJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'hadooptree.TreeBuilder.setupDefineFieldsJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'hadooptree.TreeBuilder.readSubtrees','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataInputStream.close'
'hadooptree.TreeBuilder.readFieldDefinitions','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataInputStream.close'
'hadooptree.TreeBuilder.readNewSplits','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataInputStream.close'
'fr.insarennes.fafdti.builder.treebuilder.TreeBuilderRecursive.TreeBuilderRecursive','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'fr.insarennes.fafdti.builder.treebuilder.TreeBuilderRecursive.initNodeBuilder','org.apache.hadoop.fs.Path.<init>'
'fr.insarennes.fafdti.builder.treebuilder.TreeBuilderRecursive.nodeMaker','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.avro.tool.TrevniUtil.input','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.avro.tool.TrevniUtil.output','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.createTable','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HConnectionManager.getConnection org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.setupScan','org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.filter.SingleColumnValueFilter.<init> org.apache.hadoop.hbase.client.Scan.setFilter'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.iterator','org.apache.hadoop.hbase.client.HTable.getScanner'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.table','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init>'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.put','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.HTable.put'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.putSenses','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init>'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.putCategories','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init>'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.putLabel','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init>'
'gov.llnl.ontology.mapreduce.table.TrinidadTable.close','org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.close'
'com.asakusafw.runtime.io.TsvParser.consumeEncoded','org.apache.hadoop.io.Text.append'
'structures.TupleComparator.compare','org.apache.hadoop.io.WritableComparable.compareTo'
'step3.TupleGroupPartitioner.getPartition','org.apache.hadoop.io.IntWritable.get'
'com.datasalt.pangool.tuplemr.mapred.lib.input.TupleInputFormat.TupleInputReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.tuplemr.mapred.lib.input.TupleInputFormat.TupleInputReader.initialize','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.InputSplit.getLength'
'com.datasalt.pangool.tuplemr.mapred.lib.input.TupleInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.talis.hbase.rdf.layout.hash.TupleLoaderHash.loadTuple','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut'
'com.talis.hbase.rdf.layout.hash.TupleLoaderHash.addNodeToNodesTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut'
'com.talis.hbase.rdf.layout.hash.TupleLoaderHash.unloadTuple','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete'
'com.talis.hbase.rdf.layout.hybrid.TupleLoaderHybrid.loadTuple','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut'
'com.talis.hbase.rdf.layout.hybrid.TupleLoaderHybrid.unloadTuple','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete'
'com.talis.hbase.rdf.layout.indexed.TupleLoaderIndexed.loadTuple','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.checkAndPut'
'com.talis.hbase.rdf.layout.indexed.TupleLoaderIndexed.unloadTuple','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete'
'com.talis.hbase.rdf.layout.vpindexed.TupleLoaderVPIndexed.loadTuple','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut'
'com.talis.hbase.rdf.layout.vpindexed.TupleLoaderVPIndexed.unloadTuple','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.<init> org.apache.hadoop.hbase.client.Delete.deleteColumn org.apache.hadoop.hbase.client.HTable.delete'
'com.datasalt.pangool.tuplemr.TupleMRBuilder.createJob','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setPartitionerClass org.apache.hadoop.mapreduce.Job.setGroupingComparatorClass org.apache.hadoop.mapreduce.Job.setSortComparatorClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setOutputFormatClass'
'com.datasalt.pangool.tuplemr.TupleMRConfigBuilder.initializeComparators','org.apache.hadoop.conf.Configurable.setConf'
'com.datasalt.pangool.tuplemr.serialization.TupleOfTupleOfTuples.MyHandler.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.datasalt.pangool.tuplemr.serialization.TupleOfTupleOfTuples.test','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'com.datasalt.pangool.tuplemr.TupleReducer.StaticCollector.write','org.apache.hadoop.mapreduce.ReduceContext<com.datasalt.pangool.io.DatumWrapper<com.datasalt.pangool.io.ITuple>,org.apache.hadoop.io.NullWritable,com.datasalt.pangool.tuplemr.CONTEXT_OUTPUT_KEY,com.datasalt.pangool.tuplemr.CONTEXT_OUTPUT_VALUE>.write'
'com.datasalt.pangool.tuplemr.TupleReducer.CombinerCollector.CombinerCollector','org.apache.hadoop.io.NullWritable.get'
'cascading.tuple.hadoop.TupleSerialization.getSerializationTokens','org.apache.hadoop.conf.Configuration.get'
'cascading.tuple.hadoop.TupleSerialization.setSerializations','org.apache.hadoop.io.serializer.WritableSerialization.getName org.apache.hadoop.mapred.JobConf.set'
'cascading.tuple.hadoop.TupleSerialization.getSerializations','org.apache.hadoop.conf.Configuration.get'
'cascading.tuple.hadoop.TupleSerialization.getDefaultComparator','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.util.ReflectionUtils.newInstance'
'cascading.tuple.hadoop.TupleSerialization.TupleSerialization','org.apache.hadoop.conf.Configuration.<init>'
'cascading.tuple.hadoop.TupleSerialization.getConf','org.apache.hadoop.mapred.JobConf.<init>'
'cascading.tuple.hadoop.TupleSerialization.getSerializationFactory','org.apache.hadoop.io.serializer.SerializationFactory.<init>'
'cascading.tuple.hadoop.TupleSerialization.initTokenMaps','org.apache.hadoop.conf.Configuration.getClassByName'
'cascading.tuple.hadoop.TupleSerialization.getSerialization','org.apache.hadoop.io.serializer.SerializationFactory.getSerialization'
'cascading.tuple.hadoop.TupleSerialization.getNewSerializer','org.apache.hadoop.io.serializer.SerializationFactory.getSerializer org.apache.hadoop.io.serializer.SerializationFactory.getClass org.apache.hadoop.io.serializer.SerializationFactory.getClass'
'cascading.tuple.hadoop.TupleSerialization.getNewDeserializer','org.apache.hadoop.io.serializer.SerializationFactory.getDeserializer org.apache.hadoop.io.serializer.SerializationFactory.getClass org.apache.hadoop.io.serializer.SerializationFactory.getClass'
'cascading.tuple.hadoop.TupleSerialization.SerializationElementReader.read','org.apache.hadoop.io.serializer.Deserializer.deserialize'
'cascading.tuple.hadoop.TupleSerialization.SerializationElementReader.getDeserializerFor','org.apache.hadoop.io.serializer.Deserializer.open'
'cascading.tuple.hadoop.TupleSerialization.SerializationElementReader.getClassNameFor','org.apache.hadoop.io.WritableUtils.readString'
'cascading.tuple.hadoop.TupleSerialization.SerializationElementReader.close','org.apache.hadoop.io.serializer.Deserializer.close'
'cascading.tuple.hadoop.TupleSerialization.SerializationElementWriter.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeString org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.serializer.Serializer.open org.apache.hadoop.io.serializer.Serializer.serialize'
'cascading.tuple.hadoop.TupleSerialization.SerializationElementWriter.close','org.apache.hadoop.io.serializer.Serializer.close'
'com.datasalt.pangool.tuplemr.serialization.TupleSerializer.multipleSourcesSerialization','org.apache.hadoop.io.WritableUtils.writeVInt'
'com.datasalt.pangool.solr.TupleSolrOutputFormatExample.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'com.datasalt.pangool.solr.TupleSolrOutputFormatExample.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.NullWritable.get'
'edu.umd.cloud9.io.TupleTest.testSerializeWritableFields','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init>'
'com.datasalt.pangool.tuplemr.mapred.lib.input.TupleTextInputFormat.TupleTextInputReader.getCurrentValue','org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.tuplemr.mapred.lib.input.TupleTextInputFormat.TupleTextInputReader.initialize','org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.InputSplit.getLength'
'com.datasalt.pangool.tuplemr.mapred.lib.input.TupleTextInputFormat.TupleTextInputReader.init','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open'
'com.datasalt.pangool.examples.TupleViewer.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.crunch.types.writable.TupleWritable.equals','org.apache.hadoop.io.Writable.equals'
'org.apache.crunch.types.writable.TupleWritable.toString','org.apache.hadoop.io.Writable.toString'
'org.apache.crunch.types.writable.TupleWritable.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.WritableUtils.writeVLong org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.Text.writeString org.apache.hadoop.io.Writable.write'
'org.apache.crunch.types.writable.TupleWritable.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.Text.readString org.apache.hadoop.io.Writable.readFields'
'org.apache.crunch.types.writable.TupleWritable.compareTo','org.apache.hadoop.io.Writable.equals org.apache.hadoop.io.WritableComparable.compareTo org.apache.hadoop.io.Writable.hashCode org.apache.hadoop.io.Writable.hashCode'
'edu.umd.cloud9.io.Tuple.readFields','org.apache.hadoop.io.Writable.readFields'
'edu.umd.cloud9.io.Tuple.write','org.apache.hadoop.io.Writable.write'
'edu.umd.cloud9.io.Tuple.readFields','org.apache.hadoop.io.Writable.readFields'
'edu.umd.cloud9.io.Tuple.write','org.apache.hadoop.io.Writable.write'
'edu.isi.mavuno.extract.TwitterGeoTemporalExtractor.setDocument','org.apache.hadoop.io.Text.set'
'edu.isi.mavuno.extract.TwitterGeoTemporalExtractor.getTerms','org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.extract.TwitterGeoTemporalExtractor.getPattern','org.apache.hadoop.io.Text.clear org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.append org.apache.hadoop.io.Text.append'
'edu.isi.mavuno.extract.TwitterGeoTemporalExtractor.calToText','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.isi.mavuno.input.TwitterInputFormat.TwitterRecordReader.TwitterRecordReader','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.<init>'
'edu.isi.mavuno.input.TwitterInputFormat.TwitterRecordReader.close','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close'
'edu.isi.mavuno.input.TwitterInputFormat.TwitterRecordReader.getProgress','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getProgress'
'edu.isi.mavuno.input.TwitterInputFormat.TwitterRecordReader.initialize','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize'
'edu.isi.mavuno.input.TwitterInputFormat.TwitterRecordReader.nextKeyValue','org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentKey org.apache.hadoop.io.LongWritable.set org.apache.hadoop.mapreduce.lib.input.LineRecordReader.getCurrentValue'
'st.happy_camper.flume.twitter.TwitterStreamingHBaseSink.TwitterStreamingHBaseSink','org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists'
'st.happy_camper.flume.twitter.TwitterStreamingHBaseSink.open','org.apache.hadoop.hbase.client.HTable.<init>'
'st.happy_camper.flume.twitter.TwitterStreamingHBaseSink.append','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'st.happy_camper.flume.twitter.TwitterStreamingHBaseSink.close','org.apache.hadoop.hbase.client.HTable.close'
'st.happy_camper.flume.twitter.TwitterStreamingPlugin.build','org.apache.hadoop.hbase.HBaseConfiguration.create'
'org.hackreduce.models.ngram.TwoGram.TwoGram','org.apache.hadoop.io.Text.toString'
'org.hackreduce.mappers.ngram.TwoGramMapper.configureJob','org.apache.hadoop.mapreduce.Job.setInputFormatClass'
'org.apache.pig.test.utils.TypeCheckingTestUtil.genDummyLOLoadNewLP','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.nutch.searcher.more.TypeQueryFilter.setConf','org.apache.hadoop.conf.Configuration.getFloat'
'org.apache.hive.builtins.UDAFUnionMap.Evaluator.init','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardObjectInspector'
'org.apache.hive.builtins.UDAFUnionMap.Evaluator.iterate','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject'
'org.apache.hive.builtins.UDAFUnionMap.Evaluator.merge','org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject'
'com.nexr.platform.hive.udf.UDFChr.evaluate','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.set'
'com.nexr.platform.hive.udf.UDFDateFormat.evaluate','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.jointhegrid.hivecasudfs.UDFDelete.initialize','org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.<init>'
'com.jointhegrid.hivecasudfs.UDFDelete.evaluate','org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.getPrimitiveJavaObject'
'com.jointhegrid.hivecasudfs.UDFDeleteTest.testSingleThread','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'com.nexr.platform.hive.udf.UDFSysDate.evaluate','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.set'
'com.nexr.platform.hive.udf.UDFToChar.evaluate','org.apache.hadoop.hive.serde2.io.ByteWritable.get org.apache.hadoop.hive.serde2.lazy.LazyInteger.writeUTF8NoException org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.hive.serde2.io.ByteWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.hive.serde2.io.ShortWritable.get org.apache.hadoop.hive.serde2.lazy.LazyInteger.writeUTF8NoException org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.hive.serde2.io.ShortWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.io.IntWritable.get org.apache.hadoop.hive.serde2.lazy.LazyInteger.writeUTF8NoException org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.io.LongWritable.get org.apache.hadoop.hive.serde2.lazy.LazyLong.writeUTF8NoException org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.io.FloatWritable.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.hive.serde2.io.DoubleWritable.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.hive.serde2.io.DoubleWritable.get org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.equals org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'org.apache.mahout.df.tools.UDistrib.runTool','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileUtil.createLocalTempFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataOutputStream.writeBytes org.apache.hadoop.fs.FSDataOutputStream.writeChar org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileUtil.copyMerge'
'com.digitalpebble.behemoth.uima.UIMAProcessor.process','org.apache.hadoop.mapred.Reporter.setStatus org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'com.digitalpebble.behemoth.uima.UIMAProcessor.setConf','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.digitalpebble.behemoth.uima.UIMAProcessor.convertCASToBehemoth','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapred.Reporter.incrCounter'
'org.apache.mahout.math.hadoop.stochasticsvd.UJob.start','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.submit'
'org.apache.mahout.math.hadoop.stochasticsvd.UJob.waitForCompletion','org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'org.apache.mahout.math.hadoop.stochasticsvd.UJob.UMapper.setup','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.livingsocial.hive.udf.URLDecode.evaluate','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.service.crawler.util.URLFPBloomFilter.load','org.apache.hadoop.fs.FSDataInputStream.readFully'
'org.commoncrawl.service.crawler.util.URLFPBloomFilter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource org.apache.hadoop.conf.Configuration.addResource'
'org.commoncrawl.service.crawler.util.URLFPBloomFilter.simpleTest','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.nutch.net.URLFilters.URLFilters','org.apache.hadoop.conf.Configuration.get'
'org.apache.nutch.net.URLFilters.URLFilters','org.apache.hadoop.conf.Configuration.get'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.cachedResultsAvailable','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.executeLocal','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.LocalFileSystem.delete org.apache.hadoop.fs.LocalFileSystem.delete'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.runOutlinkLocalQuery','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.runInlinksLocalQuery','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.getPos org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.fs.FSDataOutputStream.writeLong org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.readPaginatedInlinkingDomainInfo','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FSDataInputStream.readFully org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.DataInputBuffer.readInt org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.readPaginatedInlinkingDomainDetail','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.skip org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.close'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.executeRemote','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FSDataOutputStream.flush org.apache.hadoop.fs.FSDataOutputStream.close org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FSDataOutputStream.close'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.requiresRemoteDispatch','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.readPaginatedResults','org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.fs.FSDataInputStream.readLong org.apache.hadoop.fs.FSDataInputStream.readLong'
'org.commoncrawl.service.queryserver.query.URLLinksQuery.getCachedResults','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FSDataInputStream.close'
'org.apache.nutch.indexer.urlmeta.URLMetaIndexingFilter.filter','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.toString'
'org.apache.nutch.indexer.urlmeta.URLMetaIndexingFilter.setConf','org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.nutch.net.URLNormalizers.URLNormalizers','org.apache.hadoop.conf.Configuration.getInt'
'org.apache.nutch.net.URLNormalizers.findExtensions','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.commoncrawl.util.URLUtils.getHostNameFromURLKey','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength org.apache.hadoop.io.Text.getBytes'
'org.commoncrawl.util.URLUtils.testURL','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.util.URLUtils.URLFPV2RawComparator.compare','org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.read org.apache.hadoop.io.DataInputBuffer.read org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVLong org.apache.hadoop.io.DataInputBuffer.skip org.apache.hadoop.io.WritableUtils.readVLong'
'org.commoncrawl.util.URLUtils.URLFPV2RawComparator.validateComparator','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'org.lilyproject.repository.impl.UUIDRecordId.getBasicBytes','org.apache.hadoop.hbase.util.Bytes.putLong org.apache.hadoop.hbase.util.Bytes.putLong'
'com.mongodb.hadoop.examples.ufos.UfoSightingsReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'com.mongodb.hadoop.examples.ufos.UfoSightingsXMLConfig.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.accumulo.server.test.continuous.UndefinedAnalyzer.main','org.apache.hadoop.io.Text.<init>'
'sizzle.aggregators.UniqueAggregator.aggregate','org.apache.hadoop.util.bloom.Key.<init> org.apache.hadoop.util.bloom.Filter.membershipTest org.apache.hadoop.util.bloom.Filter.add'
'org.commoncrawl.mapred.pipelineV3.domainmeta.linkstats.UniqueIncomingRootDomainCounter.reduce','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.apache.mahout.clustering.spectral.common.UnitVectorizerJob.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.livingsocial.hive.udf.UnixLiberalTimestamp.evaluate','org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'com.mozilla.hadoop.UnknownPathFinder.getRegionPaths','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Writables.getHRegionInfo org.apache.hadoop.hbase.HRegionInfo.getTableDesc org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.HTableDescriptor.getTableDir org.apache.hadoop.fs.Path.toString org.apache.hadoop.hbase.HRegionInfo.getEncodedName org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'com.mozilla.hadoop.UnknownPathFinder.getAllPaths','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath'
'com.mozilla.hadoop.UnknownPathFinder.getFilesystemPaths','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.close'
'com.mozilla.hadoop.UnknownPathFinder.deleteFilesystemPaths','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.close'
'com.mozilla.hadoop.UnknownPathFinder.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init>'
'com.cloudera.recordbreaker.analyzer.UnknownTextDataDescriptor.isTextData','org.apache.hadoop.fs.FileSystem.open'
'com.cloudera.recordbreaker.analyzer.UnknownTextDataDescriptor.UnknownTextDataDescriptor','org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.apache.giraph.utils.UnmodifiableDoubleArrayIterator.next','org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.giraph.utils.UnmodifiableLongArrayIterator.next','org.apache.hadoop.io.LongWritable.<init>'
'org.apache.giraph.utils.UnmodifiableLongFloatEdgeArrayIterable.next','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.FloatWritable.<init>'
'org.apache.giraph.utils.UnmodifiableLongNullEdgeArrayIterable.next','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.NullWritable.get'
'org.apache.ivory.update.UpdateHelperTest.init','org.apache.hadoop.hdfs.MiniDFSCluster.<init>'
'com.sematext.hbase.hut.UpdatesProcessingUtil.processUpdates','org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.next'
'com.sematext.hbase.hut.UpdatesProcessingUtil.rollbackWrittenAfter','org.apache.hadoop.hbase.client.Scan.<init>'
'com.sematext.hbase.hut.UpdatesProcessingUtil.accept','org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.client.Result.getRow'
'com.sematext.hbase.hut.UpdatesProcessingUtil.rollbackWrittenBetween','org.apache.hadoop.hbase.client.Scan.<init>'
'org.cloudata.examples.weblink.UploadJob.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addArchiveToClassPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addArchiveToClassPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.filecache.DistributedCache.addArchiveToClassPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileOutputFormat.getOutputPath org.apache.hadoop.fs.FileSystem.delete'
'org.cloudata.examples.weblink.UploadJob.UploadJobMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.Reporter.progress'
'org.cloudata.examples.weblink.UploadJob.UploadJobMapper.insert','org.apache.hadoop.mapred.Reporter.progress'
'org.cloudata.examples.weblink.UploadJob.UploadJobMapper.configure','org.apache.hadoop.mapred.JobConf.get'
'org.cloudata.util.upload.UploadMap.map','org.apache.hadoop.io.Text.toString'
'org.cloudata.util.upload.UploadMap.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.getBoolean'
'org.lilyproject.repository.impl.primitivevaluetype.UriValueType.fromBytes','org.apache.hadoop.hbase.util.Bytes.toString'
'org.lilyproject.repository.impl.primitivevaluetype.UriValueType.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.hbasebook.hush.UrlManager.initializeShortIdCounter','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut'
'com.hbasebook.hush.UrlManager.addLongUrl','org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.checkAndPut org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits'
'com.hbasebook.hush.UrlManager.createShortUrl','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits'
'com.hbasebook.hush.UrlManager.createUserShortUrl','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.flushCommits'
'com.hbasebook.hush.UrlManager.getShortUrl','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong'
'com.hbasebook.hush.UrlManager.getLongUrl','org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.Get.addColumn org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.isEmpty org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.hbasebook.hush.UrlManager.generateShortId','org.apache.hadoop.hbase.client.Increment.<init> org.apache.hadoop.hbase.client.Increment.addColumn org.apache.hadoop.hbase.client.HTable.increment org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toLong'
'com.hbasebook.hush.UrlManager.getShortUrlIdsByUser','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toString'
'com.datasalt.pangool.examples.urlresolution.UrlResolution.UrlProcessor.map','org.apache.hadoop.io.Text.toString'
'com.datasalt.pangool.examples.urlresolution.UrlResolution.UrlMapProcessor.map','org.apache.hadoop.io.Text.toString'
'com.datasalt.pangool.examples.urlresolution.UrlResolution.Handler.reduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'com.datasalt.pangool.examples.urlresolution.UrlResolution.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.datasalt.pangool.examples.urlresolution.UrlResolution.main','org.apache.hadoop.util.ToolRunner.run'
'com.asakusafw.bulkloader.common.UrlStreamHandlerFactoryRegisterer.register','org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>'
'com.oreilly.springdata.hadoop.hbase.UserApp.main','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.impetus.kundera.examples.perf.dao.user.UserDaoNativeHbaseAPI.insertUser','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'com.cloudera.hadoop.hdfs.nfs.nfs4.UserIDMapper.get','org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.cloudera.hadoop.hdfs.nfs.nfs4.UserIDMapper.getCurrentUser','org.apache.hadoop.util.Shell.execCommand'
'org.apache.mahout.cf.taste.hadoop.pseudo.UserIDsMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get'
'semvec.mahout.UserItemPrefReducer.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'semvec.mahout.UserItemPrefReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.<init>'
'semvec.mahout.UserItemPrefReducer.collectUser','org.apache.hadoop.io.Text.<init>'
'com.snowballfinance.kddc.job.UserProfileJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.snowballfinance.kddc.job.UserProfileJob.run','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.snowballfinance.kddc.job.UserProfileJob.UserProfileMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.FloatWritable.<init>'
'com.snowballfinance.kddc.job.UserProfileJob.UserProfileSimReducer.reduce','org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.FloatWritable.<init>'
'io.beancounter.commons.pig.UserProfileLoader.setLocation','org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths'
'io.beancounter.commons.pig.UserProfileLoader.getInputFormat','org.apache.hadoop.mapreduce.lib.input.TextInputFormat.<init>'
'io.beancounter.commons.pig.UserProfileLoader.getNext','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentValue org.apache.hadoop.io.Text.getBytes'
'com.oreilly.springdata.hadoop.hbase.UserRepository.mapRow','org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.oreilly.springdata.hadoop.hbase.UserRepository.doInTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'org.apache.mahout.cf.taste.hadoop.item.UserVectorSplitterMapper.setup','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.fs.FileSystem.open'
'mia.recommender.ch06.UserVectorToCooccurrenceMapper.map','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.nearinfinity.blur.store.UsingHdfsDir.main','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.apache.pig.test.Util.createInputFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.get'
'org.apache.pig.test.Util.readOutput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.PathFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.get'
'org.apache.pig.test.Util.accept','org.apache.hadoop.fs.Path.getName'
'org.apache.pig.test.Util.deleteFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'org.apache.pig.test.Util.exists','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists'
'org.apache.pig.test.Util.getMkDirCommandForHadoop2_0','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.getName'
'org.apache.pig.test.Util.copyFromClusterToLocal','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.pig.test.Util.isHadoop23','org.apache.hadoop.util.VersionInfo.getVersion'
'org.apache.pig.test.Util.isHadoop203plus','org.apache.hadoop.util.VersionInfo.getVersion'
'org.apache.pig.test.Util.isHadoop205','org.apache.hadoop.util.VersionInfo.getVersion'
'org.apache.pig.test.Util.isHadoop1_0','org.apache.hadoop.util.VersionInfo.getVersion'
'org.apache.pig.test.Util.assertConfLong','org.apache.hadoop.conf.Configuration.getLong'
'org.apache.pig.test.Util.isHadoop2_0','org.apache.hadoop.util.VersionInfo.getVersion'
'com.urbanairship.datacube.Util.countRows','org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.client.ResultScanner.close'
'org.apache.avro.tool.Util.fileOrStdin','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.avro.tool.Util.fileOrStdout','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.openspaces.cassandraeds.Util.newConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set'
'cascading.load.util.Util.getNumTaskTrackers','org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers'
'cascading.load.util.Util.getMaxConcurrentMappers','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getInt'
'cascading.load.util.Util.getMaxConcurrentReducers','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.getInt'
'cascading.load.util.Util.hasNativeZlib','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.io.compress.zlib.ZlibFactory.isNativeZlibLoaded'
'com.quest.orahive.Utilities.getOraHiveJarFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'org.sifarish.common.UtilityAggregator.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.sifarish.common.UtilityAggregator.AggregateMapper.map','org.apache.hadoop.io.Text.toString'
'org.sifarish.common.UtilityAggregator.AggregateReducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.io.NullWritable.get'
'org.sifarish.common.UtilityAggregator.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.pig.impl.util.Utils.getScriptSchema','org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.impl.util.Utils.getSchema','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get'
'org.apache.pig.impl.util.Utils.getTmpFileStorageObject','org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.pig.impl.util.Utils.getPathLength','org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.listStatus'
'com.splunk.shuttl.archiver.util.UtilsPathTest.createPathByAppending_pathsWithSchemes_theNewPathShouldProperlyBeCreated','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.splunk.shuttl.archiver.util.UtilsPathTest.createPathByAppending_pathsWithSchemesAndEndingSlahs_theNewPathShouldProperlyBeCreated','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.splunk.shuttl.archiver.util.UtilsPathTest.createPathByAppending_appendPathHavingDiffrentScheme_schemeOfTheToBeAppededPathShouldBeUsed','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.splunk.shuttl.archiver.util.UtilsPathTest.createPathByAppending_appendPathWithOutScheme_schemeOfTheToBeAppededPathShouldBeUsed','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'hadooptree.Utils.loadTree','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.fs.FSDataInputStream.close'
'com.datasalt.pangool.flow.Utils.delete','org.apache.hadoop.fs.Path.getFileSystem'
'org.apache.accumulo.pig.Utils.objToText','org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.classifier.df.data.Utils.writeDataToFile','org.apache.hadoop.fs.Path.toString'
'org.apache.mahout.classifier.df.data.Utils.writeDataToTestFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.classifier.df.data.Utils.writeDatasetToTestFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.bah.culvert.test.Utils.testReadWrite','org.apache.hadoop.io.Writable.write org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.WritableComparable<com.bah.culvert.test.T>.compareTo'
'co.nubetech.hiho.uuid.UuidJob.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'co.nubetech.hiho.uuid.UuidJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'co.nubetech.hiho.uuid.UuidMapper.map','org.apache.hadoop.mapreduce.TaskAttemptID.hashCode org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init>'
'org.apache.oozie.servlet.V0JobsServlet.submitJob','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.V0JobsServlet.submitJob','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.V1JobsServlet.submitJob','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.V1JobsServlet.submitWorkflowJob','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.V1JobsServlet.submitCoordinatorJob','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.V1JobsServlet.submitBundleJob','org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.servlet.V1JobsServlet.submitHttpJob','org.apache.hadoop.conf.Configuration.get'
'.VIntWritableTest.test','org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.util.StringUtils.byteToHexString'
'.VIntWritableTest.testSizes','org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VIntWritable.<init> org.apache.hadoop.io.VIntWritable.<init>'
'org.apache.mahout.math.hadoop.stochasticsvd.VJob.start','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setCompressOutput org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputCompressorClass org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.setOutputCompressionType org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.toString org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.submit'
'org.apache.mahout.math.hadoop.stochasticsvd.VJob.waitForCompletion','org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.mapreduce.Job.isSuccessful'
'org.apache.mahout.math.hadoop.stochasticsvd.VJob.VMapper.setup','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.mozilla.grouperfish.pig.storage.VWStorage.getOutputFormat','org.apache.hadoop.mapreduce.lib.output.TextOutputFormat<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text>.<init>'
'com.mozilla.grouperfish.pig.storage.VWStorage.putNext','org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.RecordWriter.write'
'com.mozilla.grouperfish.pig.storage.VWStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'com.harioca.shell.converter.ValueConverterFactory.convertValue','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toLong org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toDouble org.apache.hadoop.hbase.util.Bytes.toFloat org.apache.hadoop.hbase.util.Bytes.toShort'
'filters.ValueFilterExample.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.filter.SubstringComparator.<init> org.apache.hadoop.hbase.filter.ValueFilter.<init> org.apache.hadoop.hbase.client.Scan.<init> org.apache.hadoop.hbase.client.Scan.setFilter org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.Get.setFilter org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.client.Result.raw org.apache.hadoop.hbase.KeyValue.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'com.asakusafw.runtime.value.ValueOptionTestRoot.restoreWritable','org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.Writable.hashCode'
'com.asakusafw.runtime.value.ValueOptionTestRoot.checkLength','org.apache.hadoop.io.Writable.getClass'
'com.asakusafw.runtime.value.ValueOptionTestRoot.toBytes','org.apache.hadoop.io.Writable.write'
'org.lilyproject.repository.impl.test.ValueTypeTest.setUpBeforeClass','org.apache.hadoop.fs.Path.<init>'
'org.lilyproject.repository.impl.test.ValueTypeTest.testBlobType','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.lilyproject.repository.impl.test.ValueTypeTest.XYPrimitiveValueType.fromBytes','org.apache.hadoop.hbase.util.Bytes.toInt org.apache.hadoop.hbase.util.Bytes.toInt'
'org.lilyproject.repository.impl.test.ValueTypeTest.XYPrimitiveValueType.toBytes','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.add'
'fr.insarennes.fafdti.hadoop.Value.Value','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.Text.<init>'
'fr.insarennes.fafdti.hadoop.Value.readFields','org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.readFields org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields'
'fr.insarennes.fafdti.hadoop.Value.write','org.apache.hadoop.io.DoubleWritable.write org.apache.hadoop.io.Text.write'
'fr.insarennes.fafdti.hadoop.Value.compareTo','org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.DoubleWritable.compareTo'
'fr.insarennes.fafdti.hadoop.Value.equals','org.apache.hadoop.io.Text.equals org.apache.hadoop.io.DoubleWritable.equals'
'fr.insarennes.fafdti.hadoop.Value.getDoubleValue','org.apache.hadoop.io.DoubleWritable.get'
'fr.insarennes.fafdti.hadoop.Value.getTextValue','org.apache.hadoop.io.Text.toString'
'fr.insarennes.fafdti.hadoop.Value.set','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.set org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.DoubleWritable.set'
'net.joshdevins.talks.hadoopstart.mr.ValuesCounterReducer.reduce','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.<init>'
'vecAdd.VecAdd.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setInt org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'mia.clustering.ch12.lastfm.VectorCreationJob.createVectors','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.toString org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Integer>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Map<java.lang.String,java.lang.Integer>>.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'mia.clustering.ch12.lastfm.VectorCreationJob.generateDictionary','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'mia.clustering.ch12.lastfm.VectorCreationJob.CreateNewConfiguration','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.mahout.math.hadoop.similarity.VectorDistanceMapper.map','org.apache.hadoop.io.WritableComparable<?>.toString org.apache.hadoop.io.DoubleWritable.<init>'
'org.apache.mahout.math.hadoop.similarity.VectorDistanceMapper.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.mahout.math.hadoop.similarity.VectorDistanceSimilarityJob.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.math.hadoop.similarity.VectorDistanceSimilarityJob.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'vector.VectorDoubleWritable.VectorDoubleWritable','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.utils.vectors.VectorDumper.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.mapred.Utils.OutputFileUtils.OutputFilesFilter.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileUtil.stat2Paths org.apache.hadoop.fs.FileSystem.globStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.io.Writable.toString'
'org.apache.mahout.utils.vectors.VectorDumper.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.mahout.utils.vectors.VectorHelper.loadTermDictionary','org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.utils.vectors.VectorHelper.loadTermDictionary','org.apache.hadoop.fs.Path.<init>'
'skywriting.examples.skyhout.input.VectorInputParserTask.invoke','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setClassLoader org.apache.hadoop.conf.Configuration.setClass org.apache.hadoop.io.serializer.WritableSerialization.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.clustering.spectral.common.VectorMatrixMultiplicationJob.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion org.apache.hadoop.fs.Path.<init>'
'org.apache.mahout.clustering.spectral.common.VectorMatrixMultiplicationJob.VectorMatrixMultiplicationMapper.map','org.apache.hadoop.io.IntWritable.get'
'skywriting.examples.skyhout.linalg.VectorMerger.mergeInputs','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.DoubleWritable.get'
'skywriting.examples.skyhout.linalg.VectorMerger.readSingleVectorFile','org.apache.hadoop.io.Text.<init>'
'skywriting.examples.skyhout.linalg.VectorMerger.writeResultFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.math.VectorWritableTest.writeAndRead','org.apache.hadoop.io.Writable.write org.apache.hadoop.io.Writable.readFields'
'com.mozilla.grouperfish.pig.eval.ml.Vectorizer.loadFeatureIndex','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors.write','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors.readAsIntMap','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open'
'org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors.read','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.open'
'goraci.Verify.VerifyMapper.map','org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.VLongWritable.set'
'goraci.Verify.VerifyReducer.reduce','org.apache.hadoop.io.VLongWritable.get org.apache.hadoop.io.VLongWritable.get org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'goraci.Verify.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.Job.waitForCompletion'
'goraci.Verify.start','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.submit'
'goraci.Verify.isComplete','org.apache.hadoop.mapreduce.Job.isComplete'
'goraci.Verify.isSuccessful','org.apache.hadoop.mapreduce.Job.isSuccessful'
'goraci.Verify.waitForCompletion','org.apache.hadoop.mapreduce.Job.waitForCompletion'
'goraci.Verify.readFlushed','org.apache.hadoop.conf.Configuration.setStrings'
'goraci.Verify.verify','org.apache.hadoop.mapreduce.Job.getCounters org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counters.findCounter org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue org.apache.hadoop.mapreduce.Counter.getValue'
'goraci.Verify.main','org.apache.hadoop.util.ToolRunner.run'
'ivory.integration.VerifyCluePositionalIndexIP.runBuildIndex','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'ivory.integration.VerifyGov2PositionalIndexIP.runBuildIndex','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'edu.umd.cloud9.integration.webgraph.VerifyGov2Webgraph.runTrecDriver','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'edu.umd.cloud9.integration.webgraph.VerifyGov2Webgraph.verifyAnchors','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf'
'org.apache.accumulo.server.test.VerifyIngest.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'edu.umd.cloud9.example.memcached.demo.VerifyLogProbInMemcached.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.Text.toString'
'org.apache.giraph.examples.VerifyMessage.VerifyMessageVertex.VerifyMessageVertexWorkerContext.postApplication','org.apache.hadoop.io.LongWritable.get'
'org.apache.giraph.examples.VerifyMessage.VerifyMessageVertex.compute','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.FloatWritable.get'
'ivory.integration.wikipedia.VerifyWikipediaProcessingCrosslingual.runBuildIndexEnSide','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'ivory.integration.wikipedia.VerifyWikipediaProcessingCrosslingual.verifyTermDocVectorsEn','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf'
'ivory.integration.wikipedia.VerifyWikipediaProcessingCrosslingual.verifyIntDocVectorsEn','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf'
'ivory.integration.wikipedia.VerifyWikipediaProcessingCrosslingual.runBuildIndexDeSide','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'ivory.integration.wikipedia.VerifyWikipediaProcessingCrosslingual.verifyTermDocVectorsDe','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf'
'ivory.integration.wikipedia.VerifyWikipediaProcessingCrosslingual.verifyIntDocVectorsDe','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getConf'
'ivory.integration.VerifyWt10gNonPositionalIndexIP.runBuildIndex','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'ivory.integration.VerifyWt10gPositionalIndexIP.runBuildIndex','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyFromLocalFile'
'org.apache.accumulo.server.test.randomwalk.bulk.Verify.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'elephantdb.store.VersionedStore.VersionedStore','org.apache.hadoop.conf.Configuration.<init>'
'elephantdb.store.VersionedStore.versionPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'elephantdb.store.VersionedStore.createVersion','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'elephantdb.store.VersionedStore.deleteVersion','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'elephantdb.store.VersionedStore.cleanup','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete'
'elephantdb.store.VersionedStore.getAllVersions','org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString'
'elephantdb.store.VersionedStore.tokenPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'elephantdb.store.VersionedStore.normalizePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified'
'elephantdb.store.VersionedStore.validateAndGetVersion','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent'
'elephantdb.store.VersionedStore.parseVersion','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'elephantdb.store.VersionedStore.createNewFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile'
'elephantdb.store.VersionedStore.mkdirs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'elephantdb.store.VersionedStore.listDir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'backtype.hadoop.datastores.VersionedStore.VersionedStore','org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.datastores.VersionedStore.versionPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.datastores.VersionedStore.createVersion','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'backtype.hadoop.datastores.VersionedStore.deleteVersion','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete'
'backtype.hadoop.datastores.VersionedStore.cleanup','org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileSystem.delete'
'backtype.hadoop.datastores.VersionedStore.getAllVersions','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.datastores.VersionedStore.tokenPath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'backtype.hadoop.datastores.VersionedStore.normalizePath','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.makeQualified'
'backtype.hadoop.datastores.VersionedStore.validateAndGetVersion','org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent'
'backtype.hadoop.datastores.VersionedStore.parseVersion','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getName'
'backtype.hadoop.datastores.VersionedStore.createNewFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.createNewFile'
'backtype.hadoop.datastores.VersionedStore.mkdirs','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs'
'backtype.hadoop.datastores.VersionedStore.listDir','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'com.cloudera.science.matching.VertexData.VertexData','org.apache.hadoop.io.Text.toString'
'com.cloudera.science.matching.VertexData.extractVertexId','org.apache.hadoop.io.Text.<init>'
'com.cloudera.science.matching.VertexData.extractVertexState','org.apache.hadoop.io.Text.<init>'
'com.cloudera.science.matching.VertexData.extractPriceIndex','org.apache.hadoop.io.Text.<init>'
'com.cloudera.science.matching.VertexData.extractEdges','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.goldenorb.io.input.VertexInput.initialize','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.serializer.SerializationFactory.<init> org.apache.hadoop.io.serializer.SerializationFactory.getDeserializer org.apache.hadoop.io.serializer.Deserializer<? extends org.goldenorb.io.input.org.apache.hadoop.mapreduce.InputSplit>.open org.apache.hadoop.io.serializer.Deserializer<? extends org.goldenorb.io.input.org.apache.hadoop.mapreduce.InputSplit>.deserialize org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapreduce.JobID.<init> org.apache.hadoop.mapreduce.JobContext.<init> org.apache.hadoop.mapreduce.JobContext.getInputFormatClass org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.mapreduce.JobContext.getJobID org.apache.hadoop.mapreduce.TaskID.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init> org.apache.hadoop.mapreduce.InputFormat<org.goldenorb.io.input.INPUT_KEY,org.goldenorb.io.input.INPUT_VALUE>.createRecordReader org.apache.hadoop.mapreduce.RecordReader<org.goldenorb.io.input.INPUT_KEY,org.goldenorb.io.input.INPUT_VALUE>.initialize'
'com.cloudera.science.matching.graph.VertexState.clearMatchId','org.apache.hadoop.io.Text.<init>'
'com.cloudera.science.matching.graph.VertexState.readFields','org.apache.hadoop.io.Text.readFields org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.readFields'
'com.cloudera.science.matching.graph.VertexState.write','org.apache.hadoop.io.Text.write org.apache.hadoop.io.WritableUtils.writeVInt'
'org.apache.giraph.examples.VertexWithDoubleValueFloatEdgeTextOutputFormat.VertexWithDoubleValueWriter.writeVertex','org.apache.hadoop.io.Text.<init>'
'com.vertica.hadoop.VerticaConfiguration.configureVertica','org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'com.vertica.hadoop.VerticaConfiguration.getConnection','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'com.vertica.hadoop.VerticaConfiguration.getInputQuery','org.apache.hadoop.conf.Configuration.get'
'com.vertica.hadoop.VerticaConfiguration.setInputQuery','org.apache.hadoop.conf.Configuration.set'
'com.vertica.hadoop.VerticaConfiguration.getParamsQuery','org.apache.hadoop.conf.Configuration.get'
'com.vertica.hadoop.VerticaConfiguration.setParamsQuery','org.apache.hadoop.conf.Configuration.set'
'com.vertica.hadoop.VerticaConfiguration.getInputParameters','org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.util.StringUtils.hexStringToByte org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.readInt org.apache.hadoop.io.DataInputBuffer.readInt'
'com.vertica.hadoop.VerticaConfiguration.setInputParams','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.writeInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.util.StringUtils.byteToHexString org.apache.hadoop.conf.Configuration.setStrings'
'com.vertica.hadoop.VerticaConfiguration.getInputDelimiter','org.apache.hadoop.conf.Configuration.get'
'com.vertica.hadoop.VerticaConfiguration.setInputDelimiter','org.apache.hadoop.conf.Configuration.set'
'com.vertica.hadoop.VerticaConfiguration.getInputRecordTerminator','org.apache.hadoop.conf.Configuration.get'
'com.vertica.hadoop.VerticaConfiguration.setInputRecordTerminator','org.apache.hadoop.conf.Configuration.set'
'com.vertica.hadoop.VerticaConfiguration.getOutputTableName','org.apache.hadoop.conf.Configuration.get'
'com.vertica.hadoop.VerticaConfiguration.setOutputTableName','org.apache.hadoop.conf.Configuration.set'
'com.vertica.hadoop.VerticaConfiguration.getOutputTableDef','org.apache.hadoop.conf.Configuration.getStrings'
'com.vertica.hadoop.VerticaConfiguration.setOutputTableDef','org.apache.hadoop.conf.Configuration.setStrings'
'com.vertica.hadoop.VerticaConfiguration.getBatchSize','org.apache.hadoop.conf.Configuration.getLong'
'com.vertica.hadoop.VerticaConfiguration.getDropTable','org.apache.hadoop.conf.Configuration.getBoolean'
'com.vertica.hadoop.VerticaConfiguration.setDropTable','org.apache.hadoop.conf.Configuration.setBoolean'
'com.vertica.hadoop.VerticaConfiguration.getOutputDelimiter','org.apache.hadoop.conf.Configuration.get'
'com.vertica.hadoop.VerticaConfiguration.setOutputDelimiter','org.apache.hadoop.conf.Configuration.set'
'com.vertica.hadoop.VerticaConfiguration.getOutputRecordTerminator','org.apache.hadoop.conf.Configuration.get'
'com.vertica.hadoop.VerticaConfiguration.setOutputRecordTerminator','org.apache.hadoop.conf.Configuration.set'
'com.vertica.hadoop.VerticaConfiguration.getOptimizePollTimeout','org.apache.hadoop.conf.Configuration.getLong'
'com.vertica.hadoop.VerticaConfiguration.setOptimizePollTimeout','org.apache.hadoop.conf.Configuration.setLong'
'com.vertica.hadoop.VerticaInputFormat.setInput','org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.vertica.hadoop.VerticaInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.vertica.hadoop.VerticaInputFormat.getSplits','org.apache.hadoop.mapreduce.JobContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt'
'com.vertica.hadoop.VerticaInputSplit.readFields','org.apache.hadoop.io.Text.readString'
'com.vertica.hadoop.VerticaInputSplit.write','org.apache.hadoop.io.Text.writeString'
'com.vertica.pig.VerticaLoader.getNext','org.apache.hadoop.mapreduce.RecordReader.nextKeyValue org.apache.hadoop.mapreduce.RecordReader.getCurrentKey org.apache.hadoop.mapreduce.RecordReader.getCurrentValue'
'com.vertica.pig.VerticaLoader.getSchema','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.vertica.pig.VerticaLoader.setLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.conf.Configuration.setBoolean'
'com.vertica.hadoop.VerticaRecordWriter.write','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'com.vertica.pig.VerticaStorer.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration'
'com.vertica.hadoop.deprecated.VerticaStreamingRecordWriter.write','org.apache.hadoop.io.Text.toString'
'edu.duke.starfish.whatif.virtualfs.VirtualFSDataSetModel.generateMapInputSpecs','org.apache.hadoop.conf.Configuration.getStrings'
'edu.duke.starfish.whatif.virtualfs.VirtualFSDataSetModel.isInputFileSplittable','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'edu.duke.starfish.whatif.virtualfs.VirtualFSDataSetModel.setVirtualInputPaths','org.apache.hadoop.conf.Configuration.set'
'edu.duke.starfish.whatif.virtualfs.VirtualFSDataSetModel.setVirtualOutputPaths','org.apache.hadoop.conf.Configuration.getInt'
'edu.duke.starfish.whatif.VirtualMRJobManager.VirtualMRJobManager','org.apache.hadoop.conf.Configuration.<init>'
'edu.umd.hooka.VocabServerClient.remoteAddOrGet','org.apache.hadoop.io.Text.utf8Length org.apache.hadoop.io.Text.writeString'
'voldemort.store.readonly.mr.azkaban.VoldemortBuildAndPushJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set'
'voldemort.store.readonly.mr.azkaban.VoldemortBuildAndPushJob.runBuildStore','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'voldemort.store.readonly.mr.azkaban.VoldemortBuildAndPushJob.getInputPath','org.apache.hadoop.fs.Path.<init>'
'voldemort.store.readonly.mr.azkaban.VoldemortSwapJob.VoldemortSwapConf.VoldemortSwapConf','org.apache.hadoop.conf.Configuration.<init>'
'voldemort.store.readonly.mr.azkaban.VoldemortSwapJob.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.makeQualified org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.flume.channel.recoverable.memory.wal.WALDataFile.newWALEntry','org.apache.hadoop.util.ReflectionUtils.newInstance'
'eu.scape_project.tb.wc.archd.test.WARCTest.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileSplit.<init> org.apache.hadoop.mapreduce.TaskAttemptID.<init> org.apache.hadoop.mapreduce.TaskAttemptContext.<init>'
'eu.scape_project.tb.wc.archd.test.WARCTest.testNextKeyValue','org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.initialize org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.nextKeyValue org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.getCurrentKey org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.Text,eu.scape_project.tb.wc.archd.hdreader.ArcRecord>.getCurrentValue org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.archive.hadoop.mapreduce.WATExtractorMapper.setTargetDir','org.apache.hadoop.conf.Configuration.set'
'org.archive.hadoop.mapreduce.WATExtractorMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem'
'org.archive.hadoop.mapreduce.WATExtractorMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.archive.hadoop.mapreduce.WATExtractorMapper.doExtract','org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.create'
'org.archive.hadoop.mapreduce.WATExtractorMapper.getPathLength','org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen'
'org.archive.hadoop.mapreduce.WATExtractorMapper.setOverride','org.apache.hadoop.conf.Configuration.setBoolean'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.WRITEHandler.wouldBlock','org.apache.hadoop.fs.Path.toUri'
'com.cloudera.hadoop.hdfs.nfs.nfs4.handlers.WRITEHandler.doHandle','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.toUri'
'org.wso2.carbon.hadoop.security.group.mapping.WSBasedCarbonGroupMapping.getCarbonRoles','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.WWWPrefixStatsCollectorStep.map','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.apache.hadoop.io.IntWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.apache.hadoop.io.IntWritable>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.WWWPrefixStatsCollectorStep.reduce','org.apache.hadoop.mapred.OutputCollector<org.commoncrawl.util.TextBytes,org.commoncrawl.util.TextBytes>.collect'
'org.commoncrawl.mapred.pipelineV3.domainmeta.crawlstats.WWWPrefixStatsCollectorStep.runStep','org.apache.hadoop.mapred.JobClient.runJob'
'goraci.Walker.run','org.apache.hadoop.conf.Configuration.<init>'
'goraci.Walker.main','org.apache.hadoop.util.ToolRunner.run'
'com.digitalpebble.behemoth.io.warc.WarcFileRecordReader.WarcFileRecordReader','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.mapred.MultiFileSplit.getPaths org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.conf.Configuration.getClassByName org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.digitalpebble.behemoth.io.warc.WarcFileRecordReader.openNextFile','org.apache.hadoop.fs.FSDataInputStream.close org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.fs.Path.toString'
'com.digitalpebble.behemoth.io.warc.WarcFileRecordReader.next','org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.LongWritable.set'
'com.digitalpebble.behemoth.io.warc.WarcFileRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'com.digitalpebble.behemoth.io.warc.WarcFileRecordReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'ivory.regression.basic.Web09catB_Baseline.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.cikm2010.Web09catB_Title_Indep.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.cikm2010.Web09catB_Title_Joint.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.nutch.scoring.webgraph.WebGraph.OutlinkDb.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.scoring.webgraph.WebGraph.OutlinkDb.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum>.collect'
'org.apache.nutch.scoring.webgraph.WebGraph.OutlinkDb.reduce','org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum>.collect'
'org.apache.nutch.scoring.webgraph.WebGraph.InlinkDb.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum>.collect'
'org.apache.nutch.scoring.webgraph.WebGraph.NodeDb.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node>.collect'
'org.apache.nutch.scoring.webgraph.WebGraph.createWebGraph','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.scoring.webgraph.WebGraph.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.scoring.webgraph.WebGraph.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.scoring.webgraph.WebGraph.OutlinkDb.configure','org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean org.apache.hadoop.mapred.JobConf.getBoolean'
'org.apache.nutch.scoring.webgraph.WebGraph.OutlinkDb.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.BooleanWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable>.collect'
'org.apache.nutch.scoring.webgraph.WebGraph.OutlinkDb.reduce','org.apache.hadoop.io.WritableUtils.clone org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.BooleanWritable.get org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum>.collect'
'org.apache.nutch.scoring.webgraph.WebGraph.InlinkDb.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum>.collect'
'org.apache.nutch.scoring.webgraph.WebGraph.NodeDb.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node>.collect'
'org.apache.nutch.scoring.webgraph.WebGraph.createWebGraph','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.StringUtils.stringifyException org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setBoolean org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.StringUtils.stringifyException'
'org.apache.nutch.scoring.webgraph.WebGraph.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.nutch.scoring.webgraph.WebGraph.run','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.Path.<init> org.apache.hadoop.util.StringUtils.stringifyException'
'org.cloudata.examples.web.WebKeyRangePartitioner.getPartition','org.apache.hadoop.io.Text.find org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.WritableComparable.toString'
'org.cloudata.examples.web.WebKeyRangePartitioner.configure','org.apache.hadoop.mapred.JobConf.get'
'org.pentaho.weblogs.WebLogs.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'org.pentaho.weblogs.WebLogs.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'org.pentaho.weblogs.WebLogs.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.apache.gora.examples.WebPageDataCreator.run','org.apache.hadoop.conf.Configuration.<init>'
'org.cloudata.examples.web.WebTableJob.exec','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.addInputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setMaxMapAttempts org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete'
'org.cloudata.examples.web.WebTableJob.WebTableMap.map','org.apache.hadoop.io.Text.getBytes org.apache.hadoop.io.Text.getLength'
'org.apache.mahout.common.distance.WeightedDistanceMeasure.configure','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.open'
'ivory.core.data.document.WeightedIntDocVector.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'ivory.core.data.document.WeightedIntDocVector.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.mahout.classifier.naivebayes.training.WeightsMapperTest.scores','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init>'
'org.apache.oozie.command.wf.WfEndXCommand.deleteWFDir','org.apache.hadoop.fs.FileSystem.getHomeDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'meetup.beeno.filter.WhileMatchFilter.filterRowKey','org.apache.hadoop.hbase.filter.Filter.filterRowKey'
'meetup.beeno.filter.WhileMatchFilter.filterAllRemaining','org.apache.hadoop.hbase.filter.Filter.filterAllRemaining'
'meetup.beeno.filter.WhileMatchFilter.filterKeyValue','org.apache.hadoop.hbase.filter.Filter.filterKeyValue'
'meetup.beeno.filter.WhileMatchFilter.filterRow','org.apache.hadoop.hbase.filter.Filter.filterRow'
'meetup.beeno.filter.WhileMatchFilter.write','org.apache.hadoop.hbase.filter.Filter.getClass org.apache.hadoop.hbase.filter.Filter.write'
'meetup.beeno.filter.WhileMatchFilter.readFields','org.apache.hadoop.hbase.filter.Filter.readFields'
'.WholeFileRecordReader.createKey','org.apache.hadoop.io.NullWritable.get'
'.WholeFileRecordReader.createValue','org.apache.hadoop.io.BytesWritable.<init>'
'.WholeFileRecordReader.getPos','org.apache.hadoop.mapred.FileSplit.getLength'
'.WholeFileRecordReader.next','org.apache.hadoop.mapred.FileSplit.getLength org.apache.hadoop.mapred.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.readFully org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.IOUtils.closeStream'
'org.pingles.cascading.cassandra.WideRowScheme.sinkInit','org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat'
'org.pingles.cascading.cassandra.WideRowScheme.sink','org.apache.hadoop.mapred.OutputCollector.collect'
'org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorDriver.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'edu.umd.cloud9.collection.wikipedia.WikipediaForwardIndex.WikipediaForwardIndex','org.apache.hadoop.conf.Configuration.<init>'
'edu.umd.cloud9.collection.wikipedia.WikipediaForwardIndex.loadIndex','org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.readUTF org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readInt org.apache.hadoop.fs.FSDataInputStream.readShort org.apache.hadoop.fs.FSDataInputStream.close'
'edu.umd.cloud9.collection.wikipedia.WikipediaForwardIndex.getDocument','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'edu.umd.cloud9.collection.wikipedia.WikipediaForwardIndex.getLastDocno','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.get'
'mia.recommender.ch06.WikipediaItemIDIndexMapper.map','org.apache.hadoop.io.Text.toString'
'org.apache.mahout.text.wikipedia.WikipediaMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.text.wikipedia.WikipediaMapper.setup','org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.fromString org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'org.apache.mahout.text.WikipediaMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.text.WikipediaMapper.setup','org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.fromString org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean'
'org.commoncrawl.util.WikipediaPage.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'org.commoncrawl.util.WikipediaPage.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'edu.umd.cloud9.collection.wikipedia.WikipediaPageInputFormat.WikipediaPageRecordReader.WikipediaPageRecordReader','org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'edu.umd.cloud9.collection.wikipedia.WikipediaPageInputFormat.WikipediaPageRecordReader.next','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.Text.toString'
'edu.umd.cloud9.collection.wikipedia.WikipediaPageInputFormat.WikipediaPageRecordReader.createKey','org.apache.hadoop.io.LongWritable.<init>'
'edu.umd.cloud9.collection.wikipedia.WikipediaPage.write','org.apache.hadoop.io.WritableUtils.writeVInt'
'edu.umd.cloud9.collection.wikipedia.WikipediaPage.readFields','org.apache.hadoop.io.WritableUtils.readVInt'
'org.apache.mahout.text.WikipediaToSequenceFile.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.text.WikipediaToSequenceFile.runJob','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.util.GenericsUtil.getClass org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.<init> org.apache.hadoop.io.DefaultStringifier<java.util.Set<java.lang.String>>.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.mahout.text.wikipedia.WikipediaXmlSplitter.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.compress.BZip2Codec.<init> org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'com.sap.hadoop.metadata.WindowingKeyPartitioner.configure','org.apache.hadoop.mapred.JobConf.getInt'
'com.urbanairship.datacube.dbharnesses.WithHTable.run','org.apache.hadoop.hbase.client.HTablePool.getTable org.apache.hadoop.hbase.client.HTablePool.putTable'
'com.urbanairship.datacube.dbharnesses.WithHTable.runWith','org.apache.hadoop.hbase.client.HTableInterface.put org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.incrementColumnValue org.apache.hadoop.hbase.client.HTableInterface.checkAndPut org.apache.hadoop.hbase.client.HTableInterface.checkAndDelete org.apache.hadoop.hbase.client.HTableInterface.batch org.apache.hadoop.hbase.client.HTableInterface.get org.apache.hadoop.hbase.client.HTableInterface.getScanner org.apache.hadoop.hbase.client.ResultScanner.close'
'edu.jhu.thrax.hadoop.features.WordCompressionRatioFeature.score','org.apache.hadoop.io.DoubleWritable.<init>'
'edu.jhu.thrax.hadoop.features.WordCountDifferenceFeature.score','org.apache.hadoop.io.IntWritable.<init>'
'azkaban.test.WordCountGrid.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'azkaban.test.WordCountGrid.Reduce.reduce','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'azkaban.test.WordCountGrid.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobClient.runJob'
'fr.eurecom.dsg.mapreduce.WordCountInMemoryCombiner.WCMapper.map','org.apache.hadoop.io.Text.toString'
'fr.eurecom.dsg.mapreduce.WordCountInMemoryCombiner.WCMapper.cleanup','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'fr.eurecom.dsg.mapreduce.WordCountInMemoryCombiner.WCReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'fr.eurecom.dsg.mapreduce.WordCountInMemoryCombiner.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.eurecom.dsg.mapreduce.WordCountInMemoryCombiner.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'org.pentaho.hadoop.sample.wordcount.WordCountMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'org.commoncrawl.tutorial.WordCountMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect org.apache.hadoop.mapred.Reporter.getCounter'
'pl.edu.icm.coansys.classification.documents.jobs.WordCountMapper_Proto.map','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'org.commoncrawl.tutorial.WordCountReducer.reduce','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.collect'
'st.happy_camper.hbase.coprocessors.wordcount.WordCountReginObserverTest.setUpBeforeClass','org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster'
'st.happy_camper.hbase.coprocessors.wordcount.WordCountReginObserverTest.tearDownAfterClass','org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster'
'st.happy_camper.hbase.coprocessors.wordcount.WordCountReginObserverTest.setUp','org.apache.hadoop.hbase.HBaseTestingUtility.createTable'
'st.happy_camper.hbase.coprocessors.wordcount.WordCountReginObserverTest.tearDown','org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HTableDescriptor.getName org.apache.hadoop.hbase.HBaseTestingUtility.deleteTable'
'st.happy_camper.hbase.coprocessors.wordcount.WordCountReginObserverTest.testDefaultCountTable','org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addCoprocessor org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'st.happy_camper.hbase.coprocessors.wordcount.WordCountReginObserverTest.testAnotherCountTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.HBaseTestingUtility.createTable org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HColumnDescriptor.<init> org.apache.hadoop.hbase.HTableDescriptor.addFamily org.apache.hadoop.hbase.HTableDescriptor.addCoprocessor org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseAdmin org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getRow org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.HBaseTestingUtility.getConfiguration org.apache.hadoop.hbase.client.HTable.<init> org.apache.hadoop.hbase.client.HTable.getScanner org.apache.hadoop.hbase.client.ResultScanner.iterator org.apache.hadoop.hbase.client.ResultScanner.close org.apache.hadoop.hbase.client.HTable.close'
'gov.llnl.ontology.mapreduce.stats.WordCountSumReducer.emitCounts','org.apache.hadoop.io.IntWritable.<init>'
'gov.llnl.ontology.mapreduce.stats.WordCountSumReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'thiswillbereplaced.WordCountTest.setUp','org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init>'
'thiswillbereplaced.WordCountTest.testMapper','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'thiswillbereplaced.WordCountTest.testReducer','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'thiswillbereplaced.WordCountTest.testMapReduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.cloudera.hbase.WordCountTest.setUp','org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.<init>'
'com.cloudera.hbase.WordCountTest.testMapper','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.cloudera.hbase.WordCountTest.testReducer','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'com.cloudera.hbase.WordCountTest.testMapReduce','org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.<init>'
'wpmcn.hadoop.WordCountTest.setUp','org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.setMapper org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.setReducer org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.setMapper org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.setReducer'
'wpmcn.hadoop.WordCountTest.testMapper','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.withOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.withOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.withOutput org.apache.hadoop.mrunit.mapreduce.MapDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.runTest'
'wpmcn.hadoop.WordCountTest.testReducer','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.withOutput org.apache.hadoop.mrunit.mapreduce.ReduceDriver<org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.runTest'
'wpmcn.hadoop.WordCountTest.testMapReduce','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.withInput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.addOutput org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.addOutput org.apache.hadoop.mrunit.mapreduce.MapReduceDriver<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.io.LongWritable>.runTest'
'org.apache.trevni.avro.WordCountUtil.writeLinesFile','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.avro.mapred.WordCountUtil.writeLinesFile','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.avro.mapred.WordCountUtil.writeLinesBytesFile','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.avro.mapred.WordCountUtil.writeLinesTextFile','org.apache.hadoop.fs.FileUtil.fullyDelete'
'org.apache.pig.test.utils.WordCount.TokenizerMapper.setup','org.apache.hadoop.conf.Configuration.get'
'org.apache.pig.test.utils.WordCount.TokenizerMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.toString'
'org.apache.pig.test.utils.WordCount.IntSumReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'org.apache.pig.test.utils.WordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.waitForCompletion'
'wpmcn.hadoop.WordCount.WordCountMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'wpmcn.hadoop.WordCount.WordCountReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set'
'wpmcn.hadoop.WordCount.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'wpmcn.hadoop.WordCount.main','org.apache.hadoop.util.ToolRunner.run'
'org.pentaho.hadoop.sample.wordcount.WordCount.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobClient.runJob'
'p3.pcap.examples.WordCount.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.WordCount.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.mapreduce.Mapper.Context.write'
'p3.pcap.examples.WordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.crunch.examples.WordCount.run','org.apache.hadoop.util.GenericOptionsParser.printGenericCommandUsage'
'org.apache.crunch.examples.WordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.example.memcached.demo.WordCount.MyMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.cloud9.example.memcached.demo.WordCount.MyReducer.reduce','org.apache.hadoop.io.IntWritable.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.IntWritable>.collect'
'edu.umd.cloud9.example.memcached.demo.WordCount.main','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setCombinerClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.JobClient.runJob'
'com.larsgeorge.WordCount.TokenizerMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.larsgeorge.WordCount.IntSumReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'com.larsgeorge.WordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.springsource.insight.plugin.hadoop.WordCount.WordCountMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.springsource.insight.plugin.hadoop.WordCount.WordCountReducer.reduce','org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.set'
'com.springsource.insight.plugin.hadoop.WordCount.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.springsource.insight.plugin.hadoop.WordCount.main','org.apache.hadoop.util.ToolRunner.run'
'com.cloudera.hbase.WordCount.Map.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.cloudera.hbase.WordCount.Reduce.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'com.cloudera.hbase.WordCount.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'com.cloudera.hbase.WordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'com.hadoopilluminated.examples.WordCount.TokenizerMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set'
'com.hadoopilluminated.examples.WordCount.IntSumReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.set'
'com.hadoopilluminated.examples.WordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.eurecom.dsg.mapreduce.WordCount.run','org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.Job.waitForCompletion'
'fr.eurecom.dsg.mapreduce.WordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'WordCount.WordCount.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'tfidf.WordCountsForDocs.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.waitForCompletion'
'tfidf.WordCountsForDocs.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'net.petrikainulainen.spring.data.apachehadoop.WordMapper.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.set org.apache.hadoop.io.IntWritable.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordMapperTest.MapEmpty','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordMapperTest.MapWithApostrophe','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordMapperTest.mapWithComma','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordMapperTest.mapWithDot','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordMapperTest.mapWithDoubleLine','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordMapperTest.mapWithExclamationMark','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordMapperTest.mapWithQuestionMark','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordMapperTest.mapWithQuationMark','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.createTable','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HConnectionManager.getConnection org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.hbase.client.HBaseAdmin.<init> org.apache.hadoop.hbase.client.HBaseAdmin.tableExists org.apache.hadoop.hbase.HTableDescriptor.<init> org.apache.hadoop.hbase.client.HBaseAdmin.createTable'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.setupScan','org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.addColumn org.apache.hadoop.hbase.client.Scan.addFamily org.apache.hadoop.hbase.client.Scan.addFamily'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.iterator','org.apache.hadoop.hbase.client.HTable.getScanner'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.table','org.apache.hadoop.hbase.client.HTable.<init>'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.getDependencyPaths','org.apache.hadoop.hbase.client.Result.getFamilyMap'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.putDependencyPaths','org.apache.hadoop.hbase.client.Put.<init>'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.putHypernymStatus','org.apache.hadoop.hbase.io.ImmutableBytesWritable.get org.apache.hadoop.hbase.client.Put.<init>'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.close','org.apache.hadoop.hbase.client.HTable.flushCommits org.apache.hadoop.hbase.client.HTable.close'
'gov.llnl.ontology.mapreduce.table.WordNetEvidenceTable.put','org.apache.hadoop.hbase.client.HTable.put'
'gov.llnl.ontology.mapreduce.stats.WordOccurrenceCountMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.WordOccurrenceCountMR.setupConfiguration','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.filecache.DistributedCache.addCacheFile'
'gov.llnl.ontology.mapreduce.stats.WordOccurrenceCountMR.setupReducer','org.apache.hadoop.mapreduce.Job.setCombinerClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setNumReduceTasks'
'gov.llnl.ontology.mapreduce.stats.WordOccurrenceCountMR.WordOccurrenceCountMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'pl.edu.icm.coansys.classification.documents.jobs.WordPerDocCountReducer.reduce','org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordReducer.reduce','org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordReducer.containsTargetWord','org.apache.hadoop.io.Text.toString'
'net.petrikainulainen.spring.data.apachehadoop.WordReducerTest.reduceWhenTargetWordIsFound','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.get'
'net.petrikainulainen.spring.data.apachehadoop.WordReducerTest.reduceWhenTargetWordIsNotFound','org.apache.hadoop.io.Text.<init>'
'net.petrikainulainen.spring.data.apachehadoop.WordReducerTest.createValues','org.apache.hadoop.io.IntWritable.<init>'
'fm.last.feathers.map.Words.configure','org.apache.hadoop.typedbytes.TypedBytesWritable.setValue'
'fm.last.feathers.map.Words.map','org.apache.hadoop.io.Text.toString org.apache.hadoop.typedbytes.TypedBytesWritable.setValue org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.typedbytes.TypedBytesWritable,org.apache.hadoop.typedbytes.TypedBytesWritable>.collect'
'org.apache.mahout.vectorizer.pruner.WordsPrunerReducer.setup','org.apache.hadoop.filecache.DistributedCache.getCacheFiles org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.fs.Path.<init>'
'gov.llnl.ontology.mapreduce.stats.WordsiMR.main','org.apache.hadoop.hbase.HBaseConfiguration.create org.apache.hadoop.util.ToolRunner.run'
'gov.llnl.ontology.mapreduce.stats.WordsiMR.setupConfiguration','org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'gov.llnl.ontology.mapreduce.stats.WordsiMR.setupReducer','org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.setOutputPath'
'gov.llnl.ontology.mapreduce.stats.WordsiMR.WordsiDependencyMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'gov.llnl.ontology.mapreduce.stats.WordsiMR.WordsiOccurrenceMapper.setup','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.toString'
'gov.llnl.ontology.mapreduce.stats.WordsiMR.emitContext','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'radlab.rain.workload.mapreduce.WorkGenMapReduceOperation.RatioMapper.map','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'radlab.rain.workload.mapreduce.WorkGenMapReduceOperation.RatioMapper.configure','org.apache.hadoop.mapred.JobConf.getRaw org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt'
'radlab.rain.workload.mapreduce.WorkGenMapReduceOperation.RatioReducer.reduce','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.setSize org.apache.hadoop.io.BytesWritable.get org.apache.hadoop.io.BytesWritable.getSize org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable>.collect org.apache.hadoop.mapred.Reporter.incrCounter org.apache.hadoop.mapred.Reporter.incrCounter'
'radlab.rain.workload.mapreduce.WorkGenMapReduceOperation.RatioReducer.configure','org.apache.hadoop.mapred.JobConf.getRaw org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt org.apache.hadoop.mapred.JobConf.getInt'
'radlab.rain.workload.mapreduce.WorkGenMapReduceOperation.execute','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.<init> org.apache.hadoop.mapred.JobClient.getClusterStatus org.apache.hadoop.mapred.ClusterStatus.getMaxReduceTasks org.apache.hadoop.mapred.ClusterStatus.getMaxReduceTasks org.apache.hadoop.mapred.ClusterStatus.getMaxMapTasks org.apache.hadoop.mapred.ClusterStatus.getMaxMapTasks org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.mapred.JobConf.setFloat org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers org.apache.hadoop.mapred.JobClient.runJob'
'org.apache.giraph.graph.WorkerAggregatorHandler.prepareSuperstep','org.apache.hadoop.io.Writable.readFields'
'org.apache.giraph.comm.netty.handler.WorkerRequestReservedMap.WorkerRequestReservedMap','org.apache.hadoop.conf.Configuration.getInt'
'com.alexholmes.hdfsslurper.WorkerThread.process','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.IOUtils.copyBytes org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.rename org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete'
'com.alexholmes.hdfsslurper.WorkerThread.stageSource','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileStatus.getPath'
'com.alexholmes.hdfsslurper.WorkerThread.hdfsFileCRC32','org.apache.hadoop.fs.Path.getFileSystem'
'com.alexholmes.hdfsslurper.WorkerThread.getHdfsTargetPath','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init>'
'com.alexholmes.hdfsslurper.WorkerThread.getDestPathFromScript','org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.service.WorkflowAppService.init','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getInt'
'org.apache.oozie.service.WorkflowAppService.readDefinition','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileStatus.getLen org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.service.WorkflowAppService.createProtoActionConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.setStrings org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.WorkflowAppService.getLibFiles','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.oozie.service.WorkflowAppService.init','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.service.WorkflowAppService.readDefinition','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.service.WorkflowAppService.createProtoActionConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.WorkflowAppService.getLibFiles','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.oozie.service.WorkflowAppService.init','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.service.WorkflowAppService.readDefinition','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open'
'org.apache.oozie.service.WorkflowAppService.createProtoActionConf','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.isFile org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.getParent org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getStrings org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.WorkflowAppService.getLibFiles','org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.oozie.service.WorkflowSchemaService.loadSchema','org.apache.hadoop.conf.Configuration.getStrings'
'org.apache.oozie.test.WorkflowTest.testWorkflowRun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.test.WorkflowTest.testWorkflowRunFromFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri'
'org.apache.oozie.test.WorkflowTest.testWorkflowRun','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.test.WorkflowTest.testWorkflowRunFromFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toUri'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.getPutForTable','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.Put.add'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.getPrefixForResource','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.getPrefixForValidator','org.apache.hadoop.hbase.util.Bytes.toBytes'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.populatePutWithResource','org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.populatePutWithResourceData','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.populatePutWithValidator','org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.populatePutWithValidatorData','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.getDeleteForTable','org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.client.Delete.deleteFamily org.apache.hadoop.hbase.client.Delete.deleteColumns'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.addResourceColumnsToDelete','org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Delete.deleteColumns'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.addResourceDataColumnsToDelete','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumns'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.addValidatorColumnsToDelete','org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Delete.deleteColumns org.apache.hadoop.hbase.util.Bytes.add org.apache.hadoop.hbase.client.Delete.deleteColumns'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.addValidatorDataColumnsToDelete','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Delete.deleteColumns'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.rowsToObject','org.apache.hadoop.hbase.client.Result.getNoVersionMap org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.populateResourceTemplateInfo','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'com.smartitengineering.cms.spi.impl.workspace.WorkspaceObjectConverter.populateValidatorTemplateInfo','org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString org.apache.hadoop.hbase.util.Bytes.toString'
'pl.edu.icm.coansys.importers.io.writers.tsv.WrapperSequenceFileWriter_Bwmeta.main','org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.BytesWritable.set org.apache.hadoop.io.IOUtils.closeStream'
'pl.edu.icm.coansys.importers.io.writers.tsv.WrapperSequenceFileWriter_Bwmeta.createSequenceFileWriter','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.io.Writable.getClass org.apache.hadoop.io.SequenceFile.createWriter'
'edu.umd.cloud9.debug.WritableComparatorTestHarness.compare','org.apache.hadoop.io.WritableComparable.write org.apache.hadoop.io.WritableComparable.write org.apache.hadoop.io.WritableComparator.compare'
'edu.umd.cloud9.io.WritableComparatorUtils.readUTF','org.apache.hadoop.io.WritableComparator.readUnsignedShort'
'com.taobao.adfs.distributed.rpc.WritableFactories.newInstance','org.apache.hadoop.conf.Configurable.setConf org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.crunch.types.writable.WritableGroupedTableType.configureShuffle','org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass'
'com.asakusafw.runtime.stage.collector.WritableSlot.store','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.Writable.write'
'com.asakusafw.runtime.stage.collector.WritableSlot.loadTo','org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.Writable.readFields'
'com.asakusafw.runtime.stage.collector.WritableSlot.write','org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'com.asakusafw.runtime.stage.collector.WritableSlot.readFields','org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.DataOutputBuffer.write'
'com.asakusafw.runtime.stage.collector.WritableSlotTest.write','org.apache.hadoop.io.DataOutputBuffer.<init> org.apache.hadoop.io.DataOutputBuffer.reset org.apache.hadoop.io.Writable.write org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength'
'com.asakusafw.runtime.stage.collector.WritableSlotTest.read','org.apache.hadoop.io.DataInputBuffer.<init> org.apache.hadoop.io.DataInputBuffer.reset org.apache.hadoop.io.DataInputBuffer.read'
'.WritableTestBase.serialize','org.apache.hadoop.io.Writable.write'
'.WritableTestBase.deserialize','org.apache.hadoop.io.Writable.readFields'
'.WritableTestBase.serializeToString','org.apache.hadoop.util.StringUtils.byteToHexString'
'.WritableTestBase.writeTo','org.apache.hadoop.util.StringUtils.byteToHexString'
'com.asakusafw.runtime.io.util.WritableUnion.WritableUnion','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.clone'
'com.asakusafw.runtime.io.util.WritableUnion.write','org.apache.hadoop.io.WritableUtils.writeVInt org.apache.hadoop.io.Writable.write'
'com.asakusafw.runtime.io.util.WritableUnion.readFields','org.apache.hadoop.io.WritableUtils.readVInt org.apache.hadoop.io.Writable.readFields'
'org.apache.oozie.util.WritableUtils.toByteArray','org.apache.hadoop.io.Writable.write'
'org.apache.oozie.util.WritableUtils.fromByteArray','org.apache.hadoop.util.ReflectionUtils.newInstance'
'org.apache.giraph.utils.WritableUtils.readFieldsFromByteArray','org.apache.hadoop.io.Writable.readFields'
'org.apache.giraph.utils.WritableUtils.writeToByteArray','org.apache.hadoop.io.Writable.write'
'org.apache.giraph.utils.WritableUtils.writeListToByteArray','org.apache.hadoop.io.Writable.write'
'org.apache.giraph.utils.WritableUtils.readListFieldsFromByteArray','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields'
'org.apache.oozie.util.WritableUtils.toByteArray','org.apache.hadoop.io.Writable.write'
'org.apache.oozie.util.WritableUtils.fromByteArray','org.apache.hadoop.util.ReflectionUtils.newInstance'
'com.datasalt.utils.commons.WritableUtils.serialize','org.apache.hadoop.io.Writable.write'
'com.datasalt.utils.commons.WritableUtils.deserialize','org.apache.hadoop.io.Writable.readFields'
'org.apache.crunch.types.writable.Writables.map','org.apache.hadoop.io.NullWritable.get org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.io.LongWritable.get org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.FloatWritable.get org.apache.hadoop.io.FloatWritable.<init> org.apache.hadoop.io.DoubleWritable.get org.apache.hadoop.io.DoubleWritable.<init> org.apache.hadoop.io.BooleanWritable.get org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.BytesWritable.getLength org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.io.BytesWritable.set'
'org.apache.crunch.types.writable.Writables.MapOutputMapFn.map','org.apache.hadoop.io.Text.<init>'
'com.mozilla.grouperfish.transforms.coclustering.display.WriteCoClusteringOutput.WriteCoClusteringOutput','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'com.mozilla.grouperfish.transforms.coclustering.display.WriteCoClusteringOutput.loadCentroids','org.apache.hadoop.io.Text.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.fs.FileSystem.close'
'com.mozilla.grouperfish.transforms.coclustering.display.WriteCoClusteringOutput.loadText','org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FileSystem.close'
'com.mozilla.grouperfish.transforms.coclustering.display.WriteCoClusteringOutput.loadPoints','org.apache.hadoop.io.IntWritable.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.Path.getName org.apache.hadoop.io.IntWritable.get org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.io.IOUtils.closeStream org.apache.hadoop.fs.FileSystem.close'
'com.mozilla.grouperfish.transforms.coclustering.display.WriteCoClusteringOutput.writeResults','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FSDataOutputStream.close'
'com.mozilla.grouperfish.transforms.coclustering.display.WriteCoClusteringOutput.writeTags','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FSDataOutputStream.write org.apache.hadoop.fs.FileSystem.close org.apache.hadoop.fs.FSDataOutputStream.close'
'org.apache.hcatalog.utils.WriteRC.run','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.hcatalog.utils.WriteRC.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hcatalog.utils.WriteText.run','org.apache.hadoop.util.GenericOptionsParser.<init> org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs org.apache.hadoop.conf.Configuration.set org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setNumReduceTasks org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.mapreduce.Job.waitForCompletion'
'org.apache.hcatalog.utils.WriteText.main','org.apache.hadoop.util.ToolRunner.run'
'lsh.mahout.io.WriteVectors.getSeqFileWriter','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.io.SequenceFile.createWriter'
'org.apache.accumulo.server.test.randomwalk.image.Write.visit','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.compareTo org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.impl.Writer.Writer','org.apache.hadoop.io.Text.<init>'
'org.apache.accumulo.core.client.impl.Writer.update','org.apache.hadoop.io.Text.<init>'
'org.apache.hcatalog.data.transfer.WriterContext.WriterContext','org.apache.hadoop.conf.Configuration.<init>'
'org.apache.hcatalog.data.transfer.WriterContext.writeExternal','org.apache.hadoop.conf.Configuration.write'
'org.apache.hcatalog.data.transfer.WriterContext.readExternal','org.apache.hadoop.conf.Configuration.readFields'
'org.apache.accumulo.server.test.WrongTabletTest.main','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init>'
'ivory.regression.basic.Wt10g_Basic.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.cikm2010.Wt10g_Desc_Indep.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.cikm2010.Wt10g_Title_Indep.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.cikm2010.Wt10g_Title_Joint.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'ivory.regression.sigir2011.Wt10g_VaryingTradeoff_Cascade.runRegression','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal'
'org.apache.oozie.util.XConfiguration.copy','org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.util.XConfiguration.injectDefaults','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'com.cloudera.lib.util.XConfiguration.copy','org.apache.hadoop.conf.Configuration.set'
'com.cloudera.lib.util.XConfiguration.injectDefaults','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.test.XDataTestCase.createCoordJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.test.XDataTestCase.writeCoordXml','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.test.XDataTestCase.createCoordAction','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.test.XDataTestCase.addRecordToWfJobTable','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.test.XDataTestCase.getAppPath','org.apache.hadoop.fs.Path.<init>'
'org.apache.oozie.test.XDataTestCase.getCoordConf','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.test.XDataTestCase.writeToFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'org.apache.oozie.test.XDataTestCase.createWorkflow','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.test.XDataTestCase.createWorkflowAction','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString'
'org.apache.oozie.test.XDataTestCase.createBundleJob','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.test.XDataTestCase.createBundleJobNegative','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.Path.toString org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.test.XDataTestCase.setClassesToBeExcluded','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.set'
'org.apache.oozie.test.XFsTestCase.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.setOwner org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission'
'org.apache.oozie.test.XFsTestCase.setAllPermissions','org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.oozie.test.XFsTestCase.createJobClient','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.test.XFsTestCase.setUp','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.setBoolean org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.set org.apache.hadoop.fs.FileSystem.getWorkingDirectory org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.makeQualified org.apache.hadoop.fs.FileSystem.exists org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.FileSystem.setOwner org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission'
'org.apache.oozie.test.XFsTestCase.setAllPermissions','org.apache.hadoop.fs.permission.FsPermission.<init> org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getFileStatus org.apache.hadoop.fs.FileStatus.isDir org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath'
'org.apache.oozie.test.XFsTestCase.createJobClient','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.service.XLogService.extractInfoForLogWebService','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.XLogService.extractInfoForLogWebService','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.oozie.service.XLogService.extractInfoForLogWebService','org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'ivory.ptc.driver.XMLFormatQueries.MyReducer.reduce','org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'ivory.ptc.driver.XMLFormatQueries.MyReducer.close','org.apache.hadoop.io.Text.set org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.Text,org.apache.hadoop.io.Text>.collect'
'ivory.ptc.driver.XMLFormatQueries.printUsage','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage'
'ivory.ptc.driver.XMLFormatQueries.run','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setNumMapTasks org.apache.hadoop.mapred.JobConf.setNumReduceTasks org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.mapred.FileInputFormat.setInputPaths org.apache.hadoop.mapred.FileOutputFormat.setOutputPath org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass org.apache.hadoop.mapred.JobConf.setMapOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputKeyClass org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobClient.runJob'
'ivory.ptc.driver.XMLFormatQueries.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.umd.cloud9.collection.XMLInputFormat.XMLRecordReader.initialize','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.io.compress.CompressionCodecFactory.<init> org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.compress.CompressionCodec.createInputStream org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength'
'edu.umd.cloud9.collection.XMLInputFormat.XMLRecordReader.nextKeyValue','org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.Text.set org.apache.hadoop.fs.Seekable.getPos org.apache.hadoop.io.DataOutputBuffer.reset'
'edu.umd.cloud9.collection.XMLInputFormat.XMLRecordReader.readUntilMatch','org.apache.hadoop.io.DataOutputBuffer.write'
'com.cloudera.recordbreaker.analyzer.XMLSchemaDescriptor.XMLSchemaDescriptor','org.apache.hadoop.fs.FileSystem.open'
'com.cloudera.circus.test.XTest.testsDestroy','org.apache.hadoop.mapred.MiniMRCluster.shutdown org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'com.cloudera.circus.test.XTest.getHadoopConf','org.apache.hadoop.mapred.JobConf.<init>'
'com.cloudera.circus.test.XTest.TestMethodListener.beforeInvocation','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs'
'com.cloudera.circus.test.XTest.TestMethodListener.createHadoopTempDir','org.apache.hadoop.security.UserGroupInformation.getCurrentUser org.apache.hadoop.security.UserGroupInformation.createProxyUser org.apache.hadoop.security.UserGroupInformation.doAs'
'com.cloudera.circus.test.XTest.TestMethodListener.run','org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.FileSystem.mkdirs'
'com.cloudera.circus.test.XTest.TestMethodListener.setUpEmbeddedHadoop','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.mapred.MiniMRCluster.createJobConf'
'org.apache.oozie.test.XTestCase.setUp','org.apache.hadoop.conf.Configuration.writeXml'
'org.apache.oozie.test.XTestCase.setUpEmbeddedHadoop','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.mapred.MiniMRCluster.createJobConf org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.security.authorize.ProxyUsers.refreshSuperUserGroupsConfiguration'
'org.apache.oozie.test.XTestCase.shutdownMiniCluster','org.apache.hadoop.mapred.MiniMRCluster.shutdown org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'org.apache.oozie.test.XTestCase.createJobConf','org.apache.hadoop.mapred.MiniMRCluster.createJobConf org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set'
'org.apache.oozie.test.XTestCase.executeWhileJobTrackerIsShutdown','org.apache.hadoop.mapred.MiniMRCluster.stopJobTracker org.apache.hadoop.mapred.MiniMRCluster.startJobTracker'
'org.apache.oozie.test.XTestCase.setUpEmbeddedHadoop','org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.security.UserGroupInformation.createUserForTesting org.apache.hadoop.hdfs.MiniDFSCluster.<init> org.apache.hadoop.hdfs.MiniDFSCluster.getFileSystem org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.mkdirs org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.permission.FsPermission.valueOf org.apache.hadoop.fs.FileSystem.setPermission org.apache.hadoop.fs.FileSystem.getUri org.apache.hadoop.mapred.MiniMRCluster.<init> org.apache.hadoop.mapred.MiniMRCluster.createJobConf org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get'
'org.apache.oozie.test.XTestCase.run','org.apache.hadoop.mapred.MiniMRCluster.shutdown org.apache.hadoop.hdfs.MiniDFSCluster.shutdown'
'com.manning.hip.ch3.xml.XmlInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'com.manning.hip.ch3.xml.XmlInputFormat.XmlRecordReader.XmlRecordReader','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek'
'com.manning.hip.ch3.xml.XmlInputFormat.XmlRecordReader.next','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.Text.set org.apache.hadoop.io.DataOutputBuffer.reset'
'com.manning.hip.ch3.xml.XmlInputFormat.XmlRecordReader.close','org.apache.hadoop.fs.FSDataInputStream.close'
'com.manning.hip.ch3.xml.XmlInputFormat.XmlRecordReader.getProgress','org.apache.hadoop.fs.FSDataInputStream.getPos'
'com.manning.hip.ch3.xml.XmlInputFormat.XmlRecordReader.readUntilMatch','org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.fs.FSDataInputStream.getPos'
'com.manning.hip.ch3.xml.XmlInputFormat.XmlRecordReader.nextKeyValue','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'org.apache.mahout.text.wikipedia.XmlInputFormat.createRecordReader','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration'
'org.apache.mahout.text.wikipedia.XmlInputFormat.XmlRecordReader.XmlRecordReader','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.mapreduce.lib.input.FileSplit.getStart org.apache.hadoop.mapreduce.lib.input.FileSplit.getLength org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.mapreduce.lib.input.FileSplit.getPath org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.seek'
'org.apache.mahout.text.wikipedia.XmlInputFormat.XmlRecordReader.next','org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.fs.FSDataInputStream.getPos org.apache.hadoop.io.LongWritable.set org.apache.hadoop.io.DataOutputBuffer.getData org.apache.hadoop.io.DataOutputBuffer.getLength org.apache.hadoop.io.Text.set org.apache.hadoop.io.DataOutputBuffer.reset'
'org.apache.mahout.text.wikipedia.XmlInputFormat.XmlRecordReader.getProgress','org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.mahout.text.wikipedia.XmlInputFormat.XmlRecordReader.readUntilMatch','org.apache.hadoop.fs.FSDataInputStream.read org.apache.hadoop.io.DataOutputBuffer.write org.apache.hadoop.fs.FSDataInputStream.getPos'
'org.apache.mahout.text.wikipedia.XmlInputFormat.XmlRecordReader.nextKeyValue','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.Text.<init>'
'edu.berkeley.chukwa_xtrace.XtrAdaptor.run','org.apache.hadoop.chukwa.ChunkImpl.<init>'
'edu.berkeley.chukwa_xtrace.XtrAdaptor.start','org.apache.hadoop.chukwa.datacollection.adaptor.AdaptorException.<init>'
'edu.berkeley.chukwa_xtrace.XtrAdaptor.run','org.apache.hadoop.chukwa.ChunkImpl.<init>'
'edu.berkeley.chukwa_xtrace.XtrAdaptor.start','org.apache.hadoop.chukwa.datacollection.adaptor.AdaptorException.<init>'
'edu.berkeley.chukwa_xtrace.XtrExtract.MapClass.map','org.apache.hadoop.chukwa.ChunkImpl.getData org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.chukwa.ChunkImpl.getData org.apache.hadoop.io.Text.<init> org.apache.hadoop.chukwa.extraction.engine.ChukwaRecord.getValue org.apache.hadoop.io.BytesWritable.<init> org.apache.hadoop.chukwa.extraction.engine.ChukwaRecord.getValue org.apache.hadoop.io.Text.<init>'
'edu.berkeley.chukwa_xtrace.XtrExtract.Reduce.reduce','org.apache.hadoop.io.BytesWritable.getBytes org.apache.hadoop.io.Text.toString org.apache.hadoop.mapreduce.v2.api.records.Counter.increment org.apache.hadoop.mapreduce.v2.api.records.Counter.increment org.apache.hadoop.mapreduce.v2.api.records.Counter.increment org.apache.hadoop.mapreduce.v2.api.records.Counter.increment org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.ArrayWritable.<init>'
'edu.berkeley.chukwa_xtrace.XtrExtract.run','org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setMapperClass org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.mapreduce.Job.setJobName org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.setMapOutputKeyClass org.apache.hadoop.mapreduce.Job.setMapOutputValueClass org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.setOutputFormatClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.submit'
'edu.berkeley.chukwa_xtrace.XtrExtract.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'edu.berkeley.chukwa_xtrace.XtrLoader.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.fs.FileSystem.getLocal org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.io.SequenceFile.createWriter org.apache.hadoop.chukwa.ChukwaArchiveKey.<init> org.apache.hadoop.chukwa.ChukwaArchiveKey.setTimePartition org.apache.hadoop.chukwa.ChunkImpl.getDataType org.apache.hadoop.chukwa.ChukwaArchiveKey.setDataType org.apache.hadoop.chukwa.ChunkImpl.getStreamName org.apache.hadoop.chukwa.ChukwaArchiveKey.setStreamName org.apache.hadoop.chukwa.ChunkImpl.getSeqID org.apache.hadoop.chukwa.ChukwaArchiveKey.setSeqId org.apache.hadoop.fs.FSDataOutputStream.close'
'edu.berkeley.chukwa_xtrace.XtrLoader.getNextChunkFromStdin','org.apache.hadoop.chukwa.ChunkImpl.<init> org.apache.hadoop.chukwa.ChunkImpl.addTag'
'org.apache.hama.bsp.YARNBSPJob.YARNBSPJob','org.apache.hadoop.yarn.conf.YarnConfiguration.<init> org.apache.hadoop.yarn.ipc.YarnRPC.create org.apache.hadoop.yarn.conf.YarnConfiguration.get org.apache.hadoop.net.NetUtils.createSocketAddr org.apache.hadoop.yarn.ipc.YarnRPC.getProxy'
'org.apache.hama.bsp.YARNBSPJob.kill','org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.protocolrecords.KillApplicationRequest.setApplicationId org.apache.hadoop.yarn.api.ClientRMProtocol.forceKillApplication'
'org.apache.hama.bsp.YARNBSPJob.waitForCompletion','org.apache.hadoop.yarn.api.records.ApplicationReport.getHost org.apache.hadoop.yarn.api.records.ApplicationReport.getRpcPort org.apache.hadoop.net.NetUtils.createSocketAddr org.apache.hadoop.ipc.RPC.waitForProxy org.apache.hadoop.yarn.util.Records.newRecord org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportRequest.setApplicationId org.apache.hadoop.yarn.api.ClientRMProtocol.getApplicationReport org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportResponse.getApplicationReport org.apache.hadoop.yarn.api.records.ApplicationReport.getFinalApplicationStatus org.apache.hadoop.yarn.api.records.ApplicationReport.getFinalApplicationStatus org.apache.hadoop.yarn.api.records.ApplicationReport.getFinalApplicationStatus org.apache.hadoop.yarn.api.ClientRMProtocol.getApplicationReport org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportResponse.getApplicationReport org.apache.hadoop.yarn.api.records.ApplicationReport.getFinalApplicationStatus org.apache.hadoop.yarn.api.records.ApplicationReport.getFinalApplicationStatus'
'com.zinnia.nectar.regression.hadoop.primitive.jobs.YDiffJob.call','org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.<init> org.apache.hadoop.mapreduce.Job.<init> org.apache.hadoop.mapreduce.Job.setJarByClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.chain.ChainMapper.addMapper org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.chain.ChainMapper.addMapper org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.setReducerClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath org.apache.hadoop.mapreduce.Job.setOutputKeyClass org.apache.hadoop.mapreduce.Job.setOutputValueClass org.apache.hadoop.mapreduce.Job.setInputFormatClass org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.<init> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.addJob org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.allFinished org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl.stop org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.FSDataInputStream.close'
'com.zinnia.nectar.regression.hadoop.primitive.mapreduce.YDiffMapper.map','org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.DoubleWritable.<init>'
'com.inadco.hbl.compiler.YamlModelParser.getCubeModel','org.apache.hadoop.conf.Configuration.get'
'com.inadco.hbl.compiler.YamlModelParser.initCubeModel','org.apache.hadoop.conf.Configuration.set'
'org.apache.hcatalog.hbase.snapshot.ZKBasedRevisionManager.initialize','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.get'
'org.apache.hcatalog.hbase.snapshot.ZKUtil.nextId','org.apache.hadoop.hbase.util.Bytes.toBytes'
'org.apache.hcatalog.hbase.snapshot.ZKUtil.currentID','org.apache.hadoop.hbase.util.Bytes.toBytes'
'skywriting.examples.skyhout.common.ZipDriver.ZipDriver','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'org.freeeed.main.ZipFileProcessor.createMapWritable','org.apache.hadoop.io.MapWritable.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.Text.<init> org.apache.hadoop.io.MapWritable.put'
'org.freeeed.main.ZipFileProcessor.emitAsMap','org.apache.hadoop.io.MD5Hash.digest'
'org.archive.hadoop.mapreduce.ZipNumOutputFormat.setZipNumLineCount','org.apache.hadoop.conf.Configuration.setInt'
'org.archive.hadoop.mapreduce.ZipNumOutputFormat.setZipNumOvercrawlDayCount','org.apache.hadoop.conf.Configuration.setInt'
'org.archive.hadoop.mapreduce.ZipNumOutputFormat.getRecordWriter','org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.conf.Configuration.getInt org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.Path.getFileSystem org.apache.hadoop.fs.FileSystem.create org.apache.hadoop.fs.FileSystem.create'
'org.archive.hadoop.mapreduce.ZipNumOutputFormat.getWorkFile','org.apache.hadoop.mapreduce.TaskAttemptContext.getTaskAttemptID org.apache.hadoop.mapreduce.TaskID.getId org.apache.hadoop.mapreduce.TaskAttemptContext.getConfiguration org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getWorkPath org.apache.hadoop.fs.Path.<init>'
'org.archive.hadoop.mapreduce.ZipNumRecordWriter.write','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.archive.hadoop.mapreduce.ZipNumRecordWriterOld.write','org.apache.hadoop.io.Text.toString org.apache.hadoop.io.Text.toString'
'org.archive.hadoop.pig.ZipNumStorage.setStoreLocation','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.archive.hadoop.pig.ZipNumStorage.putNext','org.apache.hadoop.io.Text.set org.apache.hadoop.io.Text.set org.apache.hadoop.mapreduce.RecordWriter.write'
'org.archive.hadoop.storage.ZipNumStorage.setStoreLocation','org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.mapreduce.Job.getConfiguration org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath'
'org.apache.hcatalog.templeton.tool.ZooKeeperCleanup.ZooKeeperCleanup','org.apache.hadoop.conf.Configuration.getLong org.apache.hadoop.conf.Configuration.getLong'
'org.apache.hcatalog.templeton.tool.ZooKeeperCleanup.checkAndDelete','org.apache.hadoop.conf.Configuration.get'
'org.apache.hama.ZooKeeperRunner.main','org.apache.hadoop.util.ToolRunner.run'
'org.apache.hcatalog.templeton.tool.ZooKeeperStorage.zkOpen','org.apache.hadoop.conf.Configuration.get org.apache.hadoop.conf.Configuration.getInt'
'org.apache.hcatalog.templeton.tool.ZooKeeperStorage.openStorage','org.apache.hadoop.conf.Configuration.get'
'org.goldenorb.zookeeper.ZookeeperUtils.writableToByteArray','org.apache.hadoop.io.Writable.write'
'org.goldenorb.zookeeper.ZookeeperUtils.byteArrayToWritable','org.apache.hadoop.util.ReflectionUtils.newInstance org.apache.hadoop.io.Writable.readFields org.apache.hadoop.io.Writable.readFields'
'hadoopGIS.examples.chained.map','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable>.collect'
'hadoopGIS.examples.chained.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable>.collect'
'hadoopGIS.examples.chained.configure','org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.mapred.JobConf.get org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.toString org.apache.hadoop.io.Text.<init>'
'hadoopGIS.examples.chained.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.mapred.JobClient.runJob'
'hadoopGIS.examples.chained.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'hadoopGIS.examples.create.map','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,hadoopGIS.GIS>.collect'
'hadoopGIS.examples.create.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,hadoopGIS.GIS>.collect'
'hadoopGIS.examples.create.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'hadoopGIS.examples.create.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'hadoopGIS.examples.delete.map','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.equals org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,hadoopGIS.GIS>.collect'
'hadoopGIS.examples.delete.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,hadoopGIS.GIS>.collect'
'hadoopGIS.examples.delete.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'hadoopGIS.examples.delete.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
'cmd.download.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init>'
'cmd.download.mergeToLocalFile','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.io.IOUtils.copyBytes'
'cmd.download.mergeToLocalFile2','org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.copyToLocalFile org.apache.hadoop.io.IOUtils.copyBytes'
'cmd.download.main','org.apache.hadoop.util.ToolRunner.run'
'implementations.hbaseDB.connectNode','org.apache.hadoop.hbase.HBaseConfiguration.<init> org.apache.hadoop.hbase.client.HTable.<init>'
'implementations.hbaseDB.readDB','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Get.<init> org.apache.hadoop.hbase.client.HTable.get org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Result.getValue org.apache.hadoop.hbase.util.Bytes.toString'
'implementations.hbaseDB.writeDB','org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.<init> org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.util.Bytes.toBytes org.apache.hadoop.hbase.client.Put.add org.apache.hadoop.hbase.client.HTable.put'
'implementations.hbaseDB.close','org.apache.hadoop.hbase.client.HTable.close org.apache.hadoop.hbase.HBaseConfiguration.clear'
'cmd.infer.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.Tool.run'
'cmd.infer.main','org.apache.hadoop.util.ToolRunner.run'
'cmd.stats.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.Tool.run'
'cmd.stats.main','org.apache.hadoop.util.ToolRunner.run'
'cmd.tdbloader4.run','org.apache.hadoop.util.ToolRunner.printGenericCommandUsage org.apache.hadoop.conf.Configuration.set org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.conf.Configuration.getBoolean org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.fs.FileSystem.get org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.delete org.apache.hadoop.util.Tool.run org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.util.Tool.run org.apache.hadoop.util.Tool.run org.apache.hadoop.util.Tool.run org.apache.hadoop.util.Tool.run'
'cmd.tdbloader4.main','org.apache.hadoop.util.ToolRunner.run'
'cmd.tdbloader4.createOffsetsFile','org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.listStatus org.apache.hadoop.fs.FileStatus.getPath org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.Path.getName org.apache.hadoop.fs.FileSystem.open org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.FileSystem.create'
'hadoopGIS.examples.update.map','org.apache.hadoop.io.LongWritable.<init> org.apache.hadoop.io.LongWritable.equals org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,hadoopGIS.GIS>.collect'
'hadoopGIS.examples.update.reduce','org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.io.LongWritable,hadoopGIS.GIS>.collect'
'hadoopGIS.examples.update.run','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.mapred.JobConf.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.<init> org.apache.hadoop.fs.Path.toUri org.apache.hadoop.filecache.DistributedCache.addCacheFile org.apache.hadoop.fs.Path.getName org.apache.hadoop.mapred.JobConf.set org.apache.hadoop.fs.Path.<init> org.apache.hadoop.mapred.JobConf.setJobName org.apache.hadoop.mapred.JobConf.setMapperClass org.apache.hadoop.mapred.JobConf.setReducerClass org.apache.hadoop.mapred.JobConf.setInputFormat org.apache.hadoop.mapred.JobConf.setOutputValueClass org.apache.hadoop.mapred.JobConf.setOutputFormat org.apache.hadoop.mapred.JobClient.runJob'
'hadoopGIS.examples.update.main','org.apache.hadoop.conf.Configuration.<init> org.apache.hadoop.util.ToolRunner.run'
